{"posts":[{"title":"Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image","content":"论文链接 作者:Fangchang Ma, Sertac Karaman 一、摘要 We consider the problem of dense depth prediction from a sparse set of depth measurements and a single RGB image. Since depth estimation from monocular images alone is inherently ambiguous and unreliable, to attain a higher level of robustness and accuracy, we introduce additional sparse depth samples, which are either acquired with a low-resolution depth sensor or computed via visual Simultaneous Localization and Mapping (SLAM) algorithms. We propose the use of a single deep regression network to learn directly from the RGB-D raw data, and explore the impact of number of depth samples on prediction accuracy. Our experiments show that, compared to using only RGB images, the addition of 100 spatially random depth samples reduces the prediction root-mean-square error by 50% on the NYU-Depth-v2 indoor dataset. It also boosts the percentage of reliable prediction from 59% to 92% on the KITTI dataset. We demonstrate two applications of the proposed algorithm: a plug-in module in SLAM to convert sparse maps to dense maps, and super-resolution for LiDARs. Software and video demonstration are publicly available. 二、介绍 深度感应和估计广泛工程应用： Depth sensing and estimation is of vital importance in a wide range of engineering applications, such as robotics, autonomous driving, augmented reality (AR) and 3D mapping. 稀疏深度测量值易获得： For instance, low-resolution depth sensors (e.g., a low-cost LiDARs) provide such measurements. Sparse depth measurements can also be computed from the output of SLAM and visual-inertial odometry algorithms. 三、相关工作 基于RGB的深度预测 Laina等人基于ResNet开发了一个全卷积深度残差网络 Godard等人将视差估计视为一个图像重建问题，训练神经网络使左图像扭曲变形以匹配右图像 从稀疏样本进行深度重建 Hawe等人假设视差映射在小波基础上是稀疏的，用共轭次梯度法重建了稠密的视差图像 Liu等人结合小波和轮廓波字典，实现更精确的重建 传感器融合 Mancini等人提出了一个接受RGB图像和光流图像作为输入的CNN来预测距离信息 Liao等人使用2D激光扫描仪以提供额外的深度参考信号作为输入，与单独使用RGB图像作为输入相比获得更高的准确度 Cadena等人开发了一个多模式自编码器来学习三种输入模式包括RGB,深度和语义标签。他们使用从FAST corner特征提取的深度信息作为系统输入的一部分来产生低分辨率的深度预测 四、方法 CNN结构 实验发现，瓶颈结构(即编码器-解码器)会有很好的表现，于是选择Laina等人提出的深度全卷积残差网络。网络结构如图： 网络的编码层对KITTI数据集使用ResNet-18、对NYU-Depth-v2数据集使用ResNet-50，去除最后的平均池化层和全连接层，后接一个3×3的卷积层；解码层由4个上采样层和一个双线性上采样层组成 深度采样 训练期间，从真值深度图像D∗D^*D∗中随机采样要输入的稀疏深度DDD。对于深度样本的任意目标数量mmm，计算一个伯努利概率p=mn，其中p=\\frac{m}{n}，其中p=nm​，其中nnn是D∗D^*D∗中有效深度像素的总和，对于任意像素(i,ji,ji,j): 通过这种策略每个训练样本的非零深度像素的实际数量在期望值mmm附近变化 损失函数 L2L_2L2​对数据中的异常值敏感，在实验中还会产生过度平滑的边界，另一种选择是berHu损失函数： 其中ccc等于一个batch中全部像素最大绝对误差的20%。实验还发现L1L_1L1​对RGB的深度预测问题中有更好的结果，所以将L1L_1L1​作为默认选择 实验结果 误差指标 RMSE：均方根误差 REL：平均相对绝对值误差 δi\\delta_iδi​：在阈值内的预测像素相对误差的百分比，越高越好 评估网络结构 比较了三种损失函数，不同的上采样层以及第一个卷积层 与选进技术比较 NYU-Depth-v2 Dataset： KITTI Dataset： 深度样本数量 NYU-Depth-v2 dataset： KITTI dataset： 总结 １. 提出了从RGB图像和稀疏深度样本点中预测密集的深度图像 ","link":"https://cisse-away.github.io/post/sparse-to-dense-depth-prediction-from-sparse-depth-samples-and-a-single-image/"},{"title":"Deeper Depth Prediction with Fully Convolutional Residual Networks","content":"论文链接 作者：Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, Nassir Navab 一、摘要 This paper addresses the problem of estimating the depth map of a scene given a single RGB image. We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps. In order to improve the output resolution, we present a novel way to efficiently learn feature map up-sampling within the network. For optimization, **we introduce the reverse Huber loss that is particularly suited for the task at hand and driven by the value distributions commonly present in depth maps. ** Our model is composed of a single architecture that is trained end-to-end and does not rely on post-processing techniques, such as CRFs or other additional refinement steps. As a result, it runs in real-time on images or videos. In the evaluation, we show that the proposed model contains fewer parameters and requires fewer training data than the current state of the art, while outperforming all approaches on depth estimation. Code and models are publicly available. 二、介绍 深度信息用途： Moreover, the availability of reasonably accurate depth information is well-known to improve many computer vision tasks with respect to the RGB-only counterpart, for example in reconstruction, recognition, sematic segmentation or human pose estimation 三、相关工作 Liu等人将语义分割任务与深度估计任务相结合，其中使用预测标签作为附加约束，以促进优化任务 Konrad等人在检索到的深度图上计算一个中值，然后通过交叉双边滤波进行平滑 Liu等人将优化问题表述为具有连续和离散变量势的条件随空场(CRF) Eigen等人首次使用CNN以双尺度架构来对单一图像进行回归稠密深度图 Roy和Todorovic等人提出将CNN与回归森林结合，在每个树节点使用非常浅的架构，从而限制了对大数据的需求 Liu等人提出在CNN训练中以CRF损失的形式学习一元和成对的潜在特征 Li等人和Wang等人使用分级CRF从超像素到像素级来细化他们的CNN预测 四、方法 CNN结构 AlexNet的接受域是151×151像素，VGG-16的接受域是276×276像素，而ResNet-50的接受域达到483×483 Resnet由于其残差结构可以避免网络退化和梯度消失问题 移去最后的池化层之后，若继续使用全连接层会产生过多的参数，于是作者提出了一种更少参数的上采样块 模块首先是一个unpooling层，用于提高特征图空间分辨率，之后接一个5×5的卷积层，通过ReLU激活。不过由于unpooling其后的卷积计算内容会包含很多0，作者通过将5×5卷积核分成4个小卷积核分别计算之后进行聚合，提高效率： 模块结构如图： 损失函数 回归问题的一个标准损失函数是L2L_2L2​损失函数，而作者使用BerHu损失函数，它在x∈[−c,c]x∈[−c,c]x∈[−c,c]时等于L1L_1L1​损失，在这个范围之外等于L2L_2L2​损失。每次梯度下降时都设置c=15maxic=\\frac{1}{5}max_ic=51​maxi​(∣yi~−yi∣\\lvert\\tilde{y_i}-y_i\\rvert∣yi​~​−yi​∣) 五、实验结果 NYU Depth Dataset(室内场景) 对比不同CNN变体与作者提出的结构 与先进技术对比 本文方法与其他方法预测结果： Make3D Dataset(室外场景) 与先前其他工作对比： 六、总结 这篇论文使用基于残差学习的单尺度CNN结构，去除了全连接层减少了参数量， 提出了更有效率的上采样卷积层，减少经过unpooling后的多数无效卷积操作 使用BerHu损失函数，动态调整损失函数 ","link":"https://cisse-away.github.io/post/deeper-depth-prediction-with-fully-convolutional-residual-networks/"},{"title":"Depth Map Prediction from a Single Image using a Multi-Scale Deep Network","content":"论文链接 作者: David Eigen, Christian Puhrsch and Rob Fergus 邮箱: deigen@cs.nyu.edu, cpuhrsch@nyu.edu, fergus@cs.nyu.edu 一、摘要 Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation,finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation. 二、介绍 深度估计意义： Estimating depth is an important component of understanding geometric relations within a scene. In turn, such relations help provide richer representations of objects and their environment, often leading to improvements in existing recognition tasks , as well as enabling many further applications such as 3D modeling, physics and support models, robotics, and potentially reasoning about occlusions. 单目图像案例： Potential applications include better understandings of the many images distributed on the web and social media outlets, real estate listings, and shopping sites. These include many examples of both indoor and outdoor scenes. 三、相关工作 Saxina等人使用线性回归和马尔可夫随机场（主要用于图像分割）从图像特征中预测深度 Hoiem等人不明确地预测深度而是将图像区域分成用于构成简单3D模型的几何结构（地面，天空，垂直结构） Ladicky等人通过把语义对象标签与单目深度特征结合来提升性能 Karsh等人使用一个基于SIFT Flow的kNN传递机制来估计单张图像静态背景的深度 Scharstein等人通过匹配，聚合和优化技术提出了一个包含多种2帧立体相关方法的调查和评估 Snavelys等人匹配同场景下多个未校准的照片视图来创建常见标准的准确3D重建 Konda等人在图像小块上训练一个自编码器来从立体序列中预测深度 四、方法 1.模型结构 整个网络由两部分堆叠而成：一个粗糙尺度的网络先在全局上预测场景深度；随后用一个精细尺度的网络来预测局部区域 2.尺度不变损失（scale-invariant error） 因为仅仅找到场景的平均尺度就占了总误差的很大一部分，即平均误差占比较高，于是作者提出了一个尺度不变误差来衡量场景点之间的关系，对于预测深度yyy和真实深度y∗y^*y∗,nnn个像素，下标为iii，定义尺度不变均方误差(在 log 空间)为： 其中，是对于给定(yyy,y∗y^*y∗)误差的最小化值，相当于一个正则项，表示整体的平均误差，用于进行全局尺度约束。引用CSDN博主「Xuefeng_BUPT」的理解： 在单目深度估计的问题中，从理论上说单目是无法获得尺度信息的，深度学习可以从大量的数据中学习到场景的尺度信息。但是，如果直接使用RMSE的loss函数来进行网络的训练，没有对图像尺度进行约束，导致估计得到的深度图像可能像素间相对值是准确的，但是整体深度和groundtruth给出的深度存在尺度上的差异。 此外若设置di=log(yi)−log(yi∗)d_i = log(y_i) - log(y^*_i)di​=log(yi​)−log(yi∗​)来表示像素iii预测值与真实值之间的差距，可以得到： 3.训练损失 将尺度不变误差作为训练损失： 其中di=log(yi)−log(yi∗)d_i=log(y_i)-log(y_i^*)di​=log(yi​)−log(yi∗​)并且λ∈[0,1]λ∈[0,1]λ∈[0,1]，λ=0λ=0λ=0时变成l2l_2l2​范式，λ=1λ=1λ=1时是尺度不变误差。作者发现设定λ=0.5λ=0.5λ=0.5有很好的预测质量提升 五、实验及结果 1.标准与对比 2.NYU Depth结果 3.KITTI结果 六、总结 本篇论文使用全局与局部两个尺度神经网络进行深度估计，与传统算法相比有很大提升，但预测结果仍不是很理想 本篇论文提出了尺度不变损失(Scale-Invariant Error)函数 ","link":"https://cisse-away.github.io/post/lun-wen-yue-du-depth-map-prediction-from-a-single-image-using-a-multi-scale-deep-network/"},{"title":"《动手学深度学习》——注意力机制","content":"参考课程 一、注意力提示 生物学中的注意力提示 为解释注意力是如何在视觉世界中展开的，一个双组件(two-component)的框架应运而生，在这个框架中，人们基于非自主性提示和自主性提示有选择地引导注意力的焦点 查询、键和值 在注意力机制下，将自主性提示称为查询（Queries）。给定任何查询，注意力机制通过注意力池化（attention pooling）将选择偏向于感官输入（sensory inputs）。这些感官输入被称为值（Values）。每个值都与一个键（Keys）配对，可以看作感官输入的非自主提示。 我们可以设计注意力池化层，使得给定得查询（自主性提示）可以与键（非自主性提示）进行交互，使得有偏向性得选择某些值（感官输入） 二、注意力池化：Nadaraya-Watson 核回归 非参数注意力池化 Nadaraya和Waston提出了一个想法，根据输入得位置对输出yiy_iyi​进行加权：f(x)=∑i=1nK(x−xi)∑j=1nK(x−xj)yi,f(x) = \\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i, f(x)=i=1∑n​∑j=1n​K(x−xj​)K(x−xi​)​yi​, ​ 其中KKK是核（Kernel）。上述估计器被称为Nadaraya-Wastson核回归 从注意力机制得角度重写一个更加通用得注意力池化公式：f(x)=∑i=1nα(x,xi)yi,f(x) = \\sum_{i=1}^n \\alpha(x, x_i) y_i, f(x)=i=1∑n​α(x,xi​)yi​, ​ 其中xxx是查询，(xix_ixi​, yiy_iyi​)是键值对。将查询xxx和键xIx_IxI​之间的关系建模为注意力权重α(x,xi)\\alpha(x, x_i)α(x,xi​)，这个权重将被分配给每一个对应值yiy_iyi​。 例如使用一个高斯核（Gaussian kernel），其定义为：K(u)=12πexp⁡(−u22).K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{u^2}{2}). K(u)=2π​1​exp(−2u2​). ​ 将高斯核带入得到： f(x)=∑i=1nα(x,xi)yi=∑i=1nexp⁡(−12(x−xi)2)∑j=1nexp⁡(−12(x−xj)2)yi=∑i=1nsoftmax(−12(x−xi)2)yi.\\begin{aligned} f(x) &amp;=\\sum_{i=1}^n \\alpha(x, x_i) y_i\\\\ &amp;= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}(x - x_i)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}(x - x_j)^2\\right)} y_i \\\\&amp;= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}(x - x_i)^2\\right) y_i. \\end{aligned} f(x)​=i=1∑n​α(x,xi​)yi​=i=1∑n​∑j=1n​exp(−21​(x−xj​)2)exp(−21​(x−xi​)2)​yi​=i=1∑n​softmax(−21​(x−xi​)2)yi​.​ ​ 其中若一个键xix_ixi​越接近给定的查询xxx，那么分配给这个键对应值yiy_iyi​的注意力权重就会越大 带参数注意力池化 非参数的Nadaraya-Watson核回归具有一致性(consistency)的优点：如果有足够的数据，此模型会收敛到最优结果 在查询xxx和键xix_ixi​之间的距离乘以可学习参数www： f(x)=∑i=1nα(x,xi)yi=∑i=1nexp⁡(−12((x−xi)w)2)∑j=1nexp⁡(−12((x−xi)w)2)yi=∑i=1nsoftmax(−12((x−xi)w)2)yi.\\begin{aligned}f(x) &amp;= \\sum_{i=1}^n \\alpha(x, x_i) y_i \\\\&amp;= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}((x - x_i)w)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}((x - x_i)w)^2\\right)} y_i \\\\&amp;= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}((x - x_i)w)^2\\right) y_i.\\end{aligned} f(x)​=i=1∑n​α(x,xi​)yi​=i=1∑n​∑j=1n​exp(−21​((x−xi​)w)2)exp(−21​((x−xi​)w)2)​yi​=i=1∑n​softmax(−21​((x−xi​)w)2)yi​.​ 定义模型：class NWKernelRegression(nn.Module): def __init__(self, **kwargs): super().__init__(**kwargs) self.w = nn.Parameter(torch.rand((1,), requires_grad=True)) def forward(self, queries, keys, values): queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1])) self.attention_weights = nn.functional.softmax(-((queries - keys) * self.w)**2 / 2, dim=1) return torch.bmm(self.attention_weights.unsqueeze(1), values.unsqueeze(-1)).reshape(-1) #平铺为一行 训练模型：X_tile = x_train.repeat((n_train, 1)) #沿着指定的维度重复tensor Y_tile = y_train.repeat((n_train, 1)) keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1)) #torch.eye生成对角线全1,其余全0的二维数组 values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1)) net = NWKernelRegression() loss = nn.MSELoss(reduction='none') trainer = torch.optim.SGD(net.parameters(), lr=0.5) epoch_list = [] loss_list = [] for epoch in range(5): trainer.zero_grad() l = loss(net(x_train, keys, values), y_train) / 2 l.sum().backward() trainer.step() print(f'epoch {epoch+1}, loss {float(l.sum()):.6f}') epoch_list.append(epoch+1) loss_list.append(l.sum()) plt.plot(epoch_list, loss_list) plt.xlim([1, 5]) plt.xlabel('epoch') plt.ylabel('loss') plt.show() ​ 5. 预测结果： keys = x_train.repeat((n_test, 1)) values = y_train.repeat((n_test, 1)) y_hat = net(x_test, keys, values).unsqueeze(1).detach() plot_kernel_reg(y_hat) ![](http://qzxkrnf1b.hd-bkt.clouddn.com/img/202110021307054.png) 训练完带参数的注意力汇聚模型后，在尝试拟合带噪声的训练数据时，预测结果绘制的线不如之前非参数模型的线平滑 三、注意力评分函数 注意力池化 假设有一个查询q∈Rq\\mathbf{q} \\in \\mathbb{R}^qq∈Rq和mmm个&quot;键-值&quot;对(k1,v1),…,(km,vm)(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)(k1​,v1​),…,(km​,vm​)，其中ki∈Rk\\mathbf{k}_i \\in \\mathbb{R}^kki​∈Rk, vi∈Rv\\mathbf{v}_i \\in \\mathbb{R}^vvi​∈Rv。注意力池化函数被表示成值得加权和： f(q,(k1,v1),…,(km,vm))=∑i=1mα(q,ki)vi∈Rv,f(\\mathbf{q}, (\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)) = \\sum_{i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i \\in \\mathbb{R}^v, f(q,(k1​,v1​),…,(km​,vm​))=i=1∑m​α(q,ki​)vi​∈Rv, 其中查询qqq和键kik_iki​的注意力权重(标量)是通过注意力评分函数aaa将两个向量映射成标量，再经过softmax运算得到的： α(q,ki)=softmax(a(q,ki))=exp⁡(a(q,ki))∑j=1mexp⁡(a(q,kj))∈R.\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_{j=1}^m \\exp(a(\\mathbf{q}, \\mathbf{k}_j))} \\in \\mathbb{R}. α(q,ki​)=softmax(a(q,ki​))=∑j=1m​exp(a(q,kj​))exp(a(q,ki​))​∈R. 遮蔽softmax操作 为将有意义的次元作为值去获取注意力池化，可以指定一个有效序列长度(即词元的个数)，以便在计算softmax时过滤掉查出指定范围的位置 def masked_softmax(X, valid_lens): if valid_lens is None: return nn.functional.softmax(X, dim=-1) else: shape = X.shape if valid_lens.dim() == 1: valid_lens = torch.repeat_interleave(valid_lens, shape[1]) else: valid_lens = valid_lens.reshape(-1) X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6) return nn.functional.softmax(X.reshape(shape), dim=-1) 演示效果： 加性注意力 一般当查询和键长度不同时，可以使用加性注意力作为评分函数。给定查询q∈Rq\\mathbf{q} \\in \\mathbb{R}^qq∈Rq和键k∈Rk\\mathbf{k} \\in \\mathbb{R}^kk∈Rk, 加性注意力(additive attention)的评分函数为： a(q,k)=wv⊤tanh(Wqq+Wkk)∈R,a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R}, a(q,k)=wv⊤​tanh(Wq​q+Wk​k)∈R, 其中可学习的参数是Wq∈Rh×q\\mathbf W_q\\in\\mathbb R^{h\\times q}Wq​∈Rh×q、Wk∈Rh×k\\mathbf W_k\\in\\mathbb R^{h\\times k}Wk​∈Rh×k和wv∈Rh\\mathbf w_v\\in\\mathbb R^{h}wv​∈Rh。将查询和键来连接起来后输入到一个多层感知机中，感知机隐藏单元是一个超参数hhh，通过使用tanhtanhtanh作为激活函数，并且禁用偏置项 class AdditiveAttention(nn.Module): def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs): super(AdditiveAttention, self).__init__(**kwargs) self.W_k = nn.Linear(key_size, num_hiddens, bias=False) self.W_q = nn.Linear(query_size, num_hiddens, bias=False) self.w_v = nn.Linear(num_hiddens, 1, bias=False) self.dropout = nn.Dropout(dropout) def forward(self, queries, keys, values, valid_lens): queries, keys = self.W_q(queries), self.W_k(keys) features = queries.unsqueeze(2) + keys.unsqueeze(1) features = torch.tanh(features) scores = self.w_v(features).squeeze(-1) self.attention_weights = masked_softmax(scores, valid_lens) return torch.bmm(self.dropout(self.attention_weights), values) 缩放点积注意力 点积可以得到计算效率更高的评分函数，但要求查询和键具有相同的长度ddd。为确保点积的方差在不考虑向量长度的情况下仍是1，则可使用缩放点积注意力(scaled dot-product attention)评分函数： a(q,k)=q⊤k/da(\\mathbf q, \\mathbf k) = \\mathbf{q}^\\top \\mathbf{k} /\\sqrt{d} a(q,k)=q⊤k/d​ 在实践中，通常从小批量角度来考虑提高效率，例如基于nnn个查询和mmm个键—值对计算注意力，其中查询和键的长度为ddd，值的长度为vvv。查询Q∈Rn×d\\mathbf Q\\in\\mathbb R^{n\\times d}Q∈Rn×d、键K∈Rm×d\\mathbf K\\in\\mathbb R^{m\\times d}K∈Rm×d和值V∈Rm×v\\mathbf V\\in\\mathbb R^{m\\times v}V∈Rm×v的缩放点积注意力是： softmax(QK⊤d)V∈Rn×v.\\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}. softmax(d​QK⊤​)V∈Rn×v. class DotProductAttention(nn.Module): def __init__(self, dropout, **kwargs): super(DotProductAttention, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) def forward(self, queries, keys, values, valid_lens=None): d = queries.shape[-1] # queries的倒数第一维大小 scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d) # keys的1,2维度交换(0维开始) self.attention_weights = masked_softmax(scores, valid_lens) return torch.bmm(self.dropout(self.attention_weights), values) 演示效果： 五、多头注意力 定义 为了模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，我们可以独立学习得到的hhh组不同的线性投影 模型 给定查询q∈Rdq\\mathbf{q} \\in \\mathbb{R}^{d_q}q∈Rdq​、键k∈Rdk\\mathbf{k} \\in \\mathbb{R}^{d_k}k∈Rdk​和值v∈Rdv\\mathbf{v} \\in \\mathbb{R}^{d_v}v∈Rdv​，每个注意力头hih_ihi​(i=1,…,hi = 1, \\ldots, hi=1,…,h)的计算方法为： hi=f(Wi(q)q,Wi(k)k,Wi(v)v)∈Rpv,\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v}, hi​=f(Wi(q)​q,Wi(k)​k,Wi(v)​v)∈Rpv​, 其中，可学习的参数包括Wi(q)∈Rpq×dq\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}Wi(q)​∈Rpq​×dq​、Wi(k)∈Rpk×dk\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}Wi(k)​∈Rpk​×dk​和Wi(v)∈Rpv×dv\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}Wi(v)​∈Rpv​×dv​，以及注意力池化函数fff。多头注意力的输出需要经过另一个线性转换，对应hhh个头连结后的结果，其可学习参数是Wo∈Rpo×hpv\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}Wo​∈Rpo​×hpv​： Wo[h1⋮hh]∈Rpo\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o} Wo​⎣⎢⎡​h1​⋮hh​​⎦⎥⎤​∈Rpo​ 这样每个头都可能会关注输入的不同部分，可表示比简单加权平均值更复杂的函数 模型实现： def transpose_qkv(X, num_heads): X = X.reshape(X.shape[0], X.shape[1], num_heads, -1) X = X.permute(0, 2, 1, 3) # 任意交换维度 return X.reshape(-1, X.shape[2], X.shape[3]) class MultiHeadAttention(nn.Module): def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs): super(MultiHeadAttention, self).__init__(**kwargs) self.num_heads = num_heads self.attention = DotProductAttention(dropout) self.W_q = nn.Linear(query_size, num_hiddens, bias=bias) self.W_k = nn.Linear(key_size, num_hiddens, bias=bias) self.W_v = nn.Linear(value_size, num_hiddens, bias=bias) self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias) def forward(self, queries, keys, values, valid_lens): queries = transpose_qkv(self.W_q(queries), self.num_heads) keys = transpose_qkv(self.W_k(keys), self.num_heads) values = transpose_qkv(self.W_v(values), self.num_heads) if valid_lens is not None: valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0) output = self.attention(queries, keys, values, valid_lens) output_concat = transpose_output(output, self.num_heads) return self.W_o(output_concat) 六、自注意力和位置编码 自注意力 给定一个词元组成的输入序列x1,…,xn\\mathbf{x}_1, \\ldots, \\mathbf{x}_nx1​,…,xn​，其中任意xi∈Rd\\mathbf{x}_i \\in \\mathbb{R}^dxi​∈Rd（1≤i≤n1 \\leq i \\leq n1≤i≤n）。该序列的自注意力输出为一个长度相同的序列y1,…,yn\\mathbf{y}_1, \\ldots, \\mathbf{y}_ny1​,…,yn​，其中： yi=f(xi,(x1,x1),…,(xn,xn))∈Rd\\mathbf{y}_i = f(\\mathbf{x}_i, (\\mathbf{x}_1, \\mathbf{x}_1), \\ldots, (\\mathbf{x}_n, \\mathbf{x}_n)) \\in \\mathbb{R}^d yi​=f(xi​,(x1​,x1​),…,(xn​,xn​))∈Rd 基于多头注意力对一个张量完成自注意力的计算，张量的形状为(批量大小，时间步的数目或词元序列的长度，ddd)。输出与输入的张量形状相同 num_hiddens, num_heads = 100, 5 attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens, num_hiddens, num_heads, 0.5) attention.eval() batch_size, num_queries, valid_lens = 2, 4, torch.tensor([3, 2]) X = torch.ones((batch_size, num_queries, num_hiddens)) attention(X, X, X, valid_lens).shape 比较CNN、RNN和Self-Attention 目标：将由nnn个词元组成的序列映射到另一个长度相等的序列，其中的每个输入词元或输出词元都由ddd维向量表示，比较三种架构的计算复杂性、顺序操作和最大路径长度。 对于一个卷积核大小为kkk的卷积层，序列长度是nnn，输入和输出的通道数量都是ddd，所以卷积层的计算复杂度为O(knd2)\\mathcal{O}(knd^2)O(knd2)，有O(1)\\mathcal{O}(1)O(1)个顺序操作，最大路径长度为O(n/k)\\mathcal{O}(n/k)O(n/k)。 对于循环神经网络，d×dd \\times dd×d权重矩阵的ddd维隐藏状态的乘法计算复杂度为O(d2)\\mathcal{O}(d^2)O(d2)。序列长度是nnn，所以RNN的计算复杂度为O(nd2)\\mathcal{O}(nd^2)O(nd2)。有O(n)\\mathcal{O}(n)O(n)个顺序操作无法并行化，最大路径长度也是O(n)\\mathcal{O}(n)O(n)。 对于自注意力，查询、键和值都是n×dn \\times dn×d矩阵。考虑缩放的“点-积”注意力，其中n×dn \\times dn×d矩阵乘以d×nd \\times nd×n矩阵，输出的n×nn \\times nn×n矩阵乘以n×dn \\times dn×d矩阵。所以自注意力有O(n2d)\\mathcal{O}(n^2d)O(n2d)计算复杂性。每个词元都通过自注意力直接来连接到其他词元，因此有O(1)\\mathcal{O}(1)O(1)个顺序操作可以并行计算，最大路径长度也是O(1)\\mathcal{O}(1)O(1)。 综上，CNN和自注意力都拥有并行计算的优势，而自注意力的最大路径长度最短，但因其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会很慢 位置编码 在处理词元序列时，自注意力因并行计算而放弃了顺序操作，为使用序列的顺序信息，通过在输入中添加位置编码(positional encoding)来注入绝对的或相对的位置信息。位置编码可以通过学习得到也可以直接固定得到。 例如：基于正弦函数和余弦函数的固定位置编码： 假设输入表示X∈Rn×d\\mathbf{X} \\in \\mathbb{R}^{n \\times d}X∈Rn×d包含一个序列中nnn个词元的ddd维嵌入表示。位置编码使用相同形状的位置嵌入矩阵P∈Rn×d\\mathbf{P} \\in \\mathbb{R}^{n \\times d}P∈Rn×d输出X+P\\mathbf{X} + \\mathbf{P}X+P，矩阵第iii行、第2j2j2j列上的元素为： pi,2j=sin⁡(i100002j/d),pi,2j+1=cos⁡(i100002j/d).\\begin{aligned} p_{i, 2j} &amp;= \\sin\\left(\\frac{i}{10000^{2j/d}}\\right),\\\\p_{i, 2j+1} &amp;= \\cos\\left(\\frac{i}{10000^{2j/d}}\\right).\\end{aligned} pi,2j​pi,2j+1​​=sin(100002j/di​),=cos(100002j/di​).​ class PositionalEncoding(nn.Module): def __init__(self, num_hiddens, dropout, max_len=1000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(dropout) self.P = torch.zeros((1, max_len, num_hiddens)) X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens) self.P[:, :, 0::2] = torch.sin(X) #切片，从下标0开始，间隔2 self.P[:, :, 1::2] = torch.cos(X) def forward(self, X): X = X + self.P[:, :X.shape[1], :].to(X.device) #取第2维的前X.shape[1]个元素 return self.dropout(X) encoding_dim, num_steps = 32, 60 pos_encoding = PositionalEncoding(encoding_dim, 0) pos_encoding.eval() X = pos_encoding(torch.zeros((1, num_steps, encoding_dim))) P = pos_encoding.P[:, :X.shape[1], :] plt.figure(figsize=(6, 2.5)) plt.plot(torch.arange(num_steps), P[0, :, 6].T, label='Col 6') plt.plot(torch.arange(num_steps), P[0, :, 7].T, label='Col 7') plt.plot(torch.arange(num_steps), P[0, :, 8].T, label='Col 8') plt.plot(torch.arange(num_steps), P[0, :, 9].T, label='Col 9') plt.legend(loc='best') plt.xlabel('Raw(position)') plt.show() 可与看出第6，7列的频率高于第8，9列 绝对位置信息 在二进制表示中，较高比特位的交替频率低于较低比特位。位置编码通过使用三角函数在编码维度上降低频率。由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间 相对位置信息 除了获得绝对位置信息外，上述位置编码还允许模型学习得到输入序列中相对位置信息。对于任何确定的位置偏移δ\\deltaδ，位置i+δi+\\deltai+δ处的位置编码可以线性投影位置iii处的编码来表示 七、Transformer 模型 Transformer由编码器和解码器组成，其中编码器和解码器是基于自注意力的模块叠加而成，源序列和目标序列的嵌入表示将加上位置编码再分别输入到编码器和解码器中 Transformer的编码器由多个相同的层组成，每个层有两个子层。第一个子层是多头自注意力池化(multi-head self-attention)；第二个子层是基于位置的前馈网络(positionwise feed-forward network)。每个子层都采用残差连接。之后使用层归一化(layer normalization) Transformer的解码器也有多个相同的层构成。除了编码器中的两个子层外，解码器还在两层之间插入了第三个子层：编码器-解码器注意力(encoder-decoder attention)层。它的查询来自前一个解码器层的输出，键和值来自整个编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。解码器中的每个位置只能考虑该位置之前的所有位置，即遮蔽(masked)注意力，这保留了自回归(auto-regressive)属性，详细介绍参考来源于极市平台的Vision Transformer综述 基于位置的前馈网络 基于位置的前馈网络是基于位置的原因是：对序列中所有位置的表示进行变换时使用的是同一个MLP。class PositionWiseFFN(nn.Module): def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs): super(PositionWiseFFN, self).__init__(**kwargs) self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens) self.relu = nn.ReLU() self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs) def forward(self, X): return self.dense2(self.relu(self.dense1(X))) 因为用同一个MLP对所有位置上的输入进行变换，所以当这些位置的而输入相同时，输出也相同 残差连接和层归一化 引用深度学习中的五种归一化（BN, LN, IN, GN, SN）简介中的对比，层归一化是基于特征维度进行归一化的。在NLP任务中(输入通常是变长序列)BN通常不如LN的效果好 对比LN与BN： 实现AddNorm类：class AddNorm(nn.Module): def __init__(self, normalized_shape, dropout, **kwargs): super(AddNorm, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) self.ln = nn.LayerNorm(normalized_shape) def forward(self, X, Y): return self.ln(self.dropout(Y) + X) Normalized_shape是输入第一维之后几维的大小： 编码器 编码器块实现：class EncoderBlock(nn.Module): def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs): super(EncoderBlock, self).__init__(**kwargs) self.attention = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias) self.addnorm1 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm2 = AddNorm(norm_shape, dropout) def forward(self, X, valid_lens): Y = self.addnorm1(X, self.attention(X, X, X, valid_lens)) return self.addnorm2(Y, self.ffn(Y)) Transformer编码器中的任何层不会改变输入的形状： 编码器实现：class TransformerEncoder(d2l.Encoder): def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs): super(TransformerEncoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.embedding = nn.Embedding(vocab_size, num_hiddens) # 生成对应的词嵌入，每个符号用num_hiddens维表示 self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(&quot;block&quot;+str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias)) def forward(self, X, valid_lens, *args): # 因使用范围在-1和1之间的固定位置编码，用嵌入维度的平方和进行缩放 X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self.attention_weights = [None] * len(self.blks) for i, blk in enumerate(self.blks): X = blk(X, valid_lens) self.attention_weights[i] = blk.attention.attention.attention_weights # 用于权重可视化 return X Transformer编码器输出的形状是batch_size、序列长度和num_hiddens： 解码器 对于序列到序列模型(sequence-to-squence model)，在训练阶段其输出序列的所有位置的词元都是已知的，在测试阶段其输出序列的词元是逐个生成的。 解码器块实现：class DecoderBlock(nn.Module): def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs): super(DecoderBlock, self).__init__(**kwargs) self.i = i self.attention1 = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm1 = AddNorm(norm_shape, dropout) self.attention2 = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm2 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm3 = AddNorm(norm_shape, dropout) def forward(self, X, state): enc_outputs, enc_valid_lens = state[0], state[1] # 训练阶段，输出序列的所有词元已知，所以`state[2][self.i]`初始化为`None` # 预测阶段，输出序列词元是逐个生成，所以`state[2][self.i]`包含从开始到当前位置的解码输出表示 if state[2][self.i] is None: key_values = X else: key_values = torch.cat((state[2][self.i], X), axis=1) state[2][self.i] = key_values if self.training: batch_size, num_steps, _ = X.shape # 动态调整valid_lens dec_valid_lens = torch.arange(1, num_steps + 1, device=X.device).repeat(batch_size, 1) else: dec_valid_lens = None X2 = self.attention1(X, key_values, key_values, dec_valid_lens) Y = self.addnorm1(X, X2) Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens) Z = self.addnorm2(Y, Y2) return self.addnorm3(Z, self.ffn(Z)), state 解码器实现：class TransformerDecoder(d2l.AttentionDecoder): def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs): super(TransformerDecoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.num_layers = num_layers self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(&quot;block&quot;+str(i), DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i)) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): return [enc_outputs, enc_valid_lens, [None] * self.num_layers] def forward(self, X, state): X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self._attention_weights = [[None] * len(self.blks) for _ in range(2)] for i, blk in enumerate(self.blks): X, state = blk(X, state) # 解码器自注意力权重 self._attention_weights[0][i] = blk.attention1.attention.attention_weights # &quot;编码器-解码器&quot;自注意力权重 self._attention_weights[1][i] = blk.attention2.attention.attention_weights return self.dense(X), state @property # 修饰方法，使方法可以像属性一样访问 def attention_weights(self): return self._attention_weights 训练 指定Transformer的编码器和解码器都是2层，使用4头注意力。在“英-法”机器翻译数据集上训练：num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10 lr, num_epochs, device = 0.005, 200, d2l.try_gpu() ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4 key_size, query_size, value_size = 32, 32, 32 norm_shape = [32] train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps) encoder = TransformerEncoder(len(src_vocab), key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout) decoder = TransformerDecoder(len(tgt_vocab), key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout) net = d2l.EncoderDecoder(encoder, decoder) d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device) 使用训练出的模型进行翻译，计算BLEU分数：engs = ['go .', &quot;i lost .&quot;, 'he\\'s calm .', 'i\\'m home .'] fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .'] for eng, fra in zip(engs, fras): translation, dec_attention_weight_seq = d2l.predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device, True) print(f'{eng} =&gt; {translation}, ', f'bleu {d2l.bleu(translation, fra, k=2):.3f}') ","link":"https://cisse-away.github.io/post/lesslessdong-shou-xue-shen-du-xue-xi-greatergreater-zhu-yi-li-ji-zhi/"},{"title":"《动手学深度学习》——现代卷积神经网络","content":"参考教程 深度卷积神经网络(AlexNet) 计算机视觉流水线 在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到的算法往往归于事后的解释 与训练端到端（从像素到分类结果）系统不同，经典机器学习的流水线看起来更像下面这样： 获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器 根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理 通过标准的特征提取算法（如SIFT（尺度不变特征变换、SURF（加速鲁棒特征或其他手动调整的流水线来输入数据 将提取的特征放到最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器 学习表征 观察图像特征的提取方法，SIFT、SURF、HOG（定向梯度直方图）、bags of visual words 和类似的特征提取方法占据了主导地位 另一组研究人员认为特征本身应该被学习，在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。 Alexnet 2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。 AlexNet和LeNet的架构非常相似： 模型设计： 在AlexNet的第一层，卷积窗口的形状是11×1111\\times1111×11 。 由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标 第二层中的卷积窗口形状被缩减为5×55\\times55×5，然后是3×33\\times33×3 在第一层、第二层和第五层卷积层之后，加入窗口形状为3×33\\times33×3 、步幅为2的最大汇聚层。 而且AlexNet的卷积通道数目是LeNet的10倍。 在最后一个卷积层后有两个全连接层，分别有4096个输出。 这两个巨大的全连接层拥有将近1GB的模型参数。 激活函数 AlexNet将sigmoid激活函数改为更简单的ReLU激活函数： ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算 当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易 容量控制和预处理 AlexNet通过dropout控制全连接层的模型复杂度，而LeNet只使用了权重衰减 AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色 网络结构实现：net = nn.Sequential( # 这里，我们使用一个11*11的更大窗口来捕捉对象。 # 同时，步幅为4，以减少输出的高度和宽度。 # 另外，输出通道的数目远大于LeNet nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数 nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # 使用三个连续的卷积层和较小的卷积窗口。 # 除了最后的卷积层，输出通道的数量进一步增加。 # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度 nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(), # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过度拟合 nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5), # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000 nn.Linear(4096, 10)) 使用块的网络(VGG) VGG块 由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。在最初的 VGG 论文中，作者使用了带有3×33\\times33×3卷积核、填充为 1（保持高度和宽度）的卷积层，和带有2×22\\times22×2池化窗口、步幅为 2（每个块后的分辨率减半）的最大汇聚层 块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。 在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即 3×33 \\times 33×3）比较浅层且宽的卷积更有效 VGG网络 与 AlexNet、LeNet 一样，VGG 网络可以分为两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成 原始 VGG 网络有 5 个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有 64 个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到 512。由于该网络使用 8 个卷积层和 3 个全连接层，因此它通常被称为 VGG-11 网络结构实现：def vgg_block(num_convs, in_channels, out_channels): layers = [] for _ in range(num_convs): layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)) layers.append(nn.ReLU()) in_channels = out_channels layers.append(nn.MaxPool2d(kernel_size=2,stride=2)) return nn.Sequential(*layers) def vgg(conv_arch): conv_blks = [] in_channels = 1 # 卷积层部分 for (num_convs, out_channels) in conv_arch: conv_blks.append(vgg_block(num_convs, in_channels, out_channels)) in_channels = out_channels return nn.Sequential( *conv_blks, nn.Flatten(), # 全连接层部分 nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 10)) net = vgg(conv_arch) 网络中的网络(NiN) NiN块 在每个像素位置应用一个全连接层。 如果我们将权重连接到每个空间位置，我们可以将其视为 1×1 卷积层，或作为在每个像素位置上独立作用的全连接层。 从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征 与VGG块的差异 NiN模型 NiN 和 AlexNet 之间的一个显著区别是 NiN 完全取消了全连接层。 相反，NiN 使用一个 NiN块，其输出通道数等于标签类别的数量。最后放一个 全局平均汇聚层，生成一个多元逻辑向量 NiN 设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间 实现：def nin_block(in_channels, out_channels, kernel_size, strides, padding): return nn.Sequential( nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()) net = nn.Sequential( nin_block(1, 96, kernel_size=11, strides=4, padding=0), nn.MaxPool2d(3, stride=2), nin_block(96, 256, kernel_size=5, strides=1, padding=2), nn.MaxPool2d(3, stride=2), nin_block(256, 384, kernel_size=3, strides=1, padding=1), nn.MaxPool2d(3, stride=2), nn.Dropout(0.5), # 标签类别数是10 nin_block(384, 10, kernel_size=3, strides=1, padding=1), nn.AdaptiveAvgPool2d((1, 1)), # 将四维的输出转成二维的输出，其形状为(批量大小, 10) nn.Flatten()) 含并行连结的网络(GoogLeNet) Inception块 结构图 GoogLeNet模型 结构示意： GoogLeNet 一共使用 9 个Inception块和全局平均汇聚层的堆叠来生成其估计值 Inception块之间的最大汇聚层可降低维度 第一个模块类似于 AlexNet 和 LeNet Inception块的栈从VGG继承 全局平均汇聚层避免了在最后使用全连接层 实现：class Inception(nn.Module): # `c1`--`c4` 是每条路径的输出通道数 def __init__(self, in_channels, c1, c2, c3, c4, **kwargs): super(Inception, self).__init__(**kwargs) # 线路1，单1 x 1卷积层 self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1) # 线路2，1 x 1卷积层后接3 x 3卷积层 self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1) self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1) # 线路3，1 x 1卷积层后接5 x 5卷积层 self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1) self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2) # 线路4，3 x 3最大汇聚层后接1 x 1卷积层 self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1) def forward(self, x): p1 = F.relu(self.p1_1(x)) p2 = F.relu(self.p2_2(F.relu(self.p2_1(x)))) p3 = F.relu(self.p3_2(F.relu(self.p3_1(x)))) p4 = F.relu(self.p4_2(self.p4_1(x))) # 在通道维度上连结输出 return torch.cat((p1, p2, p3, p4), dim=1) b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(), nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32), Inception(256, 128, (128, 192), (32, 96), 64), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64), Inception(512, 160, (112, 224), (24, 64), 64), Inception(512, 128, (128, 256), (24, 64), 64), Inception(512, 112, (144, 288), (32, 64), 64), Inception(528, 256, (160, 320), (32, 128), 128), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128), Inception(832, 384, (192, 384), (48, 128), 128), nn.AdaptiveAvgPool2d((1,1)), nn.Flatten()) net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10)) 批量归一化 训练神经网络时的一些挑战 数据预处理的方式通常会对最终结果产生巨大影响 对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量可能具有更广的变化范围 更深层的网络很复杂，容易过拟合 BN原理： 表达式：BN(x)=γ⊙x−μ^Bσ^B+β.\\mathrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}. BN(x)=γ⊙σ^B​x−μ^​B​​+β. 其中， μ^B\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}μ^​B​是样本均值， σ^B\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}σ^B​是小批量B\\mathcal{B}B的样本标准差。通常包含拉伸参数γ\\boldsymbol{\\gamma}γ和偏移参数β\\boldsymbol{\\beta}β，它们的形状与xxx相同且是需要与其他模型参数一起学习的参数 μ^B\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}μ^​B​和σ^B{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}}σ^B​表达式：μ^B=1∣B∣∑x∈Bx,σ^B2=1∣B∣∑x∈B(x−μ^B)2+ϵ.\\begin{aligned} \\hat{\\boldsymbol{\\mu}}_\\mathcal{B} &amp;= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x}, \\\\ \\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}^2 &amp;= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}})^2 + \\epsilon.\\end{aligned} μ^​B​σ^B2​​=∣B∣1​x∈B∑​x,=∣B∣1​x∈B∑​(x−μ^​B​)2+ϵ.​ 在方差估计值中添加一个小常量ϵ&gt;0\\epsilon &gt; 0ϵ&gt;0 ，以确保永远不会尝试除以零 批量归一化层 全连接层：h=ϕ(BN(Wx+b))\\mathbf{h} = \\phi(\\mathrm{BN}(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) ) h=ϕ(BN(Wx+b)) 卷积层：当卷积有多个输出通道时，需要对这些通道的每个输出执行批量归一化，每个通道都有自己的拉伸和偏移参数 预测过程中的批量归一化：不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差；我们可能需要使用我们的模型对逐个样本进行预测。 一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出 实现：class BatchNorm(nn.Module): # `num_features`：完全连接层的输出数量或卷积层的输出通道数。 # `num_dims`：2表示完全连接层，4表示卷积层 def __init__(self, num_features, num_dims): super().__init__() if num_dims == 2: shape = (1, num_features) else: shape = (1, num_features, 1, 1) # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0 self.gamma = nn.Parameter(torch.ones(shape)) self.beta = nn.Parameter(torch.zeros(shape)) # 非模型参数的变量初始化为0和1 self.moving_mean = torch.zeros(shape) self.moving_var = torch.ones(shape) def forward(self, X): # 如果 `X` 不在内存上，将 `moving_mean` 和 `moving_var` # 复制到 `X` 所在显存上 if self.moving_mean.device != X.device: self.moving_mean = self.moving_mean.to(X.device) self.moving_var = self.moving_var.to(X.device) # 保存更新过的 `moving_mean` 和 `moving_var` Y, self.moving_mean, self.moving_var = batch_norm( X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9) return Y 简明实现：net = nn.Sequential( nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(), nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(), nn.Linear(84, 10)) 残差网络(ResNet) 函数类 对于非嵌套函数类，较复杂的函数类并不总是向“真”函数f∗f^*f∗靠拢，相反嵌套函数类可以避免上述问题： 因此只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能 若能将新添加的层训练成恒等映射f(x)=xf(\\mathbf{x}) = \\mathbf{x}f(x)=x， 新模型和原模型将同样有效，同时由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差 残差块 正常块与残差块 残差映射往往更容易优化 当理想映射f(x)f(\\mathbf{x})f(x)极接近与恒等映射时，残差映射也易于捕捉恒等映射的细微波动 ResNet模型 ResNet-18架构： 实现：class Residual(nn.Module): #@save def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1): super().__init__() self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides) self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1) if use_1x1conv: self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides) else: self.conv3 = None self.bn1 = nn.BatchNorm2d(num_channels) self.bn2 = nn.BatchNorm2d(num_channels) self.relu = nn.ReLU(inplace=True) def forward(self, X): Y = F.relu(self.bn1(self.conv1(X))) Y = self.bn2(self.conv2(Y)) if self.conv3: X = self.conv3(X) Y += X return F.relu(Y) b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) def resnet_block(input_channels, num_channels, num_residuals, first_block=False): blk = [] for i in range(num_residuals): if i == 0 and not first_block: blk.append(Residual(input_channels, num_channels, use_1x1conv=True, strides=2)) else: blk.append(Residual(num_channels, num_channels)) return blk b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True)) b3 = nn.Sequential(*resnet_block(64, 128, 2)) b4 = nn.Sequential(*resnet_block(128, 256, 2)) b5 = nn.Sequential(*resnet_block(256, 512, 2)) net = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(512, 10)) 稠密连接网络(DenseNet) 从ResNet到DenseNet ResNet将函数fff分解为两部分：一个简单的线性项和一个更复杂的非线性项，若想将fff拓展成超过两部分的信息就可以用到DenseNet ResNet与DenseNet主要区别： DenseNet输出是连接而不是如ResNet的简单相加，即x→[x,f1(x),f2([x,f1(x)]),f3([x,f1(x),f2([x,f1(x)])]),…].\\mathbf{x} \\to \\left[ \\mathbf{x}, f_1(\\mathbf{x}), f_2([\\mathbf{x}, f_1(\\mathbf{x})]), f_3([\\mathbf{x}, f_1(\\mathbf{x}), f_2([\\mathbf{x}, f_1(\\mathbf{x})])]), \\ldots\\right].x→[x,f1​(x),f2​([x,f1​(x)]),f3​([x,f1​(x),f2​([x,f1​(x)])]),…]. 稠密网络主要由两部分构成：稠密块和过渡层 稠密快体 一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出信道。 然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结 实现：def conv_block(input_channels, num_channels): return nn.Sequential( nn.BatchNorm2d(input_channels), nn.ReLU(), nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1)) class DenseBlock(nn.Module): def __init__(self, num_convs, input_channels, num_channels): super(DenseBlock, self).__init__() layer = [] for i in range(num_convs): layer.append(conv_block( num_channels * i + input_channels, num_channels)) self.net = nn.Sequential(*layer) def forward(self, X): for blk in self.net: Y = blk(X) # 连接通道维度上每个块的输入和输出 X = torch.cat((X, Y), dim=1) return X 过度层 由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。 而过渡层可以用来控制模型复杂度。 它通过1×11\\times 11×1卷积层来减小通道数，并使用步幅为 2 的平均汇聚层减半高和宽，从而进一步降低模型复杂度 实现：def transition_block(input_channels, num_channels): return nn.Sequential( nn.BatchNorm2d(input_channels), nn.ReLU(), nn.Conv2d(input_channels, num_channels, kernel_size=1), nn.AvgPool2d(kernel_size=2, stride=2)) DenseNet模型 DenseNet 首先使用同 ResNet 一样的单卷积层和最大汇聚层 接下来，类似于 ResNet 使用的 4 个残差块，DenseNet 使用的是 4 个稠密块 在每个模块之间，ResNet 通过步幅为 2 的残差块减小高和宽，DenseNet 则使用过渡层来减半高和宽，并减半通道数 与 ResNet 类似，最后接上全局汇聚层和全连接层来输出结果 实现b1 = nn.Sequential( nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) # `num_channels`为当前的通道数 num_channels, growth_rate = 64, 32 num_convs_in_dense_blocks = [4, 4, 4, 4] blks = [] for i, num_convs in enumerate(num_convs_in_dense_blocks): blks.append(DenseBlock(num_convs, num_channels, growth_rate)) # 上一个稠密块的输出通道数 num_channels += num_convs * growth_rate # 在稠密块之间添加一个转换层，使通道数量减半 if i != len(num_convs_in_dense_blocks) - 1: blks.append(transition_block(num_channels, num_channels // 2)) num_channels = num_channels // 2 net = nn.Sequential( b1, *blks, nn.BatchNorm2d(num_channels), nn.ReLU(), nn.AdaptiveMaxPool2d((1, 1)), nn.Flatten(), nn.Linear(num_channels, 10)) ","link":"https://cisse-away.github.io/post/lesslessdong-shou-xue-shen-du-xue-xi-greatergreater-xian-dai-juan-ji-shen-jing-wang-luo/"},{"title":"《动手学深度学习》——卷积神经网络","content":"参考教程 从全连接层到卷积 不变性 平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应 局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系。最终，在后续神经网络，整个图像级别上可以集成这些局部特征用于预测 限制多层感知机 使用[X]i,j[\\mathbf{X}]_{i, j}[X]i,j​和[H]i,j[\\mathbf{H}]_{i, j}[H]i,j​分别表示输入图像和隐藏表示中位置(iii, jjj)处的像素，将参数从权重矩阵替换为四届权重张量W\\mathsf{W}W，假设U\\mathbf{U}U包含偏置参数，可将全连接层形式化为： [H]i,j=[U]i,j+∑k∑l[W]i,j,k,l[X]k,l=[U]i,j+∑a∑b[V]i,j,a,b[X]i+a,j+b.\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &amp;= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l} [\\mathbf{X}]_{k, l}\\\\ &amp;= [\\mathbf{U}]_{i, j} + \\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b} [\\mathbf{X}]_{i+a, j+b}.\\end{aligned}[H]i,j​​=[U]i,j​+k∑​l∑​[W]i,j,k,l​[X]k,l​=[U]i,j​+a∑​b∑​[V]i,j,a,b​[X]i+a,j+b​.​ 平移不变性 平移不变性意味着检测对象在输入X\\mathbf{X}X中的平移应仅导致隐藏表示H\\mathbf{H}H中的平移。也就是V\\mathsf{V}V和U\\mathbf{U}U不依赖于(iii,jjj)的值，即[V]i,j,a,b=[V]a,b[\\mathsf{V}]_{i, j, a, b} = [\\mathbf{V}]_{a, b}[V]i,j,a,b​=[V]a,b​，并且U\\mathbf{U}U是一个常数，如uuu： [H]i,j=u+∑a∑b[V]a,b[X]i+a,j+b.[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b} [\\mathbf{X}]_{i+a, j+b}. [H]i,j​=u+a∑​b∑​[V]a,b​[X]i+a,j+b​. 局部性 为收集用来训练参数[H]i,j[\\mathbf{H}]_{i, j}[H]i,j​的相关信息，不应偏离到距(iii,jjj)很远的地方。这意味着在∣a∣&gt;Δ|a|&gt; \\Delta∣a∣&gt;Δ或∣b∣&gt;Δ|b|&gt; \\Delta∣b∣&gt;Δ的范围之外，我们可以设置[V]a,b=0[\\mathbf{V}]_{a, b} = 0[V]a,b​=0： [H]i,j=u+∑a=−ΔΔ∑b=−ΔΔ[V]a,b[X]i+a,j+b.[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b} [\\mathbf{X}]_{i+a, j+b}. [H]i,j​=u+a=−Δ∑Δ​b=−Δ∑Δ​[V]a,b​[X]i+a,j+b​. 上式是一个卷积层，V\\mathbf{V}V被称为卷积核或滤波器。 以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。 参数大幅减少的代价是，我们的特征现在是平移不变的，并且当确定每个隐藏激活的值时，每一层只能包含局部的信息。 以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到未知数据中。 但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据。 通道 图像一般包含三个通道/三种原色,。因此，我们将X\\mathsf{X}X索引为[X]i,j,k[\\mathsf{X}]_{i, j, k}[X]i,j,k​。由此卷积相应地调整为[V]a,b,c[\\mathsf{V}]_{a,b,c}[V]a,b,c​ ，而不是[V]a,b[\\mathbf{V}]_{a,b}[V]a,b​。此外，由于输入图像是三维的，我们的隐藏表示H\\mathsf{H}H也最好采用三维张量： [H]i,j,d=∑a=−ΔΔ∑b=−ΔΔ∑c[V]a,b,c,d[X]i+a,j+b,c,[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c}, [H]i,j,d​=a=−Δ∑Δ​b=−Δ∑Δ​c∑​[V]a,b,c,d​[X]i+a,j+b,c​, 其中隐藏表示H\\mathsf{H}H中的ddd索引表示输出通道，而随后的输出将继续以三维张量HHH作为输入进入下一个卷积层。 所以可以定义具有多个通道的卷积层，而其中V\\mathsf{V}V是该卷积层的权重。 图像卷积 互相关运算 严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是 互相关运算 (cross-correlation)，而不是卷积运算 二维互相关运算实现：def corr2d(X, K): #@save &quot;&quot;&quot;计算二维互相关运算。&quot;&quot;&quot; h, w = K.shape Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] = (X[i:i + h, j:j + w] * K).sum() return Y 卷积层 基于上面定义的 corr2d 函数实现二维卷积层:class Conv2D(nn.Module): def __init__(self, kernel_size): super().__init__() self.weight = nn.Parameter(torch.rand(kernel_size)) self.bias = nn.Parameter(torch.zeros(1)) def forward(self, x): return corr2d(x, self.weight) + self.bias 感受野 在CNN中，对于某一层的任意元素xxx ，其 感受野 （Receptive Field）是指在前向传播期间可能影响xxx计算的所有元素（来自所有先前层） 填充和步幅 填充 在应用多层卷积时，常常丢失边缘像素，解决这个问题的简单方法即为填充(padding)：在输入图像的边界填充元素(通常填充元素0) 卷积神经网络中卷积核的高度和宽度通常为奇数，例如 1、3、5 或 7。 选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。 创建一个高度和宽度为3的二维卷积层，并在所有侧边填充1个像素：# 为了方便起见，我们定义了一个计算卷积层的函数。 # 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数 def comp_conv2d(conv2d, X): # 这里的（1，1）表示批量大小和通道数都是1 X = X.reshape((1, 1) + X.shape) Y = conv2d(X) # 省略前两个维度：批量大小和通道 return Y.reshape(Y.shape[2:]) # 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列 conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1) X = torch.rand(size=(8, 8)) comp_conv2d(conv2d, X).shape 步幅 有时为了高效计算或时减缩采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素，即步幅 输出形状：⌊(nh−kh+2∗ph)/sh+1⌋×⌊(nw−kw+2∗pw)/sw+1⌋.\\lfloor(n_h-k_h+2 * p_h)/s_h + 1\\rfloor \\times \\lfloor(n_w-k_w+2 * p_w)/s_w + 1\\rfloor. ⌊(nh​−kh​+2∗ph​)/sh​+1⌋×⌊(nw​−kw​+2∗pw​)/sw​+1⌋. 多输入多输出通道 多输入通道 我们可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和得到二维张量 多输出通道 用cic_ici​和c0c_0c0​分别表示输入和输出通道的数目，并让khk_hkh​和kwk_wkw​为卷积核的高度和宽度。为获得多个通道的输出，可为每个输出通道创建一个形状为ci×kh×kwc_i\\times k_h\\times k_wci​×kh​×kw​的卷积核张量，这样卷积核的形状为co×ci×kh×kwc_o\\times c_i\\times k_h\\times k_wco​×ci​×kh​×kw​ 1×11\\times 11×1卷积层 该卷积层失去了卷积层特有能力——在高度和宽度维度上，识别相邻元素间相互作用的能力。它的作用主要是调整网络层的通道数量和控制模型复杂性 池化层 它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性 最大池化层和平均池化层 实现：def pool2d(X, pool_size, mode='max'): p_h, p_w = pool_size Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): if mode == 'max': Y[i, j] = X[i: i + p_h, j: j + p_w].max() elif mode == 'avg': Y[i, j] = X[i: i + p_h, j: j + p_w].mean() return Y 在处理多通道输入数据时，池化层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。 这意味着池化层的输出通道数与输入通道数相同。 ","link":"https://cisse-away.github.io/post/lesslessdong-shou-xue-shen-du-xue-xi-greatergreater-juan-ji-shen-jing-wang-luo/"},{"title":"《动手学深度学习》——深度学习计算","content":"参考教程 层和块 自定义快 我们的实现将严重依赖父类，只需要提供我们自己的构造函数（Python中的__init__函数）和正向传播函数。class MLP(nn.Module): # 用模型参数声明层。这里，我们声明两个全连接的层 def __init__(self): # 调用`MLP`的父类`Block`的构造函数来执行必要的初始化。 # 这样，在类实例化时也可以指定其他函数参数，例如模型参数`params`（稍后将介绍） super().__init__() self.hidden = nn.Linear(20, 256) # 隐藏层 self.out = nn.Linear(256, 10) # 输出层 # 定义模型的正向传播，即如何根据输入`X`返回所需的模型输出 def forward(self, X): # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。 return self.out(F.relu(self.hidden(X))) net = MLP() 注意，除非我们实现一个新的运算符，否则我们不必担心反向传播函数或参数初始化，系统将自动生成这些 顺序块 为了构建我们自己的简化的MySequential，我们只需要定义两个关键函数： 一种将块逐个追加到列表中的函数 一种正向传播函数，用于将输入按追加块的顺序传递给块组成的“链条” class MySequential(nn.Module): def __init__(self, *args): super().__init__() for block in args: # 这里，`block`是`Module`子类的一个实例。我们把它保存在'Module'类的成员变量 # `_modules` 中。`block`的类型是OrderedDict。 self._modules[block] = block def forward(self, X): # OrderedDict保证了按照成员添加的顺序遍历它们 for block in self._modules.values(): X = block(X) return X net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10)) net(X) 在正向传播函数中执行代码 有时我们可能希望合并既不是上一层的结果也不是可更新参数的项。我们称之为常数参数（constant parameters）class FixedHiddenMLP(nn.Module): def __init__(self): super().__init__() # 不计算梯度的随机权重参数。因此其在训练期间保持不变。 self.rand_weight = torch.rand((20, 20), requires_grad=False) self.linear = nn.Linear(20, 20) def forward(self, X): X = self.linear(X) # 使用创建的常量参数以及`relu`和`dot`函数。 X = F.relu(torch.mm(X, self.rand_weight) + 1) # 复用全连接层。这相当于两个全连接层共享参数。 X = self.linear(X) # 控制流 while X.abs().sum() &gt; 1: X /= 2 return X.sum() net = FixedHiddenMLP() 在这个FixedHiddenMLP模型中，我们实现了一个隐藏层，其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。这个权重不是一个模型参数，因此它永远不会被反向传播更新。然后，网络将这个固定层的输出通过一个全连接层 参数管理 参数访问 当通过Sequential类定义模型时，我们可以通过索引来访问模型的任意层print(net[2].state_dict()) OrderedDict([('weight', tensor([[ 0.2065, 0.1124, -0.1230, -0.0182, -0.0172, -0.3526, -0.2300, -0.3313]])), ('bias', tensor([-0.0957]))]) 目标参数 要对参数执行任何操作，首先我们需要访问底层的数值print(type(net[2].bias)) print(net[2].bias) print(net[2].bias.data) &lt;class 'torch.nn.parameter.Parameter'&gt; Parameter containing: tensor([-0.0957], requires_grad=True) tensor([-0.0957]) # 访问梯度 net[2].weight.grad == None True 一次性访问所有参数print(*[(name, param.shape) for name, param in net[0].named_parameters()]) print(*[(name, param.shape) for name, param in net.named_parameters()]) ('weight', torch.Size([8, 4])) ('bias', torch.Size([8])) ('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1])) 参数初始化 内置初始化 将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0:def init_normal(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, mean=0, std=0.01) nn.init.zeros_(m.bias) net.apply(init_normal) net[0].weight.data[0], net[0].bias.data[0] 对某些块应用不同的初始化方法def xavier(m): if type(m) == nn.Linear: nn.init.xavier_uniform_(m.weight) def init_42(m): if type(m) == nn.Linear: nn.init.constant_(m.weight, 42) net[0].apply(xavier) net[2].apply(init_42) print(net[0].weight.data[0]) print(net[2].weight.data) 自定义初始化 我们实现了一个my_init函数来应用到net:def my_init(m): if type(m) == nn.Linear: print(&quot;Init&quot;, *[(name, param.shape) for name, param in m.named_parameters()][0]) nn.init.uniform_(m.weight, -10, 10) m.weight.data *= m.weight.data.abs() &gt;= 5 net.apply(my_init) net[0].weight[:2] 参数绑定 多个层间共享参数# 我们需要给共享层一个名称，以便可以引用它的参数。 shared = nn.Linear(8, 8) net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(8, 1)) net(X) # 检查参数是否相同 print(net[2].weight.data[0] == net[4].weight.data[0]) net[2].weight.data[0, 0] = 100 # 确保它们实际上是同一个对象，而不只是有相同的值。 print(net[2].weight.data[0] == net[4].weight.data[0]) tensor([True, True, True, True, True, True, True, True]) tensor([True, True, True, True, True, True, True, True]) ","link":"https://cisse-away.github.io/post/lesslessdong-shou-xue-shen-du-xue-xi-greatergreater-shen-du-xue-xi-ji-suan/"},{"title":"《动手学深度学习》——多层感知机","content":"参考教程 模型选择、欠拟合和过拟合 K折交叉验证 当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是采用KKK折交叉验证。这里，原始训练数据被分成KKK个不重叠的子集。然后执行KKK次模型训练和验证，每次在K−1K−1K−1个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对KKK次实验的结果取平均来估计训练和验证误差。 正向传播、反向传播和计算图 正向传播 假设输入样本是x∈Rd\\mathbf{x}\\in \\mathbb{R}^dx∈Rd，并且隐藏层不包括偏置项。这里的中间变量是： z=W(1)x,\\mathbf{z}= \\mathbf{W}^{(1)} \\mathbf{x}, z=W(1)x, 其中W(1)∈Rh×d\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}W(1)∈Rh×d是隐藏层的权重参数。然后将中间变量 z∈Rh\\mathbf{z}\\in \\mathbb{R}^hz∈Rh通过激活函数ϕ\\phiϕ后，我们得到长度为hhh的隐藏激活向量： h=ϕ(z).\\mathbf{h}= \\phi (\\mathbf{z}). h=ϕ(z). 隐藏变量hhh也是一个中间变量。假设输出层的参数只有权重W(2)∈Rq×h\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}W(2)∈Rq×h，我们可以得到输出层变量，它是一个长度为qqq的向量： o=W(2)h.\\mathbf{o}= \\mathbf{W}^{(2)} \\mathbf{h}. o=W(2)h. 假设损失函数为lll，样本标签为yyy，我们可以计算单个数据样本的损失项， L=l(o,y).L = l(\\mathbf{o}, y). L=l(o,y). 根据L2L_2L2​正则化的定义，给定超参数λ\\lambdaλ，正则化项为 s=λ2(∥W(1)∥F2+∥W(2)∥F2)s = \\frac{\\lambda}{2} \\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right) s=2λ​(∥W(1)∥F2​+∥W(2)∥F2​) 其中，矩阵的弗罗贝尼乌斯范数是将矩阵展平为向量后应用的L2L_2L2​范数。最后，模型在给定数据样本上的正则化损失为： J=L+s.J = L + s. J=L+s. 将JJJ称为目标函数。 反向传播 第一步是计算目标函数J=L+sJ=L+sJ=L+s相对于损失项 L 和正则项 s 的梯度: ∂J∂L=1 and ∂J∂s=1\\frac{\\partial J}{\\partial L} = 1 \\; \\text{and} \\; \\frac{\\partial J}{\\partial s} = 1 ∂L∂J​=1and∂s∂J​=1 根据链式法则计算目标函数关于输出层变量o\\mathbf{o}o的梯度： ∂J∂o=prod(∂J∂L,∂L∂o)=∂L∂o∈Rq\\frac{\\partial J}{\\partial \\mathbf{o}} = \\text{prod}\\left(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial \\mathbf{o}}\\right) = \\frac{\\partial L}{\\partial \\mathbf{o}} \\in \\mathbb{R}^q∂o∂J​=prod(∂L∂J​,∂o∂L​)=∂o∂L​∈Rq 计算正则化项相对于两个参数的梯度： ∂s∂W(1)=λW(1) and ∂s∂W(2)=λW(2).\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} = \\lambda \\mathbf{W}^{(1)} \\; \\text{and} \\; \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} = \\lambda \\mathbf{W}^{(2)}.∂W(1)∂s​=λW(1)and∂W(2)∂s​=λW(2). 计算最接近输出层的模型参数的梯度∂J/∂W(2)∈Rq×h\\partial J/\\partial \\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}∂J/∂W(2)∈Rq×h。使用链式法则得出： ∂J∂W(2)=prod(∂J∂o,∂o∂W(2))+prod(∂J∂s,∂s∂W(2))=∂J∂oh⊤+λW(2).\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(2)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}\\right)= \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}. ∂W(2)∂J​=prod(∂o∂J​,∂W(2)∂o​)+prod(∂s∂J​,∂W(2)∂s​)=∂o∂J​h⊤+λW(2). 为了获得关于W(1)\\mathbf{W}^{(1)}W(1)的梯度，我们需要继续沿着输出层到隐藏层反向传播。关于隐藏层输出的梯度∂J/∂h∈Rh\\partial J/\\partial \\mathbf{h} \\in \\mathbb{R}^h∂J/∂h∈Rh由下式给出： ∂J∂h=prod(∂J∂o,∂o∂h)=W(2)⊤∂J∂o.\\frac{\\partial J}{\\partial \\mathbf{h}} = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}}\\right) = {\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}.∂h∂J​=prod(∂o∂J​,∂h∂o​)=W(2)⊤∂o∂J​. 由于激活函数ϕ\\phiϕ是按元素计算的，计算中间变量z\\mathbf{z}z的梯度∂J/∂z∈Rh\\partial J/\\partial \\mathbf{z} \\in \\mathbb{R}^h∂J/∂z∈Rh需要使用按元素乘法运算符，我们用⊙\\odot⊙表示： ∂J∂z=prod(∂J∂h,∂h∂z)=∂J∂h⊙ϕ′(z).\\frac{\\partial J}{\\partial \\mathbf{z}} = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{h}}, \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}}\\right) = \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi&#x27;\\left(\\mathbf{z}\\right).∂z∂J​=prod(∂h∂J​,∂z∂h​)=∂h∂J​⊙ϕ′(z). 最后，得到最接近输入层的模型参数的梯度∂J/∂W(1)∈Rh×d\\partial J/\\partial \\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}∂J/∂W(1)∈Rh×d。根据链式法则： ∂J∂W(1)=prod(∂J∂z,∂z∂W(1))+prod(∂J∂s,∂s∂W(1))=∂J∂zx⊤+λW(1).\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{z}}, \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}^{(1)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}\\right) = \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}.∂W(1)∂J​=prod(∂z∂J​,∂W(1)∂z​)+prod(∂s∂J​,∂W(1)∂s​)=∂z∂J​x⊤+λW(1). 参数初始化 默认初始化 使用正态分布来初始化权重值，若不指定框架将使用默认的随机初始化方法 Xavier初始化 对于某些没有非线性的全连接层输出(例如隐藏变量)oio_{i}oi​的尺度分布。对于该层ninn_{in}nin​输入xjx_jxj​及其相关权重wijw_{ij}wij​，输出为： oi=∑j=1ninwijxjo_{i} = \\sum_{j=1}^{n_\\mathrm{in}} w_{ij} x_j oi​=j=1∑nin​​wij​xj​ 权重 wijw_{ij}wij​都是从同一分布中独立抽取的。此外，假设该分布具有零均值和方差σ2\\sigma^2σ2。假设层xjx_jxj​ 的输入也具有零均值和方差γ2\\gamma^2γ2 ，并且它们独立于wijw_{ij}wij​ 并且彼此独立。在这种情况下，我们可以按如下方式计算$o_i} 的平均值和方差： E[oi]=∑j=1ninE[wijxj]=∑j=1ninE[wij]E[xj]=0,Var[oi]=E[oi2]−(E[oi])2=∑j=1ninE[wij2xj2]−0=∑j=1ninE[wij2]E[xj2]=ninσ2γ2.\\begin{aligned} E[o_i] &amp; = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij} x_j] \\\\ &amp;= \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij}] E[x_j] \\\\ &amp;= 0, \\\\ \\mathrm{Var}[o_i] &amp; = E[o_i^2] - (E[o_i])^2 \\\\ &amp; = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\\\ &amp; = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\\\ &amp; = n_\\mathrm{in} \\sigma^2 \\gamma^2. \\end{aligned}E[oi​]Var[oi​]​=j=1∑nin​​E[wij​xj​]=j=1∑nin​​E[wij​]E[xj​]=0,=E[oi2​]−(E[oi​])2=j=1∑nin​​E[wij2​xj2​]−0=j=1∑nin​​E[wij2​]E[xj2​]=nin​σ2γ2.​ 保持方差不变的一种方法是设置ninσ2=1n_\\mathrm{in} \\sigma^2 = 1nin​σ2=1 。现在考虑反向传播过程，我们面临着类似的问题，尽管梯度是从更靠近输出的层传播的。使用与正向传播相同的推理，我们可以看到，除非noutσ2=1n_\\mathrm{out} \\sigma^2 = 1nout​σ2=1，否则梯度的方差可能会增大，其中noutn_outno​ut是该层的输出的数量。这使我们进退两难：我们不可能同时满足这两个条件。相反，我们只需满足： 12(nin+nout)σ2=1 或等价于 σ=2nin+nout.\\begin{aligned} \\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ 或等价于 } \\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}. \\end{aligned}21​(nin​+nout​)σ2=1 或等价于 σ=nin​+nout​2​​.​ 通常Xavier初始化从均值为零，方差σ2=2nin+nout\\sigma^2 = \\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}σ2=nin​+nout​2​的高斯分布中采样权重。 ","link":"https://cisse-away.github.io/post/lesslessdong-shou-xue-shen-du-xue-xi-greatergreater-duo-ceng-gan-zhi-ji/"},{"title":"《动手学深度学习》——线性神经网络","content":"参考教程 线性回归 线性模型 当输入包含ddd个特征时，将预测结果y^\\hat{y}y^​表示为：y^=w1x1+...+wdxd+b.\\hat{y} = w_1 x_1 + ... + w_d x_d + b. y^​=w1​x1​+...+wd​xd​+b. 将所有特征放到向量x∈Rd\\mathbf{x} \\in \\mathbb{R}^dx∈Rd中，并将所有权重放到向量w∈Rd\\mathbf{w} \\in \\mathbb{R}^dw∈Rd中：y^=w⊤x+b.\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b. y^​=w⊤x+b. 用符号表示的矩阵X∈Rn×d\\mathbf{X} \\in \\mathbb{R}^{n \\times d}X∈Rn×d可以引用整个数据集的nnn个样本。其中，X\\mathbf{X}X的每一行是一个样本，每一列是一种特征：y^=Xw+b{\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b y^​=Xw+b 损失函数 回归问题中最常用的损失函数时平方误差函数。当样本iii的预测值为y^(i)\\hat{y}^{(i)}y^​(i)，其相应的真实标签为y(i){y}^{(i)}y(i)时，平方误差可以定义为： l(i)(w,b)=12(y^(i)−y(i))2.l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2. l(i)(w,b)=21​(y^​(i)−y(i))2. 为度量模型在整个数据集上的质量，需计算在训练集nnn个样本上的损失均值 L(w,b)=1n∑i=1nl(i)(w,b)=1n∑i=1n12(w⊤x(i)+b−y(i))2.L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2. L(w,b)=n1​i=1∑n​l(i)(w,b)=n1​i=1∑n​21​(w⊤x(i)+b−y(i))2. 训练模型时，寻找一组参数(w∗,b∗\\mathbf{w}^*, b^*w∗,b∗)，这组参数能最小化在所有训练样本上的总损失: w∗,b∗=*⁡argminw,b L(w,b).\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\ L(\\mathbf{w}, b). w∗,b∗=*argminw,b​ L(w,b). 正态分布与平方损失 正态分布概率密度函数： p(x)=12πσ2exp⁡(−12σ2(x−μ)2).p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right). p(x)=2πσ2​1​exp(−2σ21​(x−μ)2). 通过给定的x\\mathbf{x}x观测到特定yyy的可能性： P(y∣x)=12πσ2exp⁡(−12σ2(y−w⊤x−b)2).P(y \\mid \\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right). P(y∣x)=2πσ2​1​exp(−2σ21​(y−w⊤x−b)2). 根据最大似然估计法，参数w\\mathbf{w}w和bbb的最优值是使整个数据集的可能性最大的值： P(y∣X)=∏i=1np(y(i)∣x(i)).P(\\mathbf y \\mid \\mathbf X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)}). P(y∣X)=i=1∏n​p(y(i)∣x(i)). 改为最小化负对数似然−log⁡P(y∣X)-\\log P(\\mathbf y \\mid \\mathbf X)−logP(y∣X)： −log⁡P(y∣X)=∑i=1n12log⁡(2πσ2)+12σ2(y(i)−w⊤x(i)−b)2.-\\log P(\\mathbf y \\mid \\mathbf X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2. −logP(y∣X)=i=1∑n​21​log(2πσ2)+2σ21​(y(i)−w⊤x(i)−b)2. 现在我们只需要假设σ\\sigmaσ是某个固定常数就可以忽略第一项，因为第一项不依赖于w\\mathbf{w}w和 bbb。现在第二项除了常数1σ2\\frac{1}{\\sigma^2}σ21​外，其余部分和前面介绍的平方误差损失是一样的。 线性回归从零实现 import random import torch def synthetic_data(w, b, num_examples): #生成数据集 &quot;&quot;&quot;生成 y = Xw + b + 噪声。&quot;&quot;&quot; X = torch.normal(0, 1, (num_examples, len(w))) y = torch.matmul(X, w) + b y += torch.normal(0, 0.01, y.shape) return X, y.reshape((-1, 1)) true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = synthetic_data(true_w, true_b, 1000) def data_iter(batch_size, features, labels): #读取数据集 num_examples = len(features) indices = list(range(num_examples)) # 这些样本是随机读取的，没有特定的顺序 random.shuffle(indices) for i in range(0, num_examples, batch_size): batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)]) yield features[batch_indices], labels[batch_indices] # 初始化模型参数 w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True) b = torch.zeros(1, requires_grad=True) def linreg(X, w, b): #定义模型 &quot;&quot;&quot;线性回归模型。&quot;&quot;&quot; return torch.matmul(X, w) + b def squared_loss(y_hat, y): #定义损失函数 &quot;&quot;&quot;均方损失。&quot;&quot;&quot; return (y_hat - y.reshape(y_hat.shape))**2 / 2 def sgd(params, lr, batch_size): #定义优化算法 &quot;&quot;&quot;小批量随机梯度下降。&quot;&quot;&quot; with torch.no_grad(): for param in params: param -= lr * param.grad / batch_size param.grad.zero_() lr = 0.03 num_epochs = 3 net = linreg loss = squared_loss #训练 for epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y) # `X`和`y`的小批量损失 # 因为`l`形状是(`batch_size`, 1)，而不是一个标量。`l`中的所有元素被加到一起， # 并以此计算关于[`w`, `b`]的梯度 l.sum().backward() sgd([w, b], lr, batch_size) # 使用参数的梯度更新参数 with torch.no_grad(): train_l = loss(net(features, w, b), labels) print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}') print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}') print(f'b的估计误差: {true_b - b}') #输出 epoch 1, loss 0.024281 epoch 2, loss 0.000082 epoch 3, loss 0.00004 w的估计误差: tensor([0.0005, 0.0003], grad_fn=&lt;SubBackward0&gt;) b的估计误差: tensor([0.0009], grad_fn=&lt;RsubBackward1&gt;) 线性回归简洁实现 import numpy as np import torch from torch.utils import data from torch import nn def synthetic_data(w, b, num_examples): #生成数据集 &quot;&quot;&quot;生成 y = Xw + b + 噪声。&quot;&quot;&quot; X = torch.normal(0, 1, (num_examples, len(w))) y = torch.matmul(X, w) + b y += torch.normal(0, 0.01, y.shape) return X, y.reshape((-1, 1)) true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = synthetic_data(true_w, true_b, 1000) def load_array(data_arrays, batch_size, is_train=True): #读取数据集 &quot;&quot;&quot;构造一个PyTorch数据迭代器。&quot;&quot;&quot; dataset = data.TensorDataset(*data_arrays) return data.DataLoader(dataset, batch_size, shuffle=is_train) batch_size = 10 data_iter = load_array((features, labels), batch_size) # 定义模型 net = nn.Sequential(nn.Linear(2, 1)) # 初始化模型参数 net[0].weight.data.normal_(0, 0.01) net[0].bias.data.fill_(0) #定义损失函数 loss = nn.MSELoss() #定义优化算法 trainer = torch.optim.SGD(net.parameters(), lr=0.03) #训练 num_epochs = 3 for epoch in range(num_epochs): for X, y in data_iter: l = loss(net(X), y) trainer.zero_grad() l.backward() trainer.step() l = loss(net(features), labels) print(f'epoch {epoch + 1}, loss {l:f}') w = net[0].weight.data print('w的估计误差：', true_w - w.reshape(true_w.shape)) b = net[0].bias.data print('b的估计误差：', true_b - b) #输出 epoch 1, loss 0.000376 epoch 2, loss 0.000101 epoch 3, loss 0.000101 w的估计误差： tensor([ 0.0006, -0.0008]) b的估计误差： tensor([5.4359e-05]) softmax回归 softmax运算 为了将未归一化的预测变换为非负并且总和为1，同时要求模型保持可导。 y^=softmax(o)其中y^j=exp⁡(oj)∑kexp⁡(ok)\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{其中}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)} y^​=softmax(o)其中y^​j​=∑k​exp(ok​)exp(oj​)​ 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，softmax回归是一个线性模型。 交叉熵损失函数及其导数 l(y,y^)=−∑j=1qyjlog⁡exp⁡(oj)∑k=1qexp⁡(ok)=∑j=1qyjlog⁡∑k=1qexp⁡(ok)−∑j=1qyjoj=log⁡∑k=1qexp⁡(ok)−∑j=1qyjoj.\\begin{aligned} l(\\mathbf{y}, \\hat{\\mathbf{y}}) &amp;= - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\ &amp;= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j\\\\ &amp;= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j. \\end{aligned} l(y,y^​)​=−j=1∑q​yj​log∑k=1q​exp(ok​)exp(oj​)​=j=1∑q​yj​logk=1∑q​exp(ok​)−j=1∑q​yj​oj​=logk=1∑q​exp(ok​)−j=1∑q​yj​oj​.​ 考虑相对于任何未归一化的预测ojo_joj​的导数。我们得到： ∂ojl(y,y^)=exp⁡(oj)∑k=1qexp⁡(ok)−yj=softmax(o)j−yj.\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j. ∂oj​​l(y,y^​)=∑k=1q​exp(ok​)exp(oj​)​−yj​=softmax(o)j​−yj​. 换句话说，导数是我们模型分配的概率（由softmax得到）与实际发生的情况（由独热标签向量表示）之间的差异。从这个意义上讲，与我们在回归中看到的非常相似，其中梯度是观测值yyy和估计值y^\\hat{y}y^​之间的差异。 ","link":"https://cisse-away.github.io/post/lesslessdong-shou-xue-shen-du-xue-xi-greatergreater-xian-xing-shen-jing-wang-luo/"},{"title":"《动手学深度学习》——预备知识","content":"参考教程 数据操作 运算 对于任意具有相同形状的张量，常见的标准算术运算符（+、-、*、/ 和 **）都可以被升级为按元素运算。x = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x**y # **运算符是求幂运算 # 输出 (tensor([ 3., 4., 6., 10.]), tensor([-1., 0., 2., 6.]), tensor([ 2., 4., 8., 16.]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1., 4., 16., 64.])) 将多个张量连结在一起X = torch.arange(12, dtype=torch.float32).reshape((3, 4)) Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) # 输出 (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]]), tensor([[ 0., 1., 2., 3., 2., 1., 4., 3.], [ 4., 5., 6., 7., 1., 2., 3., 4.], [ 8., 9., 10., 11., 4., 3., 2., 1.]])) 广播机制 广播机制 （broadcasting mechanism）的工作方式如下 首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状 其次，对生成的数组执行按元素操作 a = torch.arange(3).reshape((3, 1)) b = torch.arange(2).reshape((1, 2)) a + b #它们的形状不匹配。我们将两个矩阵广播为一个更大的 3×2 矩阵，矩阵a将复制列，矩阵b将复制行，然后再按元素相加 # 输出 tensor([[0, 1], [1, 2], [2, 3]]) 线性代数 张量算法基本性质 两个矩阵的按元素乘法称为哈达玛积（Hadamard product）（数学符号 ⊙ ）A⊙B=[a11b11a12b12…a1nb1na21b21a22b22…a2nb2n⋮⋮⋱⋮am1bm1am2bm2…amnbmn]\\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix} a_{11} b_{11} &amp; a_{12} b_{12} &amp; \\dots &amp; a_{1n} b_{1n} \\\\ a_{21} b_{21} &amp; a_{22} b_{22} &amp; \\dots &amp; a_{2n} b_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} b_{m1} &amp; a_{m2} b_{m2} &amp; \\dots &amp; a_{mn} b_{mn} \\end{bmatrix}A⊙B=⎣⎢⎢⎢⎡​a11​b11​a21​b21​⋮am1​bm1​​a12​b12​a22​b22​⋮am2​bm2​​……⋱…​a1n​b1n​a2n​b2n​⋮amn​bmn​​⎦⎥⎥⎥⎤​ A = torch.arange(20, dtype=torch.float32).reshape(5, 4) B = A.clone() A * B # 输出 tensor([[ 0., 1., 4., 9.], [ 16., 25., 36., 49.], [ 64., 81., 100., 121.], [144., 169., 196., 225.], [256., 289., 324., 361.]]) 降维 默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 为了通过求和所有行的元素来降维（轴0），指定axis=0。 因此输入的轴0的维数在输出形状中丢失A_sum_axis1 = A.sum(axis=1) A_sum_axis1, A_sum_axis1.shape # output (tensor([ 6., 22., 38., 54., 70.]), torch.Size([5])) 指定axis=1将通过汇总所有列的元素降维（轴1）。因此，输入的轴1的维数在输出形状中消失A_sum_axis1 = A.sum(axis=1) A_sum_axis1, A_sum_axis1.shape # output (tensor([ 6., 22., 38., 54., 70.]), torch.Size([5])) 非降维求和 ```python sum_A = A.sum(axis=1, keepdims=True) sum_A, sum_A.shape (tensor([[ 6.], [22.], [38.], [54.], [70.]]), torch.Size([5, 1])) ``` 点积 ```python y = torch.ones(4, dtype=torch.float32) x, y, torch.dot(x, y) (tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.)) ``` 向量积 ```python A.shape, x.shape, torch.mv(A, x) (torch.Size([5, 4]), torch.Size([4]), tensor([ 14., 38., 62., 86., 110.])) ``` 矩阵乘法 ```python B = torch.ones(4, 3) torch.mm(A, B) tensor([[ 6., 6., 6.], [22., 22., 22.], [38., 38., 38.], [54., 54., 54.], [70., 70., 70.]]) ``` 范数 L2范数∥x∥2=∑i=1nxi2,\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}, ∥x∥2​=i=1∑n​xi2​​, u = torch.tensor([3.0, -4.0]) torch.norm(u) # output tensor(5.) L1范数∥x∥1=∑i=1n∣xi∣.\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|. ∥x∥1​=i=1∑n​∣xi​∣. torch.abs(u).sum() # output tensor(7.) 微分 梯度 设函数f:Rn→Rf:\\mathbb{R}^n\\rightarrow\\mathbb{R}f:Rn→R的输入是一个nnn维向量x=[x1,x2,…,xn]⊤\\mathbf{x}=[x_1,x_2,\\ldots,x_n]^\\topx=[x1​,x2​,…,xn​]⊤，并且输出是一个标量。 函数f(x)f(\\mathbf{x})f(x)相对于xxx的梯度是一个包含nnn个偏导数的向量: ∇xf(x)=[∂f(x)∂x1,∂f(x)∂x2,…,∂f(x)∂xn]⊤\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top ∇x​f(x)=[∂x1​∂f(x)​,∂x2​∂f(x)​,…,∂xn​∂f(x)​]⊤ 其中∇xf(x)\\nabla_{\\mathbf{x}} f(\\mathbf{x})∇x​f(x)通常在没有歧义时被∇f(x)\\nabla f(\\mathbf{x})∇f(x)取代。 假设xxx为nnn维向量，在微分多元函数时经常使用以下规则: 对于所有A∈Rm×n\\mathbf{A} \\in \\mathbb{R}^{m \\times n}A∈Rm×n，都有∇xAx=A⊤\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top∇x​Ax=A⊤ 对于所有A∈Rn×m\\mathbf{A} \\in \\mathbb{R}^{n \\times m}A∈Rn×m，都有∇xx⊤A=A\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} = \\mathbf{A}∇x​x⊤A=A 对于所有A∈Rn×n\\mathbf{A} \\in \\mathbb{R}^{n \\times n}A∈Rn×n，都有∇xx⊤Ax=(A+A⊤)x\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}∇x​x⊤Ax=(A+A⊤)x ∇x∥x∥2=∇xx⊤x=2x\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}∇x​∥x∥2=∇x​x⊤x=2x 同样，对于任何矩阵X\\mathbf{X}X，我们都有∇X∥X∥F2=2X\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}∇X​∥X∥F2​=2X ","link":"https://cisse-away.github.io/post/lesslessdong-shou-xue-shen-du-xue-xi-greatergreater-yu-bei-zhi-shi/"},{"title":"《动手学深度学习》——前言","content":"参考教程 关键组件 数据 与传统机器学习方法相比，深度学习的一个主要优势是可以处理不同长度的数据。 目标函数 当任务为试图预测数值时，最常见的损失函数是平方误差（squared error），即预测值与实际值之差的平方。 当试图解决分类问题时，最常见的目标函数是最小化错误率，即预测与实际情况不符的样本比例。 有些目标函数（如平方误差）很容易被优化，有些目标（如错误率）由于不可微性或其他复杂性难以直接优化。 在这些情况下，通常会优化替代目标。 各种机器学习问题 监督学习 回归 关注最小化平方误差损失函数 分类 分类问题的常见损失函数被称为交叉熵（cross-entropy） 最常见的类别不一定是你将用于决策的类别，不确定风险的影响远远大于收益。 假设分类器80%确定我们的蘑菇不是死帽蕈。 尽管如此，我们也不会吃它，因为我们不值得冒20%的死亡风险。 换句话说，不确定风险的影响远远大于收益。 因此，我们需要将“预期风险”作为损失函数。 也就是说，我们需要将结果的概率乘以与之相关的收益（或伤害）。 在这种情况下，食用蘑菇造成的损失为 0.2×∞+0.8×0=∞0.2×∞+0.8×0=∞，而丢弃蘑菇的损失为0.2×0+0.8×1=0.80.2×0+0.8×1=0.8 分类可能变得比二元分类、多类分类复杂得多。 例如，有一些分类任务的变体可以用于寻找层次结构，层次结构假定在许多类之间存在某种关系。 因此，并不是所有的错误都是均等的。 我们宁愿错误地分入一个相关的类别，也不愿错误地分入一个遥远的类别，这通常被称为层次分类(hierarchical classification)。 层次结构相关性可能取决于你计划如何使用模型。 例如，响尾蛇和乌梢蛇血缘上可能很接近，但如果把响尾蛇误认为是乌梢蛇可能会是致命的。 因为响尾蛇是有毒的，而乌梢蛇是无毒的。 标记问题 学习预测不相互排斥的类别的问题称为多标签分类（multilabel classification）。例如人们在技术博客上贴的标签，比如“机器学习”、“技术”、“小工具”、“编程语言”、“Linux”、“云计算”、“AWS”。 一篇典型的文章可能会用5-10个标签，因为这些概念是相互关联的。 搜索 在信息检索领域，我们希望对一组项目进行排序。 搜索结果的排序也十分重要，我们的学习算法需要输出有序的元素子集。 序列学习 如果输入的样本之间是连续的，有关系的，我们的模型可能就需要拥有“记忆”功能了。 标记和解析、自动语音识别、文本到语音、机器翻译 无监督学习 聚类（clustering）问题：没有标签的情况下给数据分类 主成分分析（principal component analysis）问题：找到少量的参数来准确地捕捉数据的线性相关属性 因果关系（causality）和概率图模型（probabilistic graphical models）问题：描述观察到的许多数据的根本原因。例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？ 生成对抗性网络（generative adversarial networks）：提供一种合成数据的方法，甚至像图像和音频这样复杂的结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。 与环境互动 不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不再与环境交互。 这里所有学习都是在算法与环境断开后进行的，被称为离线学习（offline learning）。 强化学习 强化学习和环境之间的相互作用 在强化学习问题中，agent 在一系列的时间步骤上与环境交互。 在每个特定时间点，agent 从环境接收一些观察（observation），并且必须选择一个动作（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后 agent 从环境中获得 奖励（reward）。 此后新一轮循环开始，agent 接收后续观察，并选择后续操作，依此类推。 当环境可被完全观察到时，我们将强化学习问题称为马尔可夫决策过程（markov decision process）。 当状态不依赖于之前的操作时，我们称该问题为上下文赌博机（contextual bandit problem）。 当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的多臂赌博机（multi-armed bandit problem）。 ","link":"https://cisse-away.github.io/post/lesslessdong-shou-xue-shen-du-xue-xi-greatergreater-qian-yan/"},{"title":"正则表达式基础","content":"参考课程-蓝桥云课 基本语法 选择 |竖直分隔符表示选择，例如boy|girl 可以匹配boy 或者 girl 数量限定 若在一个模式中不加数量限定符则表示出现一次且仅出现一次 +表示前面的字符必须出现至少一次(1 次或多次)，例如 goo+gle 可以匹配 gooogle，goooogle等 ?表示前面的字符最多出现一次（0 次或 1 次），例如，colou?r，可以匹配 color 或者 colour -*星号代表前面的字符可以不出现，也可以出现一次或者多次（0 次、或 1 次、或多次），例如，0*42可以匹配 42、042、0042、00042 等 范围和优先级 ()圆括号可以用来定义模式字符串的范围和优先级，这可以简单的理解为是否将括号内的模式串作为一个整体 示例1：gr(a|e)y 等价于 gray|grey 示例2：(grand)?father 匹配father 和 grandfather 部分语法 字符 描述 \\ 将下一个字符标记为一个特殊字符、或一个原义字符。 例如 n 匹配字符 n。\\n 匹配一个换行符。序列 \\\\ 匹配 \\ 而 \\( 则匹配 (。 ^ 匹配输入字符串的开始位置。 $ 匹配输入字符串的结束位置。 {n} n 是一个非负整数。匹配确定的 n 次。例如 o{2} 不能匹配 Bob 中的 o，但是能匹配 food 中的两个 o。 {n,} n 是一个非负整数。至少匹配 n 次。例如 o{2,} 不能匹配 Bob 中的 o，但能匹配 foooood 中的所有 o。o{1,} 等价于 o+。o{0,} 则等价于 o*。 {n,m} m 和 n 均为非负整数，其中 n&lt;=m。最少匹配 n 次且最多匹配 m 次。例如，o{1,3} 将匹配 fooooood 中的前三个 o。o{0,1} 等价于 o?。请注意在逗号和两个数之间不能有空格。 * 匹配前面的子表达式零次或多次。例如，zo* 能匹配 z、zo 以及 zoo。* 等价于 {0,}。 + 匹配前面的子表达式一次或多次。例如，zo+ 能匹配 zo 以及 zoo，但不能匹配 z。+ 等价于 {1,}。 ? 匹配前面的子表达式零次或一次。例如，do(es)? 可以匹配 do 或 does 中的 do。? 等价于 {0,1}。 ? 当该字符紧跟在任何一个其他限制符（*，+，?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串 oooo，o+? 将匹配单个 o，而 o+ 将匹配所有 o。 . 匹配除 \\n 之外的任何单个字符。要匹配包括 \\n 在内的任何字符，请使用类似 (.｜\\n) 的模式。 (pattern) 匹配 pattern 并获取这一匹配的子字符串。该子字符串用于向后引用。要匹配圆括号字符，请使用 \\( 和 \\)。 x ｜ y 匹配 x 或 y。例如，“z ｜ food”能匹配 z 或 food。“(z ｜ f)ood”则匹配 zood 或 food。 [xyz] 字符集合（character class）。匹配所包含的任意一个字符。例如，[abc] 可以匹配 plain 中的 a。其中特殊字符仅有反斜线 \\ 保持特殊含义，用于转义字符。其它特殊字符如星号、加号、各种括号等均作为普通字符。脱字符^如果出现在首位则表示负值字符集合；如果出现在字符串中间就仅作为普通字符。连字符 - 如果出现在字符串中间表示字符范围描述；如果出现在首位则仅作为普通字符。 [^xyz] 排除型（negate）字符集合。**匹配未列出的任意字符。**例如，[^abc] 可以匹配 plain 中的 plin。 [a-z] 字符范围。**匹配指定范围内的任意字符。**例如，[a-z] 可以匹配 a 到 z 范围内的任意小写字母字符。 [^a-z] 排除型的字符范围。匹配任何不在指定范围内的任意字符。例如，[^a-z] 可以匹配任何不在 a 到 z 范围内的任意字符。 优先级 优先级从上到下，从左到右依次降低 运算符 说明 \\ 转义符 ()，(?:)，(?=)，[] 括号和中括号 *，+，?，{n}，{n,}，{n,m} 限定符 ^，$，\\ 任何元字符 定位点和序列 ｜ 选择 grep模式匹配命令 基本操作 grep命令用于打印输出文本中匹配的模式串，它使用正则表达式作为模式匹配的条件 支持单中正则表达式引擎： 参数 说明 -E POSIX 扩展正则表达式，ERE -G POSIX 基本正则表达式，BRE -P Perl 正则表达式，PCRE 常用参数 参数 说明 -b 将二进制文件作为文本来进行匹配 -c 统计以模式匹配的数目 -i 忽略大小写 -n 显示匹配文本所在行的行号 -v 反选，输出不匹配行的内容 -r 递归匹配查找 -A n n 为正整数，表示 after 的意思，除了列出匹配行之外，还列出后面的 n 行 -B n n 为正整数，表示 before 的意思，除了列出匹配行之外，还列出前面的 n 行 --color=auto 将输出中的匹配项设置为自动颜色显示 使用正则表达式 使用基本正则表达式，BRE 位置 查找/etc/group文件中以shiyanlou为开头的行grep 'shiyanlou' /etc/group grep '^shiyanlou' /etc/group 数量# 将匹配以'z'开头以'o'结尾的所有字符串 echo 'zero\\nzo\\nzoo' | grep 'z.*o' # 将匹配以'z'开头以'o'结尾，中间包含一个任意字符的字符串 echo 'zero\\nzo\\nzoo' | grep 'z.o' # 将匹配以'z'开头，以任意多个'o'结尾的字符串 echo 'zero\\nzo\\nzoo' | grep 'zo*' 选择# grep默认是区分大小写的，这里将匹配所有的小写字母 echo '1234\\nabcd' | grep '[a-z]' # 将匹配所有的数字 echo '1234\\nabcd' | grep '[0-9]' # 将匹配所有的数字 echo '1234\\nabcd' | grep '[[:digit:]]' # 将匹配所有的小写字母 echo '1234\\nabcd' | grep '[[:lower:]]' # 将匹配所有的大写字母 echo '1234\\nabcd' | grep '[[:upper:]]' # 将匹配所有的字母和数字，包括0-9，a-z，A-Z echo '1234\\nabcd' | grep '[[:alnum:]]' # 将匹配所有的字母 echo '1234\\nabcd' | grep '[[:alpha:]]' 特殊符号及说明： 特殊符号 说明 [:alnum:] 代表英文大小写字母及数字，亦即 0-9，A-Z，a-z [:alpha:] 代表任何英文大小写字母，亦即 A-Z，a-z [:blank:] 代表空白键与 [Tab] 按键两者 [:cntrl:] 代表键盘上面的控制按键，亦即包括 CR，LF，Tab，Del... [:digit:] 代表数字而已，亦即 0-9 [:graph:] 除了空白字节（空白键与 [Tab] 按键）外的其他所有按键 [:lower:] 代表小写字母，亦即 a-z [:print:] 代表任何可以被列印出来的字符 [:punct:] 代表标点符号（punctuation symbol），即：&quot;，'，?，!，;，:，#，$... [:upper:] 代表大写字母，亦即 A-Z [:space:] 任何会产生空白的字符，包括空格键，[Tab]，CR 等等 [:xdigit:] 代表 16 进位的数字类型，因此包括： 0-9，A-F，a-f 的数字与字节 排除字符echo 'geek\\ngood' | grep '[^o]' 当 ^ 放到中括号内为排除字符，否则表示行首。 使用扩展正则表达式，ERE 要通过grep使用扩展正则表达式需要加上-E 参数，或使用egrep 数量# 只匹配&quot;zo&quot; echo 'zero\\nzo\\nzoo' | grep -E 'zo{1}' # 匹配以&quot;zo&quot;开头的所有单词 echo 'zero\\nzo\\nzoo' | grep -E 'zo{1,}' 选择# 匹配&quot;www.shiyanlou.com&quot;和&quot;www.google.com&quot; echo 'www.shiyanlou.com\\nwww.baidu.com\\nwww.google.com' | grep -E 'www\\.(shiyanlou|google)\\.com' # 或者匹配不包含&quot;baidu&quot;的内容 echo 'www.shiyanlou.com\\nwww.baidu.com\\nwww.google.com' | grep -Ev 'www\\.baidu\\.com' sed流编辑器 基本格式sed [参数]... [执行命令] [输入文件]... # 形如： $ sed -i 's/sad/happy/' test # 表示将test文件中的&quot;sad&quot;替换为&quot;happy&quot; 常用参数 参数 说明 -n 安静模式，只打印受影响的行，默认打印输入数据的全部内容 -e 用于在脚本中添加多个执行命令一次执行，在命令行中执行多个命令通常不需要加该参数 -f filename 指定执行 filename 文件中的命令 -r 使用扩展正则表达式，默认为标准正则表达式 -i 将直接修改输入文件内容，而不是打印到标准输出设备 sed编辑器的执行命令 执行命令格式 [n1][,n2]command [n1][~step]command 其中n1,n2表示输入内容的行号，它们之间为,逗号则表示从 n1 到 n2 行，如果为~ 波浪号则表示从 n1 开始以 step 为步进的所有行；command 为执行动作 其中一些命令可以在后面加上作用范围 sed -i 's/sad/happy/g' test # g 表示全局范围 sed -i 's/sad/happy/4' test # 4 表示指定行中的第四个匹配字符串 常用动作指令： | 命令 | 说明 | | ---- | ------------------------------------ | | s | 行内替换 | | c | 整行替换 | | a | 插入到指定行的后面 | | i | 插入到指定行的前面 | | p | 打印指定行，通常与 -n 参数配合使用 | | d | 删除指定行 | awk文本处理语言 介绍 AWK 是一种优良的文本处理工具，Linux 及 Unix 环境中现有的功能最强大的数据处理引擎之一。 基本概念 awk 所有的操作都是基于 pattern(模式)—action(动作)对来完成 pattern {action} 在一个完整 awk 操作中，这两者可以只有其中一个，如果没有 pattern 则默认匹配输入的全部文本，如果没有 action 则默认为打印匹配内容到屏幕 awk命令基本格式awk [-F fs] [-v var=value] [-f prog-file | 'program text'] [file...] 其中 -F参数用于预先指定前面提到的字段分隔符（还有其他指定字段的方式），-v 用于预先为 awk 程序指定变量，-f参数用于指定 awk 命令要执行的程序文件，或者在不加 -f 参数的情况下直接将程序语句放在这里，最后为 awk 需要处理的文本输入，且可以同时输入多个文本文件。 示例vim test #内容 #I like linux #www.shiyanlou.com shiyanlou:~/ $ awk '{ quote&gt; if(NR==1){ quote&gt; print $1 &quot; &quot; $2 &quot; &quot; $3 quote&gt; } else { quote&gt; print} quote&gt; }' test I like linux www.shiyanlou.com awk常用的内置变量 变量名 说明 FILENAME 当前输入文件名，若有多个文件，则只表示第一个。如果输入是来自标准输入，则为空字符串 $0 当前记录的内容 $N N 表示字段号，最大值为NF变量的值 FS 字段分隔符，由正则表达式表示，默认为空格 RS 输入记录分隔符，默认为 \\n，即一行为一个记录 NF 当前记录字段数 NR 已经读入的记录数 FNR 当前输入文件的记录数，请注意它与 NR 的区别 OFS 输出字段分隔符，默认为空格 ORS 输出记录分隔符，默认为 \\n ","link":"https://cisse-away.github.io/post/zheng-ze-biao-da-shi-ji-chu/"},{"title":"数据流重定向","content":"参考课程-蓝桥云课 数据流重定向 常用重定向shiyanlou:~/ $ echo 'hello shiyanlou' &gt; redirect shiyanlou:~/ $ echo 'www.shiyanlou.com' &gt;&gt; redirect shiyanlou:~/ $ cat redirect hello shiyanlou www.shiyanlou.com 简单重定向 将一个文件作为命令的输入，标准输出作为命令的输出： cat Documents/test.c 将 echo 命令通过管道传过来的数据作为 cat 命令的输入，将标准输出作为命令的输出： echo 'hi' | cat 将 echo 命令的输出从默认的标准输出重定向到一个普通文件： echo 'hello shiyanlou' &gt; redirect cat redirect 标准错误重定向 将输出重定向到文件依旧会出现错误信息 cat Documents/test.c hello.c &gt; somefile 使用文件描述符隐藏某些错误或警告 # 将标准错误重定向到标准输出，再将标准输出重定向到文件，注意要将重定向到文件写到前面 cat Documents/test.c hello.c &gt;somefile 2&gt;&amp;1 # 或者只用bash提供的特殊的重定向符号&quot;&amp;&quot;将标准错误和标准输出同时重定向到文件 cat Documents/test.c hello.c &amp;&gt;somefilehell 使用tee命令同时重定向到多个文件 不仅要将输出重定向到文件，也要将信息打印在终端： shiyanlou:~/ $ echo 'hello shiyanlou' | tee hello hello shiyanlou shiyanlou:~/ $ cat hello hello shiyanlou 永久重定向 使用exec命令实现永久重定向。exec命令的作用是使用指定的命令替换当前的 Shell，即使用一个进程替换当前进程，或者指定新的重定向： # 先开启一个子 Shell zsh # 使用exec替换当前进程的重定向，将标准输出重定向到一个文件 exec 1&gt;somefile # 后面执行的命令的输出都将被重定向到文件中，直到退出当前子shell，或取消exec的重定向 ls exit cat somefile 创建输出文件描述符 查看当前Shell进程中打开的文件描述符 cd /dev/fd/;ls -Al 使用exec命令创建新的文件描述符 zsh exec 3&gt;somefile cd /dev/fd/;ls -Al;cd - echo &quot;this is test&quot; &gt;&amp;3 cat somefile exit 关闭文件描述符exec 3&gt;&amp;- cd /dev/fd;ls -Al;cd - 完全屏蔽命令的输出cat Documents/test.c 1&gt;/dev/null 2&gt;&amp;1 使用xargs分割参数列表 当用来处理产生大量输出结果的命令如 find，locate 和 grep 的结果十分有用 将/etc/passwd文件按 : 分割取第一个字段排序后，使用echo命令生成一个列表cut -d: -f1 &lt; /etc/passwd | sort | xargs echo ","link":"https://cisse-away.github.io/post/shu-ju-liu-chong-ding-xiang/"},{"title":"简单的文本处理","content":"参考课程-蓝桥云课 文本处理命令 tr命令 -tr 命令可以用来删除一段文本信息中的某些文字。或者将其进行转换 使用方式 tr [option]...SET1 [SET2] 常用的选项 | 选项 | 说明 | | ---- | ------------------------------------------------------------ | | -d | 删除和 set1 匹配的字符，注意不是全词匹配也不是按字符顺序匹配 | | -s | 去除 set1 指定的在输入文本中连续并重复的字符 | 示例 # 删除 &quot;hello shiyanlou&quot; 中所有的'o'，'l'，'h' $ echo 'hello shiyanlou' | tr -d 'olh' # 将&quot;hello&quot; 中的ll，去重为一个l $ echo 'hello' | tr -s 'l' # 将输入文本，全部转换为大写或小写输出 $ echo 'input some text here' | tr '[:lower:]' '[:upper:]' # 上面的'[:lower:]' '[:upper:]'你也可以简单的写作'[a-z]' '[A-Z]'，当然反过来将大写变小写也是可以的 col命令 col 命令可以将Tab换成对等数量的空格键，或反转这个操作 使用方式 col [option] 常用选项 | 选项 | 说明 | | ---- | ----------------------------- | | -x | 将Tab转换为空格 | | -h | 将空格转换为Tab（默认选项） | 示例 # 查看 /etc/protocols 中的不可见字符，可以看到很多 ^I ，这其实就是 Tab 转义成可见字符的符号 cat -A /etc/protocols # 使用 col -x 将 /etc/protocols 中的 Tab 转换为空格，然后再使用 cat 查看，你发现 ^I 不见了 cat /etc/protocols | col -x | cat -A join命令 将两个文件中包含相同内容的那一行合并在一起 使用方式 join [option]... file1 file2 常用选项 | 选项 | 说明 | | ---- | ---------------------------------------------------- | | -t | 指定分隔符，默认为空格 | | -i | 忽略大小写的差异 | | -1 | 指明第一个文件要用哪个字段来对比，默认对比第一个字段 | | -2 | 指明第二个文件要用哪个字段来对比，默认对比第一个字段 | 示例 cd /home/shiyanlou # 创建两个文件 echo '1 hello' &gt; file1 echo '1 shiyanlou' &gt; file2 join file1 file2 # 将 /etc/passwd 与 /etc/shadow 两个文件合并，指定以':'作为分隔符 sudo join -t':' /etc/passwd /etc/shadow # 将 /etc/passwd 与 /etc/group 两个文件合并，指定以':'作为分隔符，分别比对第4和第3个字段 sudo join -t':' -1 4 /etc/passwd -2 3 /etc/group paste命令 paste这个命令与join命令类似，它是在不对比数据的情况下，简单地将多个文件合并一起，以Tab隔开 使用方式 paste [option] file... 常用选项 | 选项 | 说明 | | ---- | ---------------------------- | | -d | 指定合并的分隔符，默认为 Tab | | -s | 不合并到一行，每个文件为一行 | 示例 echo hello &gt; file1 echo shiyanlou &gt; file2 echo www.shiyanlou.com &gt; file3 paste -d ':' file1 file2 file3 paste -s file1 file2 file3 ","link":"https://cisse-away.github.io/post/jian-dan-de-wen-ben-chu-li/"},{"title":"命令执行顺序控制与管道","content":"参考课程-蓝桥云课 命令执行顺序的控制 顺序执行多条命令 简单的顺序执行可以使用;来完成 有选择的执行命令 通过&amp;&amp;或||来实现选择性执行，&amp;&amp;表示若前面的命令执行结果返回0则而执行后面的，狗则不执行，||相反which cowsay&gt;/dev/null &amp;&amp; echo &quot;exist&quot; || echo &quot;not exist&quot; 管道 使用管道查看目录下大量文件ls -al /etc | less #通过管道将前一个命令(ls)的输出作为下一个命令(less)的输入，然后就可以一行一行地看 cut命令，打印每一行的某一字段 打印/etc/passwd文件中以 : 为分隔符的第 1 个字段和第 6 个字段分别表示用户名和其家目录： cut /etc/passwd -d ':' -f 1,6 打印/etc/passwd文件中每一行的前 N 个字符： # 前五个（包含第五个） cut /etc/passwd -c -5 # 前五个之后的（包含第五个） cut /etc/passwd -c 5- # 第五个 cut /etc/passwd -c 5 # 2 到 5 之间的（包含第五个） cut /etc/passwd -c 2-5 grep命令，在文本中或stdin中查找匹配字符串 grep命令的一般形式：grep [命令选项]... 用于匹配的表达式 [文件]... 查看环境变量中以 &quot;yanlou&quot; 结尾的字符串 export | grep &quot;.*yanlou$&quot; wc命令，简单小巧的计数工具 统计并输出一个文件中行、单词和字节的数目wc /etc/passwd 分别只输出行数、单词数、字节数、字符数和输入文本中最长一行的字节数：# 行数 wc -l /etc/passwd # 单词数 wc -w /etc/passwd # 字节数 wc -c /etc/passwd # 字符数 wc -m /etc/passwd # 最长行字节数 wc -L /etc/passwd 结合管道来统计/etc下面所有目录数：ls -dl /etc/*/ | wc -l sort排序命令 默认为字典排序：cat /etc/passwd | sort 反转排序：cat /etc/passwd | sort -r 按特定字段排序：cat /etc/passwd | sort -t':' -k 3 上面的-t参数用于指定字段的分隔符，这里是以&quot;:&quot;作为分隔符；-k 字段号用于指定对哪一个字段进行排序。这里/etc/passwd文件的第三个字段为数字，默认情况下是以字典序排序的，如果要按照数字排序就要加上-n参数：cat /etc/passwd | sort -t':' -k 3 -n uniq去重命令 过滤重复行history | cut -c 8- | cut -d ' ' -f 1 | sort | uniq #uniq只能去连续重复的行，需要排序才能达到全文去重的效果 输出重复行# 输出重复过的行（重复的只输出一个）及重复次数 history | cut -c 8- | cut -d ' ' -f 1 | sort | uniq -dc # 输出所有重复的行 history | cut -c 8- | cut -d ' ' -f 1 | sort | uniq -D ","link":"https://cisse-away.github.io/post/ming-ling-zhi-xing-shun-xu-kong-zhi-yu-guan-dao/"},{"title":"Linux任务计划crontab","content":"参考课程-蓝桥云课 crontab的使用 crontab简介 crontab 命令从输入设备读取指令，并将其存放于 crontab 文件中，以供之后读取和执行。通常，crontab 储存的指令被守护进程激活，crond 为其守护进程，crond 常常在后台运行，每一分钟会检查一次是否有预定的作业需要执行。 crontab使用 添加计划任务 crontab -e 显示计划任务 crontab -l 删除计划任务 crontab -r crontab的深入 每个用户使用crontab -e添加计划任务，都会在 /var/spool/cron/crontabs中添加一个该用户自己的任务文档，这样目的是为了隔离。 ","link":"https://cisse-away.github.io/post/linux-ren-wu-ji-hua-crontab/"},{"title":"linux下的帮助命令","content":"参考课程-蓝桥云课 内建命令与外部命令 内建命令 内建命令实际上是 shell 程序的一部分，其中包含的是一些比较简单的 Linux 系统命令，这些命令是写在 bash 源码的 builtins 里面的，由 shell 程序识别并在 shell 程序内部完成运行，通常在 Linux 系统加载运行时 shell 就被加载并驻留在系统内存中。而且解析内部命令 shell 不需要创建子进程，因此其执行速度比外部命令快。比如：history、cd、exit 等等。 外部命令 外部命令是 Linux 系统中的实用程序部分，因为实用程序的功能通常都比较强大，所以其包含的程序量也会很大，在系统加载时并不随系统一起被加载到内存中，而是在需要时才将其调入内存。虽然其不包含在 shell 中，但是其命令执行过程是由 shell 程序控制的。外部命令是在 Bash 之外额外安装的，通常放在/bin，/usr/bin，/sbin，/usr/sbin 等等。比如：ls、vi 等。 帮助命令的使用 help命令 help命令只能用于显示内建命令的帮助信息，对于外部命令基本都有参数--help man命令 得到的内容比用help更多更详细，而且man没有内建与外部命令的区分 man手册章节说明 | 章节数 | 说明 | | ------ | --------------------------------------------------- | | 1 | Standard commands （标准命令） | | 2 | System calls （系统调用） | | 3 | Library functions （库函数） | | 4 | Special devices （设备说明） | | 5 | File formats （文件格式） | | 6 | Games and toys （游戏和娱乐） | | 7 | Miscellaneous （杂项） | | 8 | Administrative Commands （管理员命令） | | 9 | 其他（Linux 特定的）， 用来存放内核例行程序的文档。 | info命令 info命令可以显示更完整的GNU工具信息 ","link":"https://cisse-away.github.io/post/linux-xia-de-bang-zhu-ming-ling/"},{"title":"Linux文件系统操作与磁盘管理","content":"参考课程-蓝桥云课 查看磁盘和目录的容量 使用df命令查看磁盘的容量df 可以添加-h参数以更容易看懂的方式显示 使用du命令查看目录的容量du #默认以块的大小展示 du -h du -h -d 0 ~ #只查看1级目录的信息 创建虚拟磁盘 dd命令简介shiyanlou:~/ $ dd of=test bs=10 count=1 shiyanlou 记录了1+0 的读入 记录了1+0 的写出 10 bytes copied, 3.01556 s, 0.0 kB/s shiyanlou:~/ $ du -b test 10 test shiyanlou:~/ $ cat test shiyanlou 上述命令从标准输入设备读入用户输入（缺省值，所以可省略）然后输出到 test 文件，bs（block size）用于指定块大小（缺省单位为 Byte，也可为其指定如 K，M，G 等单位），count 用于指定块数量 dd在拷贝的同时还可以实现数据转换：shiyanlou:~/ $ dd if=/dev/stdin of=test bs=10 count=1 conv=ucase shiyanlou 记录了1+0 的读入 记录了1+0 的写出 10 bytes copied, 2.91177 s, 0.0 kB/s shiyanlou:~/ $ cat test SHIYANLOU 使用dd命令创建虚拟镜像文件dd if=/dev/zero of=virtual.img bs=1M count=256 使用mkfs命令格式化磁盘sudo mkfs.ext4 virtual.img 使用mount命令挂载磁盘到目录树 查看主机已经挂载的文件系统： sudo mount mount命令一般格式： mount [options] [source] [directory] 常用操作： mount [-o [操作选项]] [-t 文件系统类型] [-w|--rw|--ro] [文件系统源] [挂载点] 挂载虚拟磁盘镜像到/mnt目录： mount -o loop -t ext4 virtual.img /mnt # 也可以省略挂载类型，很多时候 mount 会自动识别 # 以只读方式挂载 mount -o loop --ro virtual.img /mnt # 或者 mount -o loop,ro virtual.img /mnt 使用unmount命令卸载已挂载磁盘# 命令格式 sudo umount 已挂载设备名或者挂载点，如： sudo umount /mnt 使用fdisk为磁盘分区 查看硬盘分区表信息： sudo fdisk -l 进入磁盘分区模式 sudo fdisk virtual.img ","link":"https://cisse-away.github.io/post/linux-wen-jian-xi-tong-cao-zuo-yu-ci-pan-guan-li/"},{"title":"Linux文件打包与解压缩","content":"参考课程-蓝桥云课 zip压缩打包程序 使用zip打包文件夹shiyanlou:~/ $ zip -r -q -o shiyanlou.zip /home/shiyanlou/Desktop shiyanlou:~/ $ du -h shiyanlou.zip 12K shiyanlou.zip shiyanlou:~/ $ file shiyanlou.zip shiyanlou.zip: Zip archive data, at least v1.0 to extract 其中-r表示递归打包包含子目录的全部内容，-q参数表示为安静模式，-o表示输出文件，需在其后紧跟打包输出文件名 设置压缩级别为 9 和 1（9 最大，1 最小），重新打包： zip -r -9 -q -o shiyanlou_9.zip /home/shiyanlou/Desktop -x ~/*.zip zip -r -1 -q -o shiyanlou_1.zip /home/shiyanlou/Desktop -x ~/*.zip 注意：这里只能使用绝对路径，否则不起作用 创建加密zip包zip -r -e -o shiyanlou_encryption.zip /home/shiyanlou/Desktop 使用unzip命令解压缩zip文件 解压到当前目录unzip shiyanlou.zip 使用安静模式，将文件解压到指定目录：unzip -q shiyanlou.zip -d ziptest 只查看压缩包的内容：unzip -l shiyanlou.zip 使用-O参数指定编码类型：unzip -O GBK 中文压缩文件.zip tar打包工具 创建一个tar包cd /home/shiyanlou tar -P -cf shiyanlou.tar /home/shiyanlou/Desktop 其中-P保留绝对路径符，-c表示创建一个 tar 包文件，-f用于指定创建的文件名，注意文件名必须紧跟在-f参数之后，比如不能写成tar -fc shiyanlou.tar，可以写成tar -f shiyanlou.tar -c ~。你还可以加上-v参数以可视的的方式输出打包的文件 解包一个文件（-x参数）到指定路径的已存在目录（-C参数）：mkdir tardir tar -xf shiyanlou.tar -C tardir 只查看不解包文件-t参数：tar -tf shiyanlou.tar 在创建 tar 文件的基础上添加-z参数，使用gzip来压缩文件：tar -czf shiyanlou.tar.gz /home/shiyanlou/Desktop 解压*.tar.gz文件：tar -xzf shiyanlou.tar.gz 使用其他压缩工具 压缩文件格式 参数 *.tar.gz -z *.tar.xz -J *tar.bz2 -j ","link":"https://cisse-away.github.io/post/linux-wen-jian-da-bao-yu-jie-ya-suo/"},{"title":"Linux环境变量与文件查找","content":"参考课程-蓝桥云课 环境变量 相关命令 命 令 说 明 set 显示当前 Shell 所有变量，包括其内建环境变量（与 Shell 外观等相关），用户自定义变量及导出的环境变量。 env 显示与当前用户相关的环境变量，还可以让命令在指定环境中运行。 export 显示从 Shell 中导出成环境变量的变量，也能通过它将自定义变量导出为环境变量。 示例shiyanlou:Desktop/ $ declare tmp shiyanlou:Desktop/ $ tmp=shiyanlou shiyanlou:Desktop/ $ echo $tmp shiyanlou shiyanlou:Desktop/ $ zsh #创建子shell，从默认的bash切换到zsh shiyanlou:Desktop/ $ echo $tmp #值为空，表示该变量无效 shiyanlou:Desktop/ $ exit #退出 shiyanlou:Desktop/ $ export tmp #导出tmp为环境变量 shiyanlou:Desktop/ $ zsh shiyanlou:Desktop/ $ echo $tmp #变量有效 shiyanlou 按变量的生存周期划分，Linux变量可分为两类： 永久的：需要修改配置文件，变量永久生效 临时的：使用 export 命令行声明即可，变量在关闭 shell 时失效 两个重要文件 /etc/bashrc(有的Linux没有这个文件)：存放shell变量 /etc/profile：存放shell变量和环境变量，对所有用户永久生效 注意：没有用户目录下的隐藏文件.profile只对当前用户永久生效 添加自定义路径到“PATH”环境变量 添加自定义路径： PATH=$PATH:/home/shiyanlou/mybin 每次启动Shell时自动添加自定义路径到PATH 在每个用户的 home 目录中有一个 Shell 每次启动时会默认执行一个配置脚本，以初始化环境，包括添加一些用户自定义环境变量等等。实验楼的环境使用的 Shell 是 zsh，它的配置文件是 .zshrc，相应的如果使用的 Shell 是 Bash，则配置文件为 .bashrc。 echo &quot;PATH=$PATH:/home/shiyanlou/mybin&quot; &gt;&gt; .zshrc 修改和删除已有变量 变量修改 变量设置方式 说明 ${变量名#匹配字串} 从头向后开始匹配，删除符合匹配字串的最短数据 ${变量名##匹配字串} 从头向后开始匹配，删除符合匹配字串的最长数据 ${变量名%匹配字串} 从尾向前开始匹配，删除符合匹配字串的最短数据 ${变量名%%匹配字串} 从尾向前开始匹配，删除符合匹配字串的最长数据 ${变量名/旧的字串/新的字串} 将符合旧字串的第一个字串替换为新的字串 ${变量名//旧的字串/新的字串} 将符合旧字串的全部字串替换为新的字串 变量删除 使用unset命令删除一个环境变量unset mypath 让环境变量立即生效 在Shell中修改了一个配置脚本文件之后，每次都要退出终端重新打开甚至重启主机之后其才能生效。可以使用source命令来让其立即生效：cd /home/shiyanlou source .zshrc source命令别名.，所以上条命令可替换成：. ./.zshrc #后面的文件必须指定完整的绝对或相对路径名，source则不需要 搜索文件 whereis简单快速shiyanlou:Desktop/ $ whereis who who: /usr/bin/who /usr/share/man/man1/who.1.gz whereis只能搜索二进制文件(-b)，man帮助文件(-m)和源代码文件(-s) locate快而全 这个命令不是内置命令，在部分环境中需要手动安装，然后执行更新sudo apt-get update sudo apt-get install locate sudo updatedb 查找usr/share/下所有jpg文件：locate /usr/share/*.jpg 若想只统计数目可以加上-c参数，-i参数可以忽略大小写进行查找，whereis的-b、-m和-s同样可以用 which小而精 which 本身是 Shell 内建的一个命令，我们通常使用which来确定是否安装了某个指定的程序，因为它只从PATH环境变量指定的路径中去搜索命令并且返回第一个搜索到的结果 shiyanlou:Desktop/ $ which man /usr/bin/man shiyanlou:Desktop/ $ which nginx /usr/sbin/nginx shiyanlou:Desktop/ $ which ping /bin/ping find精而细 find应该是这几个命令中最强大的了，它不但可以通过文件类型、文件名进行查找而且可以根据文件的属性（如文件的时间戳，文件的权限等）进行搜索 基本命令格式find [path][option] [action] ","link":"https://cisse-away.github.io/post/linux-huan-jing-bian-liang-yu-wen-jian-cha-zhao/"},{"title":"C++学习笔记","content":"课程链接 C++语言程序设计基础(2021春) C++语言程序设计进阶(2021春) 第一章 绪论 C++支持的程序设计方法 面向过程的程序设计方法 面向对象的程序设计方法 面向过程： ​ 优点：性能比面向对象高，因为类调用时需要实例化，开销比较大，比较消耗资源;比如单片机、嵌入式开发、 Linux/Unix等一般采用面向过程开发，性能是最重要的因素 ​ 缺点：没有面向对象易维护、易复用、易扩展 面向对象： ​ 优点：易维护、易复用、易扩展，由于面向对象有封装、继承、多态性的特性，可以设计出低耦合的系统，使系统 更加灵活、更加易于维护 ​ 缺点：性能比面向过程低 泛型程序设计方法（写的代码可以被很多不同类型的对象所重用，一般由继承实现） C++程序的开发过程 算法设计 源程序编辑 编译 连接：连接目标程序以及库中的某些文件，生成可执行文件 运行调试 三种不同类型的翻译程序 汇编程序：将汇编语言源程序翻译成目标程序 编译程序：将高级语言源程序翻译成目标程序 解释程序：将高级语言源程序翻译成机器指令，边翻译边执行 第二章 C++简单程序设计 各基本类型的取值范围 符号常量 常量定义语句形式：const 数据类型 常量名 = 常量值 符号常量定义时一定要初始化，在程序中间不能改变其值 逗号运算和逗号表达式 格式：表达式1，表达式2 求解顺序及结果：先求表达式1，再求表达式2，最终结果为表达式2的值 枚举类型 语法形式：enum &lt;枚举类型名&gt; {变量值列表} // 例 enum Weekday{SUN, MON, TUE, WED, THU, FRI, SAT}; // 默认情况下：SUN=0,MON=1, ......, SAT=6 注意：整数值不能直接赋给枚举变量，要进行强制类型转换，枚举值可以赋给整型变量 示例程序：#include &lt;iostream&gt; using namespace std; enum GameResult {WIN, LOSE, TIE, CANCEL}; int main() { GameResult result; enum GameResult omit = CANCEL; for (int count = WIN; count &lt;= CANCEL; count++) { result = GameResult(count); if (result == omit) cout &lt;&lt; &quot;The game was cancelled&quot; &lt;&lt; endl; else { cout &lt;&lt; &quot;The game was played &quot;; if (result == WIN) cout &lt;&lt; &quot;and we won!&quot;; if (result == LOSE) cout &lt;&lt; &quot;and we lost.&quot;; cout &lt;&lt; endl; } } return 0; } /* 输出： The game was played and we won! The game was played and we lost. The game was played The game was cancelled */ auto 类型与 decltype类型 auto：编译器通过初始值自动判断变量的类型 decltype：定义一个变量与某一表达式的类型相同，但并不用该表达式初始换变量// sum的类型就是函数f返回值的类型 decltype(f()) sum = x; 为了解决复杂的类型声明而使用的关键字 可以作用于变量、表达式及函数名 第三章 函数 函数的参数传递 在函数调用时才分配形参的存储单元 值传递是单项传递，引用传递可实现双向传递 常引用作参数可以保障实参数据的安全 一般引用作形参： 不用作参数传递（实参赋值给形参），节省开销，提高效率。 函数可以改变形参的值 实参不能是常量 常引用作形参： 不用作参数传递（实参赋值给形参），节省开销，提高效率。 函数不能改变形参的值 实参可以是常量 引用的概念 定义一个引用时，必须同时对它进行初始化，使它指向一个已存在的对象 一旦一个引用被初始化后，就不能改为指向其他对象 含有可变参数的函数 initializer_listinitializer_list &lt;变量类型&gt; 变量名 initializer_list是一个类模板 其对象中的元素永远是常量值，无法改变元素的值 内联函数 在函数声明前加上关键字incline 编译时在调用出用函数体进行替换，节省了参数传递，控制转移等开销 内联函数体内不能有循环语句和switch语句 内联函数的定义必须出现在内联函数第一次被调用之前 对内联函数不能进行异常接口声明 constexpr函数 语法规定： constexpr修饰的函数在其所有参数都是constexpr时，一定返回constexpr; 函数体中必须有且只有一条return语句 示例：constexpr int get_size() {return 20;} constexpr int foo = get_size(); 带默认参数值的函数 默认参数值的说明次序：有默认参数的形参必须列在形参列表的最右 默认参数值与函数的调用位置：若一个函数有原型声明，且原型声明在定义之前，则默认参数值应在函数原型声明中给出；若只有函数的定义，或函数定义在前，则默认参数值可以函数定义中给出 函数重载 功能相近的函数在相同的作用域内衣相同函数名声明 示例：// 形参类型不同 int add(int x, int y); float add(float x, float y); // 形参个数不同 int add(int x, int y); int add(int x, int y, int z); 第四章 类与对象 如果紧跟在类名称的后面声明私有成员，则关键字private可以省略 private类型与protect类型 private修饰的成员变量只有类内可直接访问 protect修饰的成员变量类内和子类均可直接访问 默认构造函数 调用时可以不需要实参的构造函数 参数表为空的构造函数 全部参数都有默认值的构造函数 示例：// 下面两个函数若在类中同时出现将产生编译错误 Clock(); Clock(int newH = 0, int newM = 0, int newS = 0); 委托构造函数 类中往往有多个构造函数，只是参数表和初始化列表不同，其初始化算法都是相同的， 这时，为了避免代码重复，可以使用委托构造函数。 示例：// 构造函数 Clock(int newH, int newM, int newS): hour(newH), minute(newM), second(newS){ } // 委托构造函数 Clock(): Clock(0, 0, 0) {} 复制构造函数 其形参为本类的对象引用 作用：用一个已存在的对象去初始化同类型的新对象 示例：class 类名 { public : 类名（形参）；//构造函数 类名（const 类名 &amp;对象名）；//复制构造函数 // ... }； 类名::类（ const 类名 &amp;对象名）//复制构造函数的实现 { 函数体 } 复制构造函数被调用的情况 定义一个对象时，已本类另一个对象作为初始值，发生复制构造 若函数的形参是类的对象，调用函数时，将使用实参对象初始化形参对象，发生复制构造 如果函数的返回值是类的对象，函数执行完成返回主调函数时，将使用return语句中的对象初始化一个临时无名对象传递给主调函数，此时发生复制构造 前向引用声明 若需要在某个类的声明之前引用该类，则应进行前向引用声明 在提供一个完整的类声明之前，不能声明该类的对象，也不能在内联成员函数中使用该类的对象 当使用前向引用声明时，只能使用被声明的符号，而不能设计类的任何细节 示例：// 正确示例 class B; //前向引用声明 class A { public: void f(B b); }; class B { public: void g(A a); }; //----------------------- // 错误示例 class Fred; //前向引用声明 class Barney { Fred x; //错误：类Fred的声明尚不完善 }; class Fred { Barney y; }; // 应改为: class Fred; //前向引用声明 class Barney { Fred *x; }; class Fred { Barney y; }; 结构体与类 类的缺省访问权限时private，结构体的缺省访问权限是public 结构体主要用来保存数据、没有什么操作的类型 联合体 特点： 成员共用同一组内存单元 任何两个成员不会同时有效 示例：union Mark { //表示成绩的联合体 char grade; //等级制的成绩 bool pass; //只记是否通过课程的成绩 int percent; //百分制的成绩 }; 枚举类 语法形式：enum class 枚举类型名: 底层类型 {枚举值列表}; 示例：enum class Type { General, Light, Medium, Heavy}; enum class Type: char { General, Light, Medium, Heavy}; enum class Category { General=1, Pistol, MachineGun, Cannon}; 常与switch, case一同使用 第五章 数据的共享与保护 对象的生存期 静态生存期 与程序的运行期相同 在文件作用域中声明的对象具有这种生存期 在函数内部声明静态生存期对象，要用关键字static 动态生存期 块作用域中声明的，没有用static修饰的对象是动态生存期的对象（习惯称局部生 存期对象） 开始于程序执行到声明点时，结束于命名该标识符的作用域结束处 类的静态数据成员 用关键字static声明 为该类的所有对象共享 必须在类外定义和初始化，用(::)来指明所属的类 类的静态函数成员 类外代码可以使用类名和作用域操作符来调用静态成员函数 静态成员函数主要用于处理该类的静态数据成员，可以直接调用静态成员函数 如果访问非静态成员，要通过对象来访问 类的友元 友元是C++提供的一种破坏数据封装和数据隐藏的机制 通过将一个模块声明为另一个模块的友元，一个模块能够引用到另一个模块中本是被隐藏的信息 可以使用友元函数和友元类 友元函数 友元函数在它的函数体中能够通过对象名访问private和protected成员 作用：增加灵活性，使程序员可以在封装和快速性方面做合理选择 访问对象中的成员必须通过对象名 友元函数不是成员函数 友元类 若一个类为另一个类的友元，则此类的所有成员都能访问对方类的私有成员 类的友元关系是单向的，若声明B类是A类的友元，B类的成员函数就可以访问A类的私有和保护数据，但A类的成员函数却不能访问B类的私有、保护数据 示例：class A { friend class B; public: void display() { cout &lt;&lt; x &lt;&lt; endl; } private: int x; }; class B { public: void set(int i); void display(); private: A a; }; void B::set(int i) { a.x=i; } void B::display() { a.display(); } 常类型 常对象：必须进行初始化,不能被更新const 类名 对象名 常成员：用const进行修饰的类成员：常数据成员和常函数成员 常成员函数不更新对象的数据成员 常成员函数声明格式：类型说明符 函数名（参数表）const;//在实现部分也要带const关键字 通过常对象只能调用它的常成员函数 常引用：被引用的对象不能被更新const 类型说明符 &amp;引用名 常数组：数组元素不能被更新类型说明符 const 数组名[大小] 常指针：指向常量的指针 C++程序的一般组织结构 类声明文件(.h文件) 类实现文件(.cpp文件) 类的使用文件(main()所在的.cpp文件) 条件编译指令 #if 常量表达式1 程序正文1 //当“ 常量表达式1”非零时编译 #elif 常量表达式2 程序正文2 //当“ 常量表达式2”非零时编译 #else 程序正文3 //其他情况下编译 #endif #ifdef 标识符 程序段1 #else 程序段2 #endif // 如果“标识符”经#defined定义过，且未经undef删除，则编译程序段1；否则编译程序段2 #ifndef 标识符 程序段1 #else 程序段2 #endif // 如果“标识符”未被定义过，则编译程序段1；否则编译程序段2 第六章 数组、指针与字符串 整数0可以赋给指针，表示空指针 允许定义或声明指向void类型的指针，该指针可以被赋予任何类型对象的地址 C++11使用nullptr关键字，是表达更准确，类型安全的空指针 指向常量的指针 不能通过指向常量的指针改变所指对象的值，但指针本身可以改变，可以指向其他的对象 指针类型的常量 若声明指针常量，则指针本身的值不能被改变 指针类型的函数 注意不要将非静态局部地址用作函数的返回值，例如：int* function(){ int local=0; //非静态局部变量作用域和寿命都仅限于本函数体内 return &amp;local; }//函数运行结束时，变量local被释放 指向函数的指针 示例：#include &lt;iostream&gt; using namespace std; int compute(int a, int b, int(*func)(int, int)) { return func(a, b);} int max(int a, int b) // 求最大值 { return ((a &gt; b) ? a: b);} int min(int a, int b) // 求最小值 { return ((a &lt; b) ? a: b);} int sum(int a, int b) // 求和 { return a + b;} int main(){ int a, b, res; cout &lt;&lt; &quot;请输入整数a：&quot;; cin &gt;&gt; a; cout &lt;&lt; &quot;请输入整数b：&quot;; cin &gt;&gt; b; res = compute(a, b, &amp; max); cout &lt;&lt; &quot;Max of &quot; &lt;&lt; a &lt;&lt; &quot; and &quot; &lt;&lt; b &lt;&lt; &quot; is &quot; &lt;&lt; res &lt;&lt; endl； res = compute(a, b, &amp; min); cout &lt;&lt; &quot;Min of &quot; &lt;&lt; a &lt;&lt; &quot; and &quot; &lt;&lt; b &lt;&lt; &quot; is &quot; &lt;&lt; res &lt;&lt; endl; res = compute(a, b, &amp; sum); cout &lt;&lt; &quot;Sum of &quot; &lt;&lt; a &lt;&lt; &quot; and &quot; &lt;&lt; b &lt;&lt; &quot; is &quot; &lt;&lt; res &lt;&lt; endl;} C++11 的智能指针 unique_ptr ：不允许多个指针共享资源，可以用标准库中的move函数转移指针 shared_ptr ：多个指针共享资源 weak_ptr ：可复制shared_ptr，但其构造或者释放对资源不产生影响 移动构造 第七章 继承与派生 单继承时派生类的定义class 派生类名：继承方式 基类名{ 成员声明；} 多继承时派生类的定义class 派生类名：继承方式1 基类名1，继承方式2 基类名2，...{ 成员声明； } // 注意：每一个“继承方式”，只用于限制对紧随其后之基类的继承 三种继承方式 公有继承 继承的访问控制 基类的public和protected成员：访问属性在派生类中保持不变 基类的private成员：不可直接访问 访问权限 派生类中的成员函数：可以直接访问基类中的public和protected成员，但不能直接访问基类的private成员 通过派生类的对象：只能访问public成员 私有继承 继承的访问控制 基类的public和protected成员：都以private身份出现在派生类中 基类的private成员：不可直接访问 访问权限 派生类中的成员函数：可以直接访问基类中的public和protected成员，但不能直接访问基类的private成员 通过派生类的对象：不能直接访问从基类继承的任何成员 保护继承 继承的访问控制 基类的public和protected成员：都以protected身份出现在派生类中 基类的private成员：不可直接访问 访问权限 派生类中的成员函数：可以直接访问基类中的public和protected成员，但不能直接访问基类的private成员 通过派生类的对象：不能直接访问从基类继承的任何成员 protected成员的特点与作用 对建立其所在类对象的模块来说，它与private成员的性质相同 对于其派生类来说，它与public成员的性质相同 既实现了数据隐藏，又方便继承，实现代码重用 类型转换 公有派生类对象可以被当作基类的对象使用，反之则不可 派生类的对象可以隐含转换为基类对象 派生类的对象可以初始化基类的引用 派生类的指针可以隐含转换为基类的指针 通过基类对象名、指针只能使用从基类继承的成员 派生类的构造函数 单继承时构造函数的定义语法派生类名::派生类名(基类所需的形参，本类成员所需的形参): 基类名(参数表), 本类成员初始化列表{ //其他初始化； }； 多继承时构造函数的定义语法派生类名::派生类名(参数表) : 基类名1(基类1初始化参数表), 基类名2(基类2初始化参数表), ... 基类名n(基类n初始化参数表), 本类成员初始化列表 { //其他初始化； }； 构造函数的执行顺序 调用基类构造函数 顺序按照它们被继承时声明的顺序（从左向右） 对初始化列表中的成员进行初始化 顺序按照它们在类中定义的顺序 对象成员初始化时自动调用其所属类的构造函数。由初始化列表提供参数 执行派生类的构造函数体中的内容 访问从基类继承的成员 作用域限定 当派生类与基类中有相同成员时： 若未特别限定，则通过派生类对象使用的是派生类中的同名成员 如要通过派生类对象访问基类中被隐藏的同名成员，应使用基类名和作用域操作符 （::）来限定 二义性问题 如果从不同基类继承了同名成员，但是在派生类中没有定义同名成员，“派生类对 象名或引用名.成员名”、“派生类指针-&gt;成员名”访问成员存在二义性问题 解决方式：用类名限定 虚基类 需要解决的问题 当派生类从多个基类派生，而这些基类又共同基类，则在访问此共同基类中 的成员时，将产生冗余，并有可能因冗余带来不一致性 虚基类声明 以virtual说明基类继承方式 例：class B1:virtual public B 作用 主要用来解决多继承时可能发生的对同一基类继承多次而产生的二义性问题 为最远的派生类提供唯一的基类成员，而不重复产生多次复制 注意：在第一级继承时就要将共同基类设计为虚基类 示例：#include &lt;iostream&gt; using namespace std; class Base0 { public: int var0; void fun0() { cout &lt;&lt; &quot;Member of Base0&quot; &lt;&lt; endl; } }; class Base1: virtual public Base0 { public: int var1; }; class Base2: virtual public Base0 { public: int var2; }; class Derived: public Base1, public Base2 { //定义派生类Derived public: int var; void fun() { cout &lt;&lt; &quot;Member of Derived&quot; &lt;&lt; endl; } }; int main() { Derived d; d.var0 = 2; //直接访问虚基类的数据成员 d.fun0(); //直接访问虚基类的函数成员 return 0; } 第八章 多态性 运算符重载为成员函数 重载为类成员的运算符函数定义形式函数类型 operator 运算符（形参）{ ......}// 参数个数=原操作数个数-1 （后置++、--除外） 双目运算符重载规则 如果要重载 B 为类成员函数，使之能够实现表达式oprd1 B oprd2，其中oprd1为A 类对象，则 B 应被重载为 A 类的成员函数，形参类型应该是 oprd2 所属的类 型 经重载后，表达式oprd1 B oprd2 相当于 oprd1.operator B(oprd2) 前置单目运算符重载规则 如果要重载U为类成员函数，使之能够实现表达式 U oprd，其中 oprd 为A类对 象，则U应被重载为 A 类的成员函数，无形参 经重载后， 表达式U oprd相当于 oprd.operator U() 后置单目运算符 ++和--重载规则 如果要重载 ++或--为类成员函数，使之能够实现表达式 oprd++ 或oprd-- ，其 中 oprd 为A类对象，则++或-- 应被重载为 A 类的成员函数，且具有一个int 类 型形参 经重载后，表达式oprd++ 相当于oprd.operator ++(0) 示例：(++i与i++区别原理)class Clock {//时钟类定义 public: Clock(int hour = 0, int minute = 0, int second = 0); void showTime() const; //前置单目运算符重载 Clock&amp; operator ++ (); //后置单目运算符重载 Clock operator ++ (int); private: int hour, minute, second; }; Clock::Clock(int hour, int minute, int second) { if (0 &lt;= hour &amp;&amp; hour &lt; 24 &amp;&amp; 0 &lt;= minute &amp;&amp; minute &lt; 60 &amp;&amp; 0 &lt;= second &amp;&amp; second &lt; 60) { this-&gt;hour = hour; this-&gt;minute = minute; this-&gt;second = second; } else cout &lt;&lt; &quot;Time error!&quot; &lt;&lt; endl; } void Clock::showTime() const { //显示时间 cout &lt;&lt; hour &lt;&lt; &quot;:&quot; &lt;&lt; minute &lt;&lt; &quot;:&quot; &lt;&lt; second &lt;&lt; endl; } Clock &amp; Clock::operator ++ () { second++; if (second &gt;= 60) { second -= 60; minute++; if (minute &gt;= 60) { minute -= 60; hour = (hour + 1) % 24; } } return *this; } Clock Clock::operator ++ (int) { //注意形参表中的整型参数 Clock old = *this; ++(*this); //调用前置“++”运算符 return old; } 运算符重载为非成员函数 运算符重载为非成员函数的规则 双目运算符 B重载后， 表达式oprd1 B oprd2 等同于operator B(oprd1, oprd2) 前置单目运算符 B重载后， 表达式B oprd等同于operator B(oprd ) 后置单目运算符++和--重载后， 表达式oprd B 等同于operator B(oprd, 0) 示例：class Complex { public: Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) { } friend Complex operator+(const Complex &amp;c1, const Complex &amp;c2)； friend Complex operator-(const Complex &amp;c1, const Complex &amp;c2)； friend ostream &amp; operator&lt;&lt;(ostream &amp;out, const Complex &amp;c)； private: double real; //复数实部 double imag; //复数虚部 }; Complex operator+(const Complex &amp;c1, const Complex &amp;c2){ return Complex(c1.real+c2.real, c1.imag+c2.imag); } Complex operator-(const Complex &amp;c1, const Complex &amp;c2){ return Complex(c1.real-c2.real, c1.imag-c2.imag); } ostream &amp; operator&lt;&lt;(ostream &amp;out, const Complex &amp;c){ out &lt;&lt; &quot;(&quot; &lt;&lt; c.real &lt;&lt; &quot;, &quot; &lt;&lt; c.imag &lt;&lt; &quot;)&quot;; return out; } 虚函数 虚函数时实现运行时多态性基础 构造函数不能时虚函数，析构函数可以是虚函数 虚函数的声明：virtual 函数类型 函数名（形参表）; 虚函数声明只能出现在类定义中的函数原型声明中，而不能在成员函数实现的时候 在派生类中可以对基类中的成员函数进行覆盖 派生类可以不显式地用virtual声明虚函数，系统判断条件为： 该函数是否与基类的虚函数有相同的名称、参数个数及对应参数类型 该函数是否与基类的虚函数有相同的返回值或者满足类型兼容规则的指针、 引用型的返回值 若派生类从名称、参数及返回值三个方面满足上述条件就会自动确定为虚函数 虚析构函数使用场景： 可能通过基类指针删除派生类对象 如果你打算允许其他人通过基类指针调用对象的析构函数（通过delete这样做是正 常的），就需要让基类的析构函数成为虚函数，否则执行delete的结果是不确定的 虚表 每个多态类有一个虚表，其中有当前类的各个虚函数的入口地址 虚表示意图 抽象类 纯虚函数 纯虚函数是一个在基类中声明的虚函数，它在该基类中没有定义具体的操作内容， 要求各派生类根据实际需要定义自己的版本 纯虚函数的声明格式为：virtual 函数类型 函数名(参数表) = 0; 抽象类 带有纯虚函数的类称为抽象类:class 类名{ virtual 类型 函数名(参数表)=0; //其他成员……} 注意： 抽象类只能作为基类来使用 不能定义抽象类的对象 C++11：override与final override 多态行为的基础：基类声明虚函数，继承类声明一个函数覆盖该虚函数 覆盖要求： 函数签名（函数名、参数列表、const ）完全一致 final 用来避免类被继承，或是基类的函数被改写 示例：struct Base1 final { }; struct Derived1 : Base1 { }; // 编译错误：Base1为final，不允许被继承 第九章 模板与群体数据 函数模板定义语法 语法形式：template &lt;模板参数表&gt;函数定义 模板参数表的内容 类型参数：class（或typename）标识符 常量参数：类型说明符 标识符 模板参数：template &lt;参数表&gt; class 标识符 类模板 类模板的声明： 类模板：template &lt;模板参数表&gt;class 类名 {类成员声明}; 若需要在类模板以外定义其成员函数，则要采用以下的形式：template &lt;模板参数表&gt;类型名 类名&lt;模板参数标识符列表&gt;::函数名（参数表） 第十章 泛型程序设计与C++标准模板库 泛型程序设计的基本概念 编写不依赖于具体数据类型的程序 将算法从特定的数据结构中抽象出来，成为通用的 C++的模板为泛型程序设计奠定了关键的基础 STL的基本组件 容器(container) 基本容器类模板 顺序容器 array（数组）、vector（向量）、deque（双端队列）、forward_list（单链表）、list（列表） (有序)关联容器 set（集合）、multiset（多重集合）、map（映射）、multimap（多重映射） 无序关联容器 unordered_set （无序集合）、unordered_multiset（无序多重集合） unordered_map（无序映射）、unorder_multimap（无序多重映射） 容器适配器 ：stack（栈）、queue（队列）、priority_queue（优先队列） 迭代器 函数对象 算法 第十一章 流类库与输入/输出 第十二章 异常处理 异常处理的语法 示例： #include &lt;iostream&gt; using namespace std; int divide(int x, int y) { if (y == 0) throw x; return x / y; } int main() { try { cout &lt;&lt; &quot;5 / 2 = &quot; &lt;&lt; divide(5, 2) &lt;&lt; endl; cout &lt;&lt; &quot;8 / 0 = &quot; &lt;&lt; divide(8, 0) &lt;&lt; endl; cout &lt;&lt; &quot;7 / 1 = &quot; &lt;&lt; divide(7, 1) &lt;&lt; endl; } catch (int e) { cout &lt;&lt; e &lt;&lt; &quot; is divided by zero!&quot; &lt;&lt; endl; } cout &lt;&lt; &quot;That is ok.&quot; &lt;&lt; endl; return 0; } // 输出 5 / 2 = 28 is divided by zero! That is ok. 自动析构 找到到一个匹配的catch异常处理后 初始化异常参数 将从对应的try块开始到异常被抛掷处之间构造（且尚未析构）的所有自动 对象进行析构 从最后一个catch处理之后开始恢复执行 C++标准库各种异常类所代表的异常 ","link":"https://cisse-away.github.io/post/cxue-xi-bi-ji/"},{"title":"Python学习笔记","content":"课程链接 Python教程 Python基础 整数 对于很大的数，例如10000000000，很难数清楚0的个数。Python允许在数字中间以_分隔，即10_000_000_000和10000000000是一样的 浮点数 对于很大或很小的浮点数，就必须用科学计数法表示，把10用e替代，1.23×1091.23×10^91.23×109就是1.23e9，或者12.3e8，0.000012可以写成1.2e-5 字符串 若字符串内部既包含'又包含&quot;则可以用转义字符\\来标识，如I'm &quot;OK&quot;!可以写成：'I\\'m \\&quot;OK\\&quot;!' 若字符串里面有很多字符都需要转义，为了简化，Python还允许用r''表示''内部的字符串默认不转义:&gt;&gt;&gt; print('\\\\\\t\\\\') \\ \\ &gt;&gt;&gt; print(r'\\\\\\t\\\\') \\\\\\t\\\\ Python中的两种除法 /除法计算结果是浮点数，即使两个整数整除 //除法只取结果的整数部分 字符编码 Python提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符：&gt;&gt;&gt; ord('A') 65 &gt;&gt;&gt; chr(66) 'B' 以Unicode表示的str通过encode()方法可以编码为指定的bytes:&gt;&gt;&gt; 'ABC'.encode('ascii') b'ABC' &gt;&gt;&gt; '中文'.encode('utf-8') b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' 相反，从网络或磁盘上读取了的字节流数据就是bytes。要把bytes变为str，就需要用decode()方法:&gt;&gt;&gt; b'ABC'.decode('ascii') 'ABC' &gt;&gt;&gt; b'\\xe4\\xb8\\xad\\xe6\\x96\\x87'.decode('utf-8') '中文' Python源代码文件头：#!/usr/bin/env python3 # -*- coding: utf-8 -*- 第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序，Windows系统会忽略这个注释；第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码 格式化 使用% ：&gt;&gt;&gt; 'Hello, %s' % 'world' 'Hello, world' &gt;&gt;&gt; 'Hi, %s, you have $%d.' % ('Michael', 1000000) 'Hi, Michael, you have $1000000.' 使用format()函数 ：&gt;&gt;&gt; 'Hello, {0}, 成绩提升了 {1:.1f}%'.format('小明', 17.125) 'Hello, 小明, 成绩提升了 17.1%' 使用f-string :&gt;&gt;&gt; r = 2.5 &gt;&gt;&gt; s = 3.14 * r ** 2 &gt;&gt;&gt; print(f'The area of a circle with radius {r} is {s:.2f}') The area of a circle with radius 2.5 is 19.62 有序列表list与tuple list可以随时添加，删除和更改元素；tuple一经初始化就不能修改&gt;&gt;&gt; classmates = ['Michael', 'Bob', 'Tracy'] # list &gt;&gt;&gt; classmates = ('Michael', 'Bob', 'Tracy') # tuple range()函数 生成一个整数序列，再通过list()函数转换为list&gt;&gt;&gt; list(range(5)) [0, 1, 2, 3, 4] dict字典类型 使用键-值(key-value)存储&gt;&gt;&gt; d = {'Michael': 95, 'Bob': 75, 'Tracy': 85} &gt;&gt;&gt; d['Michael'] 95 set集合类型 set也是一组key的集合，但不存储value，且key不能重复：&gt;&gt;&gt; s = set([1, 1, 2, 2, 3, 3]) &gt;&gt;&gt; s {1, 2, 3} set可进行交集、并集等操作：&gt;&gt;&gt; s1 = set([1, 2, 3]) &gt;&gt;&gt; s2 = set([2, 3, 4]) &gt;&gt;&gt; s1 &amp; s2 {2, 3} &gt;&gt;&gt; s1 | s2 {1, 2, 3, 4} 函数 空函数 pass语句什么都不做，可用来作为占位符让程序先跑起来 参数检查 数据类型检查可以使用内置函数isinstance()实现：def my_abs(x): if not isinstance(x, (int, float)): raise TypeError('bad operand type') if x &gt;= 0: return x else: return -x 默认参数 设置默认参数时，必选参数在前，默认参数在后 有多个默认参数时，调用时既可以按顺序提供默认参数，又可以不按顺序提供部分默认参数。当不安顺序提供部分默认参数时，需要把参数名写上 可变参数 对于参数个数不确定的函数可将参数变为可变参数：def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum &gt;&gt;&gt; calc(1, 2) 5 &gt;&gt;&gt; calc() 0 &gt;&gt;&gt; nums = [1, 2, 3] &gt;&gt;&gt; calc(*nums) 14 关键字参数 关键字参数允许传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict：def person(name, age, **kw): print('name:', name, 'age:', age, 'other:', kw) &gt;&gt;&gt; person('Bob', 35, city='Beijing') name: Bob age: 35 other: {'city': 'Beijing'} &gt;&gt;&gt; person('Adam', 45, gender='M', job='Engineer') name: Adam age: 45 other: {'gender': 'M', 'job': 'Engineer'} &gt;&gt;&gt; extra = {'city': 'Beijing', 'job': 'Engineer'} &gt;&gt;&gt; person('Jack', 24, **extra) name: Jack age: 24 other: {'city': 'Beijing', 'job': 'Engineer'} 参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数 高级特性 切片&gt;&gt;&gt; L = ['Michael', 'Sarah', 'Tracy', 'Bob', 'Jack'] &gt;&gt;&gt; L[0:3] ['Michael', 'Sarah', 'Tracy'] &gt;&gt;&gt; L[:3] ['Michael', 'Sarah', 'Tracy'] &gt;&gt;&gt; L[1:3] ['Sarah', 'Tracy'] &gt;&gt;&gt; L[-2:] ['Bob', 'Jack'] &gt;&gt;&gt; L[-2:-1] ['Bob'] &gt;&gt;&gt; L = list(range(100)) &gt;&gt;&gt; L[:10:2] [0, 2, 4, 6, 8] &gt;&gt;&gt; L[::5] [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95] 迭代 dict迭代：&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3} &gt;&gt;&gt; for key in d: ... &gt;&gt;&gt; for value in d.values(): ... &gt;&gt;&gt; for k, v in d.items(): ... 对list实现下标循环：&gt;&gt;&gt; for i, value in enumerate(['A', 'B', 'C']): ... 列表生成式 生成list&gt;&gt;&gt; list(range(1, 11)) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 根据特定规则生成list&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0] [4, 16, 36, 64, 100] 双层循环：&gt;&gt;&gt; [m + n for m in 'ABC' for n in 'XYZ'] ['AX', 'AY', 'AZ', 'BX', 'BY', 'BZ', 'CX', 'CY', 'CZ'] 调用函数：&gt;&gt;&gt; L = ['Hello', 'World', 'IBM', 'Apple'] &gt;&gt;&gt; [s.lower() for s in L] ['hello', 'world', 'ibm', 'apple'] if...else： 在一个列表生成式中，for前面的if ... else是表达式，而for后面的if是过滤条件，不能带else&gt;&gt;&gt; [x if x % 2 == 0 else -x for x in range(1, 11)] [-1, 2, -3, 4, -5, 6, -7, 8, -9, 10] 生成器 若列表元素可以按照某种算法推算出来，可在循环的过程中不断推算出后续的元素，这样就不必创建完整的list，从而节省大量的空间 创建generator&gt;&gt;&gt; g = (x * x for x in range(10)) &gt;&gt;&gt; g &lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; &gt;&gt;&gt; next(g) 0 &gt;&gt;&gt; next(g) 1 ... 通过循环遍历生成器 python &gt;&gt;&gt; g = (x * x for x in range(10)) &gt;&gt;&gt; for n in g: ... print(n) ... 函数式创建generatordef fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b a, b = b, a + b n = n + 1 return 'done' &gt;&gt;&gt; f = fib(6) &gt;&gt;&gt; f &lt;generator object fib at 0x104feaaa0&gt; 变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行 函数式编程 高阶函数 变量可以指向函数，即函数本身可以赋值给变量 接受另一个函数作为参数的函数称为高阶函数def add(x, y, f): return f(x) + f(y) map()函数和reduce()函数 map()函数接受两个参数，一个是函数，另一个是Iterable, map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回：&gt;&gt;&gt; def f(x): ... return x * x ... &gt;&gt;&gt; r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9]) &gt;&gt;&gt; list(r) [1, 4, 9, 16, 25, 36, 49, 64, 81] reduce()函数把一个函数作用在一个序列上，且这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，效果为：reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) filter()函数 filter()函数用于过滤序列，接收一个函数和一个序列，并将传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素，例如：def is_odd(n): return n % 2 == 1 list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15])) # 结果: [1, 5, 9, 15] sorted()函数 直接对list进行排序：&gt;&gt;&gt; sorted([36, 5, -12, 9, -21]) [-21, -12, 5, 9, 36] 通过key函数实现自定义排序：&gt;&gt;&gt; sorted([36, 5, -12, 9, -21], key=abs) [5, 9, -12, -21, 36] 若要反向排序，可传入第三个参数reverse=True 匿名函数 关键字lambda表示匿名函数，冒号前面的x表示函数参数 只能有一个表达式，返回值即为表达式的结果 匿名函数也是一个函数对象，可赋值给一个变量，再利用变量来调用该函数 装饰器 在代码运行期间动态增加功能的方式称为&quot;装饰器&quot;(Decorator) 示例：# 定义decorator def log(func): def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper # 通过@，将decorator置于函数定义处 @log # 相当于执行了now = log(now) def now(): print('2015-3-25') &gt;&gt;&gt; now() call now(): 2015-3-25 偏函数 根据需要将一个函数的某些参数固定(即设置默认值)，返回新的函数 示例：&gt;&gt;&gt; import functools &gt;&gt;&gt; int2 = functools.partial(int, base=2) &gt;&gt;&gt; int2('1000000') 64 # int2()函数base默认值为10 模块 使用模块 Python模块的标准文件模板#!/usr/bin/env python3 # -*- coding: utf-8 -*- ' a test module ' # 表示模块的文档注释，任何模块代码的第一个字符串都被视为模块的文档注释 __author__ = 'Michael Liao' ... 有关if __name__=='__main__: 当我们在命令行运行模块文件时，Python解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该模块时，if判断将失败 作用域 正常的函数和变量名是公开的（public），可以被直接引用 类似__xxx__这样的变量是特殊变量，可以被直接引用，但是有特殊用途 类似_xxx和__xxx这样的函数或变量就是非公开的（private），不应该被直接引用 面向对象编程 类和实例 通过class关键字定义类class Student(object): # object表示该类从object类继承而来，默认为object，支持多继承 pass &gt;&gt;&gt; bart = Student() &gt;&gt;&gt; bart.name = 'Bart Simpson' # 可自由地给一个实例绑定属性 &gt;&gt;&gt; bart.name 'Bart Simpson' &quot;构造函数&quot;class Student(object): def __init__(self, name, score): self.name = name self.score = score # 这样再创建实例时就不能传入空的参数 访问限制 为防止外部代码自由修改内部属性，可把属性改为私有变量：class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print('%s: %s' % (self.__name, self.__score)) 继承和多态 当子类和父类都存在相同的方法时，子类的方法覆盖了父类的方法 静态语言 vs 动态语言 对于静态语言（例如Java）来说，如果需要传入Animal类型，则传入的对象必须是Animal类型或者它的子类，否则，将无法调用其中的run()方法 对于Python这样的动态语言来说，则不一定需要传入Animal类型。我们只需要保证传入的对象有一个run()方法就可以了 获取对象信息 type()函数 判断对象类型 isinstance()函数 判断一个对象是否是该类型本身或位于该类型的父继承链上 dir()函数 获取一个对象的所有属性和方法，返回一个list 实例属性和类属性 相同的实例属性将屏蔽掉类属性 面向对象高级编程 使用__slots__ 给实例绑定方法：&gt;&gt;&gt; def set_age(self, age): # 定义一个函数作为实例方法 ... self.age = age ... &gt;&gt;&gt; from types import MethodType &gt;&gt;&gt; s.set_age = MethodType(set_age, s) # 给实例绑定一个方法 给类绑定方法：&gt;&gt;&gt; def set_score(self, score): ... self.score = score ... &gt;&gt;&gt; Student.set_score = set_score 限制实例的属性：class Student(object): __slots__ = ('name', 'age') # 用tuple定义允许绑定的属性名称 # 此时只能对创建的实例绑定name, age属性，绑定其他属性将报错 注意：__slots__定义的属性仅对当前类实例起作用，对继承的子类不起作用 使用@property Python内置的@property装饰器就是负责把一个方法变成属性调用的class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError('score must be an integer!') if value &lt; 0 or value &gt; 100: raise ValueError('score must between 0 ~ 100!') self._score = value &gt;&gt;&gt; s = Student() &gt;&gt;&gt; s.score = 60 # OK，实际转化为s.set_score(60) &gt;&gt;&gt; s.score # OK，实际转化为s.get_score() 60 把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@score.setter，负责把一个setter方法变成属性赋值 使用枚举类from enum import Enum Month = Enum('Month', ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')) for name, member in Month.__members__.items(): print(name, '=&gt;', member, ',', member.value) # valule属性默认从1开始计数 使用元类 利用type()动态创建类 type()函数既可以返回一个对象的类型，又可以创建出新的类型，比如，我们可以通过type()函数创建出Hello类，而无需通过class Hello(object)...的定义：&gt;&gt;&gt; def fn(self, name='world'): # 先定义函数 ... print('Hello, %s.' % name) ... &gt;&gt;&gt; Hello = type('Hello', (object,), dict(hello=fn)) # 创建Hello class &gt;&gt;&gt; h = Hello() &gt;&gt;&gt; h.hello() Hello, world. 要创建一个class对象，type()函数依次传入3个参数： class的名称 继承的父类集合，如果只有一个父类，注意tuple的单元素写法 class的方法名称与函数绑定 IO编程 文件读写 读文件with open('/path/to/file', 'r') as f: print(f.read()) # 常用函数 read() # 一次性读取文件的全部内容 read(size) # 每次最多读取size个字节的内容 readline() # 每次最多读取size个字节的内容 readlines() # 一次读取所有内容并按行返回list 写文件with open('/Users/michael/test.txt', 'w') as f: f.write('Hello, world!') 操作文件和目录 创建一个目录：os.mkdir('filepath') 删除一个目录：os.rmdir('filepath') 合并两个路径：os.path.join('path1', 'path2') 拆分路径（后一部分为最后级别的目录或文件名）：os.path.split() 正则表达式 直接给出字符就是精确匹配，\\d可以匹配一个数字，\\w可以匹配一个字母或数字，\\s可以匹配一个空格或Tab等空白符 .可以匹配任意字符 *表示任意个字符，+表示至少一个字符，？表示0个或1个字符，{n}表示n个字符，{n, m}表示n-m个字符 特殊字符需要用\\转义 精确匹配 [0-9a-zA-Z\\_]可以匹配一个数字、字母或者下划线 [0-9a-zA-Z\\_]+可以匹配至少由一个数字、字母或者下划线组成的字符串 [a-zA-Z\\_][0-9a-zA-Z\\_]*可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串 [a-zA-Z\\_][0-9a-zA-Z\\_]{0, 19}更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符） A|B可以匹配A或B ^表示行的开头，^\\d表示必须以数字开头 $表示行的结束，\\d$表示必须以数字结束 ","link":"https://cisse-away.github.io/post/python-xue-xi-bi-ji/"},{"title":"How we could teach our bodies to heal faster","content":"Speaker：Kaitlyn Sadtler 视频链接 原视频 摘要 I work to create materials that instruct our immune system to give us the signals to grow new tissues. 我的工作是创造不同的材料来指导我们的免疫系统，向身体传达信号，生长出新的组织。 So in this case, instead of the immune cells rushing off towards infection to fight bacteria, they're rushing toward an injury. I discovered a specific type of immune cell, the helper T cell, was present inside that material that I implanted and absolutely critical for wound healing. 所以在这种情况下，免疫细胞并没有冲向感染处去对抗细菌，而是聚集在了伤口处。我发现了一种特殊类型的免疫细胞，辅助T细胞，存在于我植入的材料中，并且对伤口愈合至关重要。 I'm working to create materials that give us the signals to build new tissue by changing the immune response. 我正在努力创造一种材料，通过改变免疫反应来产生信号，让我们的身体去再生新的组织。 In the future, we could see a scar-proof band-aid, a moldable muscle filler or even a wound-healing vaccine. 在未来，我们可以看到一种防疤痕创可贴，一种可塑形的肌肉填充物，甚至一种帮助伤口愈合疫苗。 But with these advances, and working with our immune system to help build tissue and heal wounds, we could begin seeing products on the market that work with our body's defense system to help us regenerate, and maybe one day be able to keep pace with a salamander. 但随着这些进步，结合我们的免疫系统在构建组织和愈合伤口上的帮助，我们会开始看到市场上的一些产品，可以与我们身体的 防御系统协作，帮助我们再生，也许有一天，我们能够跟上蝾螈的步伐。 ","link":"https://cisse-away.github.io/post/how-we-could-teach-our-bodies-to-heal-faster/"},{"title":"How to control someone else's arm with your brain","content":"Speaker：Greg Gage 视频链接 原视频 摘要 While many people are fascinated by the brain, they can't really tell you that much about the properties about how the brain works because we don't teach neuroscience in schools. 尽管很多人都对大脑十分着迷，他们却讲不出太多关于大脑的特征，以及它是怎样工作的，因为学校没有神经科学的课。 And one of the reasons why is that the equipment is so complex and so expensive that it's really only done at major universities and large institutions. 其中一个原因就是相应的设备太复杂又太昂贵，这些研究只有在高等学府 和大型研究机构才能进行。 And that's a shame because one out of five of us, that's 20 percent of the entire world, will have a neurological disorder. 很遗憾的是，我们每五个人当中就有一个，也就是全球人口的20%会受神经性失调的困扰。 They're going to go down across your corpus callosum, down onto your spinal cord to your lower motor neuron out to your muscles here, and that electrical discharge is going to be picked up by these electrodes right here and we're going to be able to listen to exactly what your brain is going to be doing. 这些信号会向下穿过胼胝体，进入你的脊髓，到达下运动神经元，再到达你这里的肌肉，这种电信号释放 会被这里的电极接收，于是我们就能够听到你的大脑到底想要做些什么。 And so it turns out that there is a nerve that's right here that runs up here that innervates these three fingers, and it's close enough to the skin that we might be able to stimulate that so that what we can do is copy your brain signals going out to your hand and inject it into your hand, so that your hand will move when your brain tells your hand to move. 实际上这里有一根神经一路上来并支配这三根手指的活动，而这根神经又离皮肤足够近，让我们能够 刺激它，所以我们能做的是复制从你(SK)的大脑向手臂发出的信号, 并把这个信号注入到你(MG)手臂里，于是你(MG)的手臂会在你(SK)的大脑告诉手臂要动的时候动。 ","link":"https://cisse-away.github.io/post/how-to-control-someone-elses-arm-with-your-brain/"},{"title":"How to stay calm when you know you'll be stressed","content":"Speaker：Daniel Levitin 视频链接 原视频 摘要 I started wondering, are there things that I can do, systems that I can put into place, that will prevent bad things from happening? Or at least if bad things happen, will minimize the likelihood of it being a total catastrophe. 我开始想，我能做些什么，有什么切实可行的方法，可以防止不好的事发生呢？或者至少就算不好的事情真的会发生，也能把损失降到最小。 Danny shared with me that he'd been practicing something called prospective hindsight. It's something that he had gotten from the psychologist Gary Klein, who had written about it a few years before, also called the pre-mortem. Danny和我分享了他一直在练习的称为”预测后见之明“的东西。这是他从心理学家Gary Klein那里学到的。Gary前几年已经写了这个理论，也称为&quot;事前剖析&quot;。 In the pre-mortem, Danny explained, you look ahead and you try to figure out all the things that could go wrong, and then you try to figure out what you can do to prevent those things from happening, or to minimize the damage. Danny解释，在事前剖析中，你要预测，尝试想出可能出错的所有事情，然后你要尝试可能的方法来防止这些错误发生，或将损失降到最小。 Remember, when you're under stress, the brain releases cortisol. Cortisol is toxic, and it causes cloudy thinking. 记住，当你有压力时，大脑会释放皮质醇。皮质醇是有毒的，它会导致思维不清晰。 ","link":"https://cisse-away.github.io/post/how-to-stay-calm-when-you-know-youll-be-stressed/"},{"title":"How to find work you love","content":"Speaker：Scott Dinsmore 视频链接 原视频 摘要 So eight years ago, I got the worst career advice of my life. I had a friend tell me, &quot;Don't worry about how much you like the work you're doing now. It's all about just building your resume.&quot; 八年前，我听到一个有生以来最烂的职场建议。有个朋友跟我说“ 斯科特，别考虑你喜不喜欢现在的工作，重要的是简历上好看。” I read some altogether different advice from Warren Buffett, and he said, &quot;Taking jobs to build up your resume is the same as saving up sex for old age.&quot; 我从沃伦·巴菲特的书里读到了完全不同的建议，他说，“为了让简历好看而工作，就跟节省着性生活等老了再用一样。” As I've made these discoveries, I noticed a framework of really three simple things that all these different passionate world-changers have in common. 伴随着这些发现，我注意到充满热情要改变世界的人都做过三件相同的事。 The first part of this three-step passionate work framework is becoming a self-expert and understanding yourself, because if you don't know what you're looking for, you're never going to find it. 想要满载热情地投入工作， 第一步要成为自己的专家，了解自己，因为如果你都不知道自己想要什么， 还何谈“找到”二字呢。 And so the first step of our compass is finding out what our unique strengths are. What are the things that we wake up loving to do no matter what, whether we're paid or we're not paid, the things that people thank us for? 所以第一步就要找到自己的独特优势。什么事是你每天一睁眼睛就想去做的，不管能否从中获得报酬，而且是对别人也有益的事？ And the Strengths Finder 2.0 is a book and also an online tool. I highly recommend it for sorting out what it is that you're naturally good at. 强烈推荐一本书，也是个在线工具，叫做《发现你的优势2.0》，能帮你找到自己最擅长的事。 And next, what's our framework or our hierarchy for making decisions? We have to figure out what it is to make these decisions, so we know what our soul is made of. 第二步，弄清让我们做出决定的根本原因。弄清自己的各种决定背后真正的原因，就能使我们了解最真实的自己。 And then the next step is our experiences. What we love, what we hate, what we're good at, what we're terrible at. And if we don't spend time paying attention to that and assimilating that learning and applying it to the rest of our lives, it's all for nothing. 第三，就是经验。我们喜欢什么、讨厌什么，擅长什么、不擅长什么。如果我们不在意这些信息，不去消化已知的经验，并且用到今后的生活里，那么这些都没有意义。 There's this quote by Jim Rohn and it says. &quot;You are the average of the five people you spend the most time with.&quot; 吉米·罗恩曾经说过，“你是你最常接触的五个人的平均值。” To sum things up, in terms of these three pillars, they all have one thing in common more than anything else. They are 100 percent in our control. 总结起来，上面的三件事都有一个共同特点。它们完全由我们自己掌控。 ","link":"https://cisse-away.github.io/post/how-to-find-work-you-love/"},{"title":"5 tips to improve your critical thinking","content":"视频链接 原视频 摘要 Every day, a sea of decisions stretches before us. Some are small and unimportant, but others have a larger impact on our lives. 无时不刻，我们都在做各种各样的决定， 有些微不足道， 有些则对我们的一生都有很大影响。 There are many ways to improve our chances, and one particularly effective technique is critical thinking. 有很多提高正确几率方法，其中最有效的方法之一就是批判性思维。 There are many different ways of approaching critical thinking, but here's one five-step process that may help you solve any number of problems. 有很多运用批判性思维的方法，这里讲的方法，只需五步就可以帮你解决各种问题。 One: formulate your question. In other words, know what you're looking for. 第一：明确你的问题。 换句话说，知道你想要什么。 Two: gather your information. There's lots of it out there, so having a clear idea of your question will help you determine what's relevant. 第二：搜集信息。信息无所不在，所以明晰你的问题可以帮你决定哪些信息是有用的。 Three: apply the information, something you do by asking critical questions. 第三：运用信息， 可以通过问关键性问题对信息加以运用。 Four: consider the implications. 第四：考虑后果及影响。 Five: explore other points of view. 第五：了解其他观点。 This five-step process is just one tool, and it certainly won't eradicate difficult decisions from our lives. 这五个步骤仅仅是工具，也不可能完全消除我们所面对的难题。 ","link":"https://cisse-away.github.io/post/5-tips-to-improve-your-critical-thinking/"},{"title":"The story of 'Oumuamua, the first visitor from another star system","content":"Speaker：Karen J.Meech 视频链接 原视频 摘要 On October 19, 2017, Pan-STARRS spotted an object moving rapidly between the stars, and this time the usual follow-up measurements of position and speed showed something completely different. By October 22nd, we had enough data to realize that this object wasn't from our solar system. 2017年的10月19日，泛星计划发现了一个在星系间快速移动的物体，这一次，例行的位置和速度测量结果显示了完全不一样的结果。到10月22日，我们有了足够的数据 证明这个物体不是来自太阳系。 In honor of it being discovered by a telescope in Hawaii, we consulted two experts on Hawaiian culture -- a Hawaiian navigator and a linguist -- to propose a name. And they suggested &quot;'Oumuamua,&quot; which means scout or messenger from the distant past reaching out to us. 因为发现它的望远镜是在夏威夷，于是我们找了两位夏威夷文化的专家， 一位夏威夷向导和一位语言学家来想一个名字。他们提议叫“奥陌陌”，意思是经过长途跋涉找到我们的侦察兵或者信使。 So while it is rotating around its short axis, it's also rolling around the long axis and nodding up and down. This very energetic, excited motion is almost certainly the result of it being violently tossed out of its home solar system. 它在绕自己短轴旋转的同时，也在绕长轴翻转，同时上下摇晃。这种有力的、激烈的运动形式，几乎可以肯定是因为它被猛烈地抛出了自己的太阳系。 We think that 'Oumuamua may be more of a flattened oval. 我们认为奥陌陌可能更像一个扁平的椭圆。 Whatever it is, we believe it's a natural object, but we can't actually prove that it's not something artificial. 无论原因是什么， 我们都相信它是自然形成的， 但我们也无法证明它不是人造的。 More importantly, I think this visitor from afar has really brought home the point that our solar system isn't isolated. We're part of a much larger environment, and in fact, we may even be surrounded by interstellar visitors and not even know it. 更重要的是，这位远道而来的访客证明了一点，我们的太阳系并不是孤立的。我们是一个更大世界的一部分， 实际上，我们周围可能有许多星际访客，而我们还没有察觉。 ","link":"https://cisse-away.github.io/post/the-story-of-oumuamua-the-first-visitor-from-another-star-system/"},{"title":"How we'll become  cyborgs and extend human poteential","content":"Speaker：Hugh Herr 视频链接 原视频 摘要 I'm a bionic man, but I'm not yet a cyborg. 我是一个仿生人， 但还不是生化人。 Stated simply, when I think about moving, that command is communicated to the synthetic part of my body. However, those computers can't input information into my nervous system. 简言之，当我想移动时，这个命令被传达到我身体的合成部分。然而，计算机却无法将信息输入到我的神经系统。 NeuroEmbodied Design will extend our nervous systems into the synthetic world, and the synthetic world into us, fundamentally changing who we are. 神经体现设计将我们的神经系统延伸到合成世界中，同时也将合成世界延伸到我们体内，这些都将从根本上改变我们。 The current amputation paradigm hasn't changed fundamentally since the US Civil War and has grown obsolete in light of dramatic advancements in actuators, control systems and neural interfacing technologies. 自美国内战以来，截肢规范并没有发生根本性的改变，随着执行器、控制系统和神经接口技术的迅猛发展，原来的截肢规范早已过时。 We invented the agonist-antagonist myoneural interface, or AMI, for short. The AMI is a method to connect nerves within the residuum to an external, bionic prosthesis. 我们发明了主动肌和对抗肌肌神经的接口，缩写为AMI。AMI是一种将残肢中的神经连接到外部仿生假肢的方法。 When a limb is amputated, the surgeon connects these opposing muscles within the residuum to create an AMI. Artificial electrodes are then placed on each AMI muscle, and small computers within the bionic limb decode those signals to control powerful motors on the bionic limb. 当肢体被截掉以后，外科医生把残肢内这些拮抗肌连接起来，形成AMI。然后，将人造电极放在每块AMI 肌肉上，仿生肢体内的小计算机对这些信号解码，以控制仿生肢上的强力马达。 Here's Jim descending steps, reaching with his bionic toe to the next stair tread, automatically exhibiting natural motions without him even trying to move his limb. 这是吉姆走下台阶，正伸出仿生脚趾到下一个台阶，自动地展现自然的动作，甚至不用有意识地试着移动肢体。 I believe the reach of NeuroEmbodied Design will extend far beyond limb replacement and will carry humanity into realms that fundamentally redefine human potential. 我相信，神经体现设计的范围将远超替换肢体的范围，它会将人类带入从根本上重新定义人类潜能的领域。 In this 21st century, designers will extend the nervous system into powerfully strong exoskeletons that humans can control and feel with their minds. Muscles within the body can be reconfigured for the control of powerful motors, and to feel and sense exoskeletal movements, augmenting humans' strength, jumping height and running speed. 在二十一世纪，设计师们会把神经系统延伸到强大的外骨骼，人类能用思想去控制和感知它们。体内的肌肉可重新配置，以控制强大的马达，并感受和感知外骨骼的动作，进而增强人类力量、跳跃高度和跑步速度。 In this 21st century, I believe humans will become superheroes. Humans may also extend their bodies into non-anthropomorphic structures, such as wings, controlling and feeling each wing movement within the nervous system. 在二十一世紀，我相信人类会成为超级英雄。人类也可以延伸自己的身体到拟人化的结构中，如翅膀，在神经系统内控制和感受每一个翅膀动作。 ","link":"https://cisse-away.github.io/post/how-well-become-cyborgs-and-extend-human-poteential/"},{"title":"Adventures of an asteroid hunter","content":"Speaker：Carrie Nugent 视频链接 原视频 摘要 These are all of the known near-Earth asteroids, which at last count was 13,733. Each one has been imaged, cataloged and had its path around the Sun determined. 这些是已知的近地行星，它们的最新统计量为13733。每一个都被拍照，分类并且确定了它绕太阳旋转的轨道。 Three years ago today, a small asteroid exploded over the city of Chelyabinsk, Russia. That object was about 19 meters across, or about as big as a convenience store. Objects of this size hit the Earth every 50 years or so. 三年前的今天， 一个小行星在俄罗斯的车里雅宾斯克爆炸。那个东西有19米宽，或者说像一个便利店一样大。像这样大的物体每五十年左右撞击一次地球。 66 million years ago, a much larger object hit the Earth, causing a massive extinction. That object was about 10 kilometers across, and 10 kilometers is roughly the cruising altitude of a 747 jet. 六百六十万年前，一个更大的物体撞击地球，造成了物种大灭绝。那个物体有十千米宽，而十千米是波音747的巡航高度。 One of the reasons NEOWISE is so valuable is that it sees the sky in the thermal infrared. This is a vital capability since some asteroids are as dark as coal and can be difficult or impossible to spot with other telescopes. NEOWISE有这样高价值的原因之一便是它使用红外感应观测天空。这是一项很重要的能力，因为一些小行星像黑碳一样暗，其他的探测器很难或不可能发现这类行星。 The community, together, discovered over 90 percent of asteroids bigger than one kilometer across -- objects capable of massive destruction to Earth. But the job's not done yet. An object 140 meters or bigger could decimate a medium-sized country. 参与的组织共同发现直径大于一公里的 百分之九十以上的小行星。这些物体能对地球造成巨大破坏。但是工作并没有结束。一个140米宽或更大的物体能够摧毁一座中型城市。 If we found a hazardous asteroid with significant early warning, we could nudge it out of the way. 如果我们发现一个极具危险性的小行星并提前发出有效警告，我们便可以把它推走。 ","link":"https://cisse-away.github.io/post/adventures-of-an-asteroid-hunter/"},{"title":"4 questions you should always ask your doctor","content":"Speaker：Christer Mjåset 视频链接 原视频 摘要 Even though most indications for treatments in the world are standardized, there is a lot of unnecessary variation of treatment decisions, especially in the Western world. 尽管世界上大部分的治疗指示已经标准化了，但是有很多是没必要的治疗手段变种，尤其是在西方国家。 The waiting list for an MRI would quadruple, maybe even more. And you would all take the spot on that list from someone who really had cancer. 核磁共振成像的排队人数会增至四倍，甚至更多。而你们会占用了名单上那些真的有癌症的人的名额。 Is this really necessary? What are the risks? Are there other options? And what happens if I don't do anything? Ask them when your doctor wants to send you to an MRI, when he prescribes antibiotics or suggests an operation. “这真的必要吗？”“有什么风险？”“有没有其它选择？”“如果我什么都不做会怎么样？”当医生让你做核磁共振成像时要问医生，还有当他给你开抗生素或者建议手术时也要问。 What we know from research is that one out of five of you, 20 percent, will change your opinion on what to do. 从研究中我们知道5个人中有1个，也就是 20%，在问了之后会改变你们的想法。 ","link":"https://cisse-away.github.io/post/4-questions-you-should-always-ask-your-doctor/"},{"title":"Fake videos of real people—and how to spot them","content":"Speaker：Supasorn Suwajanakorn 视频链接 原视频 摘要 So I set out to see if this could be done and eventually came up with a new solution that can build a model of a person using nothing but these: existing photos and videos of a person. 于是我开始探索这个能不能搞定，并最终找到了一个新的解决方案，只需使用下面这些东西就能构建人的模型：个人现存的照片和视频。 First, we introduce a new technique that can reconstruct a high-detailed 3D face model from any image without ever 3D-scanning the person. 首先我们引入一种新的技术可以从任何图像中重建一个高细节的3D人脸模型，而且无需经对真人进行3D扫描。 And we are still missing color here, so next, we develop a new blending technique that improves upon a single averaging method and produces sharp facial textures and colors. 这里我们仍然缺少肤色，所以下一步，我们开发了一种新的混合技术 改善了平均模型， 并产生尖锐的面部纹理和肤色。 As a researcher, I'm also working on countermeasure technology, and I'm part of an ongoing effort at AI Foundation, which uses a combination of machine learning and human moderators to detect fake images and videos, fighting against my own work. 作为一名研究人员，我也在研究对抗技术，我是人工智能基金会持续努力的一份子，它结合了机器学习和人工模型来识别假图像和视频，与我们自己的工作做斗争。 Despite all this, though, fake videos could do a lot of damage, even before anyone has a chance to verify, so it's very important that we make everyone aware of what's currently possible so we can have the right assumption and be critical about what we see. 此外，假视频可以带来很大危害，甚至在人们有机会验证它之前，所以让大家意识到这可能是什么非常重要，这样我们才能得到正确的推断，并对看到的保持谨慎。 There's still a long way to go before we can fully model individual people and before we can ensure the safety of this technology. 在个人完全建模以及确保技术的安全性方面，仍有很长的路要走。 ","link":"https://cisse-away.github.io/post/fake-videos-of-real-people-and-how-to-spot-them/"},{"title":"Is anatomy destiny","content":"Speaker：Alice Dreger 视频链接 原视频 摘要 And a lot of what I've worked on is people who have atypical sex -- so people who don't have the standard male or the standard female body types. And as a general term, we can use the term &quot;intersex&quot; for this. 另外，许多我接触的人他们的性别与众不同—即他们没有很标准的男性特征或者女性特有的身体特征。总的来说，这个症状可以被叫做双性人。 Intersex comes in a lot of different forms. There is a syndrome called androgen insensitivity syndrome. When the child is born, she looks like a girl. And it's often not until she hits puberty and she's growing and developing breasts, but she's not getting her period. And people do some tests and figure out that, instead of having ovaries inside and a uterus, she has testes inside, and she has a Y chromosome. 双性可以有多种的形式。有一种叫做睾丸不敏感（睾丸女性化）综合征的症状。当婴儿诞生时，她看起来像个女孩儿。大多数情况，直到青春期她的胸部开始发育的时候她却没有女性周期。于是人们检查发现她并没有卵巢和子宫，在她身体内有睾丸，并且她携带着Y染色体 。 Now what's important to understand is you may think of this person as really being male, but they're really not. Most females like me are actually androgen-sensitive. Somebody like me has actually had a brain exposed to more androgens than the woman born with testes who has androgen insensitivity syndrome. 现在，有一个重要问题需要说明的是大家一定认为这个人实际上是个男孩儿，其实并非如此。大多数像我一样的女性实际上是雄激素敏感的，有的像我一样女性，大脑暴露在过多的雄性激素下，比那个出生时有睾丸的女孩，那个雄性激素不敏感综合征的女孩有的雄性激素还多。 So sex is really complicated—it's not just that intersex people are in the middle of all the sex spectrum—in some ways, they can be all over the place. 所以说性别是个非常复杂的东西—并不是说那些双性的人就处在男性和女性之间—在某些方面，他们可以覆盖整个性特征的范围。 The reason that children with these kinds of bodies are often &quot;normalized&quot; by surgeons is not because it actually leaves them better off in terms of physical health. In many cases, people are actually perfectly healthy. The reason they're often subject to various kinds of surgeries is because they threaten our social categories. 拥有这些身体特征的孩子们常被外科医生手术纠正的原因实际上并不是为了身体健康。因为很多情况下，他们的身体其实都很健康，他们接受各种外科手术的原因是因为他们受到我们社会上对人分类的威胁。 So what we have is a sort of situation where the farther our science goes, the more we have to admit to ourselves that these categories that mapped very simply to stable identity categories are a lot more fuzzy than we thought. And it's not just in terms of sex. It's also in terms of race. 因此我们现在的情况是科学越发展，我们越得承认这些性别的分类形式是过于简单地给特性分了类，实际情况要比我们想象的模糊的多。并且这不仅仅是关于性别的问题。也同样是关于种族的问题。 As we begin to look at all that commonality, we have to begin to question why we maintain certain divisions. There are some anatomical divisions that make sense to me and that I think we should retain. But the challenge is trying to figure out which ones they are and why do we retain them, and do they have meaning. 当我们开始去审视所有的公民 我们不得不开始质疑为什么我们去维护一个固定的界限。有一些在解剖学上的界限对我们来说是有意义的，我想那些应该被保留。但是艰巨的任务是试着去找出哪些是应该保留的，为什么我们要保留它们，它们有什么意义。 So we have these anatomical categories that persist, that are in many ways problematic and questionable. And the question to me becomes: What do we do, as our science gets to be so good in looking at anatomy, that we reach the point where we have to admit that a democracy that's been based on anatomy might start falling apart? I don't want to give up the science, but at the same time, it feels sometimes like the science is coming out from under us. 因此我们所坚持的一些结构上的生物分类，它们在很多方面是有问题并且需要质疑的。对我来说，问题就变成了： 我们要做什么，当我们的科学在解剖学上如此先进的时候，我们已经到了不得不承认基于解剖学上的人的民主也许正在崩溃的时候？我不想对科学失去希望 但是同时，有的时候隐约觉得 科学的发展是受到我们影响的。 And what we know from cross-cultural studies is that females, on average are more inclined to be very attentive to complex social relations and to taking care of people who are, basically, vulnerable within the group. 我们从跨文化研究中得知女性，平均的看更倾向于留意 复杂的社会关系 同时更加关心、同情人群中的弱势群体。 I considered myself a feminist. One of my graduate advisors said, &quot;Tell me what's feminine about feminism.&quot; 我自认是女权主义者。我的一位指导教授说：“告诉我女权主义女性化在哪里”。 When we take democracy beyond anatomy, is to think less about the individual body in terms of the identity, and think more about those relationships. So that as we the people try to create a more perfect union, we're thinking about what we do for each other. 当我们抛开人种的分别来讲民主—也就是较少的考虑到人的个体关于人的身份特性，而是更多的考虑人们之间的关系.因此，当人们试图创造一个更加美好的联合体的时候我们需要考虑的是我们为他人都做了什么。 ","link":"https://cisse-away.github.io/post/is-anatomy-destiny/"},{"title":"How Airbnb designs for trust","content":"Speaker：Joe Gebbia 视频链接 原视频 摘要 Maybe the people that my childhood taught me to label as strangers were actually friends waiting to be discovered. 也许，在我童年时期被定义为陌生人的那些人，其实是等待被发现的朋友。 It turns out, a well-designed reputation system is key for building trust. 事实证明， 一个精心设计的信誉体系是建立信任的关键。 The research showed, not surprisingly, we prefer people who are like us. The more different somebody is, the less we trust them. 研究得出了一个意料之中的结果，我们更喜欢与我们相似的人。与我们差异越大的人，我们越是不信任他们。 High reputation beats high similarity. 高的信誉评价就会比高相似度更可信。 ","link":"https://cisse-away.github.io/post/how-airbnb-designs-for-trust/"},{"title":"Leetcode刷题笔记—无重复字符的最长子串","content":"题目链接 题目描述 给定一个字符串，请你找出其中不含有重复字符的最长子串的长度。 示例 1: 输入: s = &quot;abcabcbb&quot; 输出: 3 解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。 示例 2: 输入: s = &quot;bbbbb&quot; 输出: 1 解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。 示例 3: 输入: s = &quot;pwwkew&quot; 输出: 3 解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。 请注意，你的答案必须是子串的长度，&quot;pwke&quot; 是一个子序列，不是子串。 示例 4: 输入: s = &quot;&quot; 输出: 0 提示： 0 &lt;= s.length &lt;= 5 * ${10^4}$ s 由英文字母、数字、符号和空格组成 个人题解 C++实现：int lengthOfLongestSubstring(string s) { int max = 0; for(int i = 0; i &lt; s.length(); i++) { string ss; ss += s[i]; int l = 1; for(int j = i + 1; j &lt; s.length(); j++) { if(ss.find(s[j]) == string::npos) { ss += s[j]; l++; } else { break; } } if(max &lt; l) max = l; } return max; } Python3实现：def lengthOfLongestSubstring(self, s: str) -&gt; int: max = 0 for i in range(len(s)): ss = &quot;&quot; ss += s[i] for j in range(i+1, len(s)): if s[j] in ss : break else: ss += s[j] if max &lt; len(ss) : max = len(ss) return max Leetcode题解 滑动窗口C++实现：int lengthOfLongestSubstring(string s) { // 哈希集合，记录每个字符是否出现过 unordered_set&lt;char&gt; occ; int n = s.size(); // 右指针，初始值为 -1，相当于我们在字符串的左边界的左侧，还没有开始移动 int rk = -1, ans = 0; // 枚举左指针的位置，初始值隐性地表示为 -1 for (int i = 0; i &lt; n; ++i) { if (i != 0) { // 左指针向右移动一格，移除一个字符 occ.erase(s[i - 1]); } while (rk + 1 &lt; n &amp;&amp; !occ.count(s[rk + 1])) { // 不断地移动右指针 occ.insert(s[rk + 1]); ++rk; } // 第 i 到 rk 个字符是一个极长的无重复字符子串 ans = max(ans, rk - i + 1); } return ans; } 滑动窗口Python3实现：def lengthOfLongestSubstring(self, s: str) -&gt; int: # 哈希集合，记录每个字符是否出现过 occ = set() n = len(s) # 右指针，初始值为 -1，相当于我们在字符串的左边界的左侧，还没有开始移动 rk, ans = -1, 0 for i in range(n): if i != 0: # 左指针向右移动一格，移除一个字符 occ.remove(s[i - 1]) while rk + 1 &lt; n and s[rk + 1] not in occ: # 不断地移动右指针 occ.add(s[rk + 1]) rk += 1 # 第 i 到 rk 个字符是一个极长的无重复字符子串 ans = max(ans, rk - i + 1) return ans 利用&quot;集合&quot;数据结构来查重 ","link":"https://cisse-away.github.io/post/leetcode-shua-ti-bi-ji-wu-chong-fu-zi-fu-de-zui-chang-zi-chuan/"},{"title":"Linux文件基本操作","content":"参考课程-蓝桥云课 一、新建 新建空白文件 在当前目录创建一个空白文件:touch &lt;文件名&gt; 若当前目录存在一个 test 文件夹，则touch命令，则会更改该同名文件夹的时间戳而不是新建文件 新建目录 在当前目录创建一个空目录: mkdir &lt;目录名&gt; 使用-p参数创建多级目录： mkdir -p &lt;新建路径&gt; 若当前目录已经创建了一个文件，再使用mkdir新建同名的文件夹，系统会报错文件已存在 二、复制 复制文件 使用cp命令复制一个文件到指定目录：cp &lt;文件名&gt; &lt;目录路径&gt; 复制目录 cp命令无法直接复制一个目录，需要加上-r或-R参数进行递归复制：cp -r &lt;待复制目录名&gt; &lt;目标目录&gt; 三、删除 删除文件 使用rm命令删除一个文件： rm &lt;文件名&gt; 若文件为只读权限，可使用-f参数强制删除： rm -f &lt;文件名&gt; 删除目录 同复制操作一样，需加上-r或-R参数： rm -r &lt;目录名&gt; 若权限不足可加-f参数强制删除： rm -rf &lt;目录名&gt; 四、移动文件与文件重命名 移动文件 使用mv命令移动文件：mv &lt;源目录文件&gt; &lt;目的目录&gt; 重命名文件 使用mv命令重命名文件：mv &lt;旧文件名&gt; &lt;新文件名&gt; 批量重命名 使用rename命令搭配正则表达式：rename '正则表达式' &lt;文件&gt; 五、查看文件 使用cat，tac和nl命令查看文件 前两个命令都是用来打印文件内容到标准输出，其中cat为正序显示，tac为倒序显示： cat &lt;文件&gt; tac &lt;文件&gt; 可以加上-n参数显示行号： cat -n &lt;文件&gt; nl命令是比cat -n更专业的行号打印命令。常用参数： -b : 指定添加行号的方式，主要有两种： -b a:表示无论是否为空行，同样列出行号(&quot;cat -n&quot;就是这种方式) -b t:只列出非空行的编号并列出（默认为这种方式） -n : 设置行号的样式，主要有三种： -n ln:在行号字段最左端显示 -n rn:在行号字段最右边显示，且不加 0 -n rz:在行号字段最右边显示，且加 0 -w : 行号字段占用的位数(默认为 6 位) 使用more和less命令分页查看文件 使用more命令打开文件： more &lt;文件&gt; 打开后默认只显示一屏内容，终端底部显示当前阅读的进度。可以使用Enter键向下滚动一行，使用 Space键向下滚动一屏，按下h显示帮助，q退出。 less命令的使用基本与more一致 使用head和tail命令查看文件 这两个命令负责只查看文件的头(或尾)几行（默认十行）：tail &lt;文件&gt; 使用-n参数可指定显示的行数：tail -n &lt;行数&gt; &lt;文件&gt; 六、查看文件类型 使用file命令查看文件的类型：file &lt;文件&gt; ","link":"https://cisse-away.github.io/post/linux-wen-jian-ji-ben-cao-zuo/"},{"title":"How to build your  confidence — and spark it in others","content":"Speaker：Brittany Packnett 视频链接 原视频 摘要 By most measures, we have more knowledge and more resources now than at any other point in history, and still injustice abounds and challenges persist. If knowledge and resources were all that we needed, we wouldn't still be here. And I believe that confidence is one of the main things missing from the equation. 在大多数情况下，我们拥有远超过去的知识和资源，却未能克服所有的不公和挑战。如果知识和资源能满足我们所有的需求，那我们就不应该仍在原地踏步。 在我眼中，自信是平衡的天平上所缺失的重要的一块。 After all, what are academic skills without the confidence to use those skills to go out and change the world. 如果没有信心改变世界的话，要学术技能又有什么用呢？ If you don't have enough confidence, it could be because you need to readjust your goal. If you have too much confidence, it could be because you're not rooted in something real. 如果你不够自信，这可能说明你需要重新调整你的目标。如果你太过自信，这可能说明你不脚踏实地。 并非所有人都缺少自信。 In my estimation, it takes at least three things: permission, community and curiosity. Permission births confidence, community nurtures it and curiosity affirms it. 据我估计，需要三个因素：允许、团体和好奇心。自信诞生于允许，成长于团体，增强于好奇心。 In education, we've got a saying, that you can't be what you can't see. 在教育界，我们有一句话，你无法成为你看不到的东西。 Confidence needs permission to exist and community is the safest place to try confidence on. 自信需要允许来孕育，而团体是尝试自信最安全的地方。 In community, I can find my confidence and your curiosity can affirm it. 在团体中，我能找到我的自信心，而你的好奇心能巩固它。 ","link":"https://cisse-away.github.io/post/how-to-build-your-confidence-and-spark-it-in-others/"},{"title":"Leetcode刷题笔记—两数相加","content":"题目链接 题目描述 给你两个非空的链表，表示两个非负的整数。它们每位数字都是按照逆序的方式存储的，并且每个节点只能存储一位数字。 请你将两个数相加，并以相同形式返回一个表示和的链表。 你可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例 1： 输入： l1 = [2,4,3], l2 = [5,6,4] 输出： [7,0,8] 解释： 342 + 465 = 807. 示例 2： 输入： l1 = [0], l2 = [0] 输出： [0] 示例 3： 输入： l1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9] 输出： [8,9,9,9,0,0,0,1] 提示： 每个链表中的节点数在范围 [1, 100] 内 0 &lt;= Node.val &lt;= 9 题目数据保证列表表示的数字不含前导零 个人题解 C++实现：ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) { ListNode *p = new ListNode(); ListNode *h = p; ListNode *p1 = l1, *p2 = l2; int f = 0; while(p1 != NULL &amp;&amp; p2 != NULL) { int sum = p1-&gt;val + p2-&gt;val + f; ListNode* t = new ListNode(sum % 10); if(sum &gt; 9) f = 1; else f = 0; p-&gt;next = t; p = t; p1 = p1-&gt;next; p2 = p2-&gt;next; } while(p1 != NULL) { int sum = p1-&gt;val + f; ListNode* t = new ListNode(sum % 10); if(sum &gt; 9) f = 1; else f = 0; p-&gt;next = t; p = t; p1 = p1-&gt;next; } while(p2 != NULL) { int sum = p2-&gt;val + f; ListNode* t = new ListNode(sum % 10); if(sum &gt; 9) f = 1; else f = 0; p-&gt;next = t; p = t; p2 = p2-&gt;next; } if (f == 1) { ListNode* t = new ListNode(1); p-&gt;next = t; } return h-&gt;next; } Python3实现：def addTwoNumbers(self, l1: ListNode, l2: ListNode) -&gt; ListNode: h = p = ListNode() f = 0 while l1 != None and l2 != None: sum = l1.val + l2.val + f t = ListNode(sum % 10) p.next = t p = t if sum &gt; 9: f = 1 else: f = 0 l1 = l1.next l2 = l2.next while l1 != None: sum = l1.val + f t = ListNode(sum % 10) p.next = t p = t if sum &gt; 9: f = 1 else: f = 0 l1 = l1.next while l2 != None: sum = l2.val + f t = ListNode(sum % 10) p.next = t p = t if sum &gt; 9: f = 1 else: f = 0 l2 = l2.next if f == 1: p.next = ListNode(1) return h.next Leetcode题解 C++实现：ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) { ListNode *head = nullptr, *tail = nullptr; int carry = 0; while (l1 || l2) { int n1 = l1 ? l1-&gt;val: 0; int n2 = l2 ? l2-&gt;val: 0; int sum = n1 + n2 + carry; if (!head) { head = tail = new ListNode(sum % 10); } else { tail-&gt;next = new ListNode(sum % 10); tail = tail-&gt;next; } carry = sum / 10; if (l1) { l1 = l1-&gt;next; } if (l2) { l2 = l2-&gt;next; } } if (carry &gt; 0) { tail-&gt;next = new ListNode(carry); } return head; } Python3实现：def addTwoNumbers(self, l1, l2): count = 0 ret = ListNode() tmp = ret while l1 or l2 or count: num = 0 if l1: num += l1.val l1 = l1.next if l2: num += l2.val l2 = l2.next if count: num += count count -= 1 count, num = divmod(num, 10) tmp.next = ListNode(num) tmp = tmp.next return ret.next divmod() 函数把除数和余数运算结果结合起来，返回一个包含商和余数的元组(a // b, a % b)。 ","link":"https://cisse-away.github.io/post/leetcode-shua-ti-bi-ji-liang-shu-xiang-jia/"},{"title":"Linux目录结构","content":"参考课程-蓝桥云课 参考课程-菜鸟教程 一、目录结构 FHS标准 FHS（英文：Filesystem Hierarchy Standard 中文：文件系统层次结构标准），多数 Linux 版本采用这种文件组织形式，FHS 定义了系统中每个区域的用途、所需要的最小构成的文件和目录同时还给出了例外处理与矛盾处理。 在 Linux 系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件: /etc: 这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动 /bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录 /bin, /usr/bin是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin则是给 root 使用的指令 /var: 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在 /var/log 目录下，另外 mail 的预设放置也是在这里。 使用tree查看目录结构：tree &lt;目录&gt; tree命令列出指定目录下的所有文件，包括子目录里的文件，详细参数说明 FHS 依据文件系统使用的频繁与否以及是否允许用户随意改动（注意，不是不能，学习过程中，不要怕这些），将目录定义为四种交互作用的形态，如下表所示： 二、目录路径 使用cd命令切换目录 进入上一级目录：cd .. 进入你的home目录：cd ~ 获取目前所在的工作目录的绝对路径名称：pwd ","link":"https://cisse-away.github.io/post/linux-mu-lu-jie-gou/"},{"title":"A better way to talk about love","content":"Speaker：Mandy Len Catron 视频链接 原视频 摘要 Most of us will probably fall in love a few times over the course of our lives, and in the English language, this metaphor, falling, is really the main way that we talk about that experience. 我们大多数人在一生中 会不止一次坠入爱河， 而在语言中，我们使用“坠入”一词， 很大程度上也形容了恋爱的体验。 Falling is accidental, it's uncontrollable. It's something that happens to us without our consent. And this is the main way we talk about starting a new relationship. “坠入”是意外的，是不可控制的。它的发生是未经自己允许的。而这就是我们讨论一段新恋情时的描述方式。 I want to focus on one metaphor in particular, which is the idea of love as madness. When I first started researching romantic love, I found these madness metaphors everywhere. The history of Western culture is full of language that equates love to mental illness. 我想着重谈谈这样一个比喻，那就是将爱情喻为疯狂。一开始研究浪漫爱情的时候，我发现这种比喻无处不在。西方历史文化中，有许多文字将爱情比喻为精神疾病。 I expected my first love to feel like madness, and of course, it met that expectation very well. But loving someone like that -- as if my entire well-being depended on him loving me back -- was not very good for me or for him. 我期盼着能在初恋中感受到疯狂，显然这个期望被很好的满足了。不过这样爱着一个人，好像我的全部都取决于他回馈的爱，对自己是很不好的，对他也一样。 It's kind of an interesting feedback loop. Love is powerful and at times painful, and we express this in our words and stories, but then our words and stories prime us to expect love to be powerful and painful. 这好像是一个有趣的反馈循环。爱情很伟大，但有时也让我们痛苦，我们用词句和故事来表达这点，然后这些文字又使得我们盲目期待爱情就应是伟大而痛苦的。 Johnson and Lakoff suggest a new metaphor for love: love as a collaborative work of art. 约翰逊和拉克夫提出了爱情的一种新比喻： 爱情是一个合作完成的艺术品。 Reframing love as something I get to create with someone I admire, rather than something that just happens to me without my control or consent, is empowering. 把爱情当作一个与我所爱的人共同创造的东西，而不是一个不经我控制或同意就发生在我身上的东西， 这想法非常激动人心。 The beautiful thing about the collaborative work of art is that it will not paint or draw or sculpt itself. This version of love allows us to decide what it looks like. 一个合作完成的艺术品的魅力所在，就是它不会自己描绘或塑造自己。这种爱情让我们主动决定它的美。 ","link":"https://cisse-away.github.io/post/a-better-way-to-talk-about-love/"},{"title":"Xposed框架、模块合集[转载]","content":"原链接 链接模块有失效可点击右下角评论会在第一时间补发 更多模块加入交流Q群太极：831335812 点击链接加入： https://jq.qq.com/?_wv=1027&amp;k=nOVamWyM ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊ 重要的事情说三遍！！！ 下载地址统一密码：OJBK 下载地址统一密码：OJBK 下载地址统一密码：OJBK ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊ 注意：如果您从未听说过此应用或从未接触过此方面的知识，建议您立刻关闭此页面 模块都有封号危险，有的几率更大一点而已，qx很少听说，qn会封号的听到的比较多 【QQ专用模块】 花Q： https://nalankang.lanzous.com/b00tg274j QQ复读机： https://nalankang.lanzous.com/b00u9f1yf H.entai.Q(增强模块)： https://nalankang.lanzous.com/b00u73uof 绿豆(增强模块)： https://nalankang.lanzous.com/b00u74h3c QN(增强模块)： https://nalankang.lanzous.com/b00tsdawd QX(增强模块)： https://www.lanzoui.com/b00u0c8md QQ瘦身(清理垃圾)： https://nalankang.lanzous.com/b00tszuzi ColorQQ2(主题美化)： https://nalankang.lanzous.com/b00u9sekh QQ省电模块： https://nalankang.lanzous.com/b00u9clng QScript脚本模块： https://nalankang.lanzous.com/b00u94zfe XAutoDaily(QQ自动签到)： https://nalankang.lanzous.com/b00udwffg ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【微信专用模块】 微X模块 ： https://www.lanzoui.com/b00tujq4j 微分组： https://nalankang.lanzous.com/b00u7165a 微群发： https://nalankang.lanzous.com/b00u7164j 微密友： https://nalankang.lanzous.com/b00u716hc 微增强： https://nalankang.lanzous.com/b00u7311i 微抖图： https://nalankang.lanzous.com/b00u7163i 畅玩微信： https://nalankang.lanzous.com/b00u757wh 企业助手： https://nalankang.lanzous.com/b00u716mh 微信学英语： https://nalankang.lanzous.com/b00udu6vg MDWechat(微信美化)： https://nalankang.lanzous.com/b00u7166b V++： https://nalankang.lanzous.com/iFHt3n8udfe ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊ 【系统增强模块】 全局复制： https://www.lanzoui.com/iNjcbh1ex8d Xp快译： https://nalankang.lanzous.com/tp/ipnxCn6pb3i 强制截图： https://www.lanzoui.com/i2d70h1ez8f 文本自定义： https://www.lanzoui.com/izfDpdcai0d 指纹zf： https://nalankang.lanzous.com/b00twbp5i 伪装WiFi连接： https://www.lanzoui.com/b00tvgw6f 原生音乐通知栏： https://www.lanzoui.com/b00tsmf3i 移除截屏延迟： https://nalankang.lanzous.com/isXezjesckd 短信验f·码提取： https://nalankang.lanzous.com/b00uaxssf xSuite(修改应用图标)： https://www.lanzoui.com/i49dhgt6zij 无障碍-Daemon： https://www.lanzoui.com/iRz8qim5hda 锁定·通知： https://www.coolapk.com/apk/me.singleneuron.locknotification Xposed edge por(边缘手势)： https://www.lanzoui.com/b00tvgqqj 锤子BigBang_XP专版： https://www.coolapk.com/apk/com.forfan.bigbang.coolapk 导航·栏·扩展模块： https://www.coolapk.com/apk/com.egguncle.xposednavigationbar ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【核心破解】 核心破解3.3(安卓10-11)： https://www.lanzoui.com/b00u2j7xc 核心破解2.2(安卓9-10)： https://wwi.lanzoui.com/iQS6Gjdk6zi 核心破解2.1(安卓8.x)： https://www.lanzoui.com/iq7ZYjdk6ve 核心破解2.0(安卓7.x)： https://wwi.lanzoui.com/ic56Ljgow5g 核心破解1.4(安卓4.x 5.x 6.x)： https://www.lanzoui.com/iaKd1jdk6pi ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【蚂·蚁森·林模块】 蚂·蚁森·林XQE版： https://www.lanzoui.com/b00tsmwvi 蚂·蚁森·林秋风版： https://www.lanzoui.com/b00tphmhg 蚂·蚁森·林蜡笔小新版： https://nalankang.lanzous.com/b00u7fmyj ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【应用变量模块】 应用伪装： https://nalankang.lanzous.com/b00u9amhe 应用变量： https://www.lanzoui.com/iFeH6h1ehxc ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【模拟位置】 模拟位置： https://www.lanzoui.com/b00tvpb1i lataclysm(伪装位置)： https://nalankang.lanzous.com/izZ3xljlaqh ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【哔哩哔哩模块】 哔哩补丁： https://nalankang.lanzous.com/igGPQlrcepe 哔哩漫游： https://www.lanzoui.com/b00ttp0kj ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【钉钉模块】 钉钉打卡： https://www.lanzoui.com/b00tw4j8d 钉钉助手： https://www.lanzoui.com/b00ty6xeh ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【抖·音模块】 抖·音助手： https://www.lanzoui.com/b00tuzfsd 抖·音+： https://nalankang.lanzous.com/b00u6mqej 抖·音伴侣： https://www.lanzoui.com/b00t5ypuj AdAway： https://www.lanzoui.com/b00tsmbgh 要你命三千： https://www.lanzoui.com/b00tw4jhc 诺诺诺嗯： https://wwi.lanzous.com/b00u5t5wh HookBox(超级Hook盒子)： https://nalankang.lanzous.com/b00uembsb ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【皮皮虾快手模块】 皮皮虾助手： https://www.lanzoui.com/b00tvgswh 诺诺诺嗯： https://wwi.lanzous.com/b00u5t5wh 要你命三千： https://www.lanzoui.com/b00tw4jhc 快手+： https://nalankang.lanzous.com/b00u8r2cf AdAway： https://www.lanzoui.com/b00tsmbgh ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【百度贴吧模块】 帖吧TS(帖吧净化)： https://www.lanzoui.com/b00tzwihc 百度贴吧伴侣： https://www.lanzoui.com/b00u0l13g 百度贴吧助手： https://nalankang.lanzous.com/b00uecoyf ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【魅族专用模块】 Flyme8 助手： https://www.lanzoui.com/b00t637bc FXposed： https://www.lanzoui.com/idOHzht8kva MZXPT(魅族工具箱)： https://www.lanzoui.com/b00tw4g4b ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【MIUI专用模块】 小米净化： https://www.lanzoui.com/b00tyatha ChiMi(MIUI增强模块)： https://www.lanzoui.com/b00t6n0ti CustoMIUIzer(MIUI专用)： https://www.lanzoui.com/b00tyaawb XMiTools： https://www.coolapk.com/apk/com.tianma.tweaks.miui Miui12双排状态栏： https://www.coolapk.com/apk/com.lz.teemo MIUI双开限制解除模块： https://nalankang.lanzous.com/b00u9zola 解锁MIUI键盘优化： https://nalankang.lanzous.com/b00ud00la MIUI全局高帧： https://nalankang.lanzous.com/b00ud00wb 米窗-全局小窗： https://www.coolapk.com/apk/com.sunshine.freeform ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【一加专用模块】 万·物： https://www.coolapk.com/apk/com.bangtu.opaod OnePlus Themes： https://www.coolapk.com/apk/com.jizhi.optheme 一加拓展XP： https://www.coolapk.com/apk/com.davidking.optools 一加zw拓展： https://www.coolapk.com/apk/com.davidking.xposed.opfingerprint 一加状态栏网速： https://www.coolapk.com/apk/me.seasonyuu.xposed.networkspeedindicator.h2os ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【应用破.解】 BuyTool幸运内购： https://www.lanzoui.com/b00ttwnti ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【网易云音乐模块】 杜比大喇叭β版： https://nalankang.lanzous.com/b00u9dtif UnblockMusic.Pro： https://nalankang.lanzous.com/b00u9dtwj 网易云音乐插件： https://nalankang.lanzous.com/iFEzDgt5bri 云村清洁工： https://nalankang.lanzous.com/in7DElr49xe 网易云专辑封面禁旋转： https://nalankang.lanzous.com/b00u9ezsh ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【运动修改模块】 运动修改器： https://www.lanzoui.com/b00to5bpa 运动加速器： https://nalankang.lanzous.com/b00u8tp0d 运动模拟器： https://nalankang.lanzous.com/iAVp7lridyd ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【应用管理模块】 应用控制器： https://www.lanzoui.com/ili9Rgt6rib 应用管理： https://www.lanzoui.com/ibNpugt6sih 阻止运行： https://nalankang.lanzous.com/b00tjjhaf 绿色守护： https://www.lanzoui.com/b00tw4f3e Thanox(灭霸)： https://nalankang.lanzous.com/b00ucbm7i ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【隐私保护模块】 X隐私保护： https://nalankang.lanzous.com/b00uanape XPrivacyLua： https://nalankang.lanzous.com/b00u9131a ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【状态栏变色模块】 PerfectColorBar： https://www.lanzoui.com/irKuHgt707e 微聚旧版： https://www.coolapk.com/apk/io.tvcfish.xposedbox 微聚新版： https://www.coolapk.com/apk/io.ikws4.weiju ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【隐藏Root模块】 zuper(隐藏Xposed和Root)： https://nalankang.lanzous.com/i2iuJgvqumf RootCloak(隐藏Root)： https://wwi.lanzoui.com/iD7LLjks9ab ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【对话框取消模块】 对话框取消beta版： https://nalankang.lanzous.com/iX6Cvl4nzjc 对话框取消： https://nalankang.lanzous.com/iHJzpl4nyuh ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【去广告模块】 大圣净化： https://nalankang.lanzous.com/b00u8uhij 悟空加速： https://nalankang.lanzous.com/b00u8uhja AdBlocker： https://nalankang.lanzous.com/b00tf5ive 油guan去广告模块合集： https://nalankang.lanzous.com/b00u8om6b ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【存储重定向模块】 XInternalSD： https://nalankang.lanzous.com/i44wggt6jni SD重定向： https://nalankang.lanzous.com/i1U2bgt6jgb LT NoLitter： https://nalankang.lanzous.com/iyvTfgt6jmh ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【建·行工·行模块】 建H忽略root修复版： https://www.lanzoui.com/ifQ6Chnuekj 建H殺S： https://www.lanzoui.com/insPchnubsj BankRX(建H工H忽略root)： https://www.lanzoui.com/iQH4Nhpiuni ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【微博模块】 微博+： https://nalankang.lanzous.com/b00u8r25i AdAway： https://www.lanzoui.com/b00tsmbgh ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【重力工具箱】 GravityBox[安卓8] ： https://nalankang.lanzous.com/b00u9xezc GravityBox[安卓9] ： https://nalankang.lanzous.com/b00u9xf7a GravityBox[安卓10] ： https://nalankang.lanzous.com/b00u9xfdg GravityBox[安卓11] ： https://nalankang.lanzous.com/b00u9xffi ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【小红米模块】 X小红书： https://nalankang.lanzous.com/b00u9cabi 小红书+： https://nalankang.lanzous.com/b00ucb8pc ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【其他模块】 放开我的剪贴板： https://www.coolapk.com/apk/cn.nlifew.clipmgr 定时清理(清理文件和文件夹)： https://www.lanzoui.com/i62C4ilqksd xposed黑名单(禁用Xposed)： https://www.lanzoui.com/iuJe2h1efgd 去伱大.爷的内置浏览器： https://www.lanzoui.com/iJLBYiqgs6j AllTrans(应用内翻译)： https://www.coolapk.com/feed/23531453?shareKey=ODIwMTk2NDJhMzVkNWZkNzE1MGY~&amp;shareUid=462071&amp;shareFrom=com.coolapk.market_11.0-beta4 知了(知乎增强模块)： https://www.lanzoui.com/b00tt2w1e 面具hide增强模块： https://www.lanzoui.com/iMmPihuq4wj 血轮眼(禁用服务)： https://nalankang.lanzous.com/b00ucdbhe 你才长按复制访问： https://www.lanzoui.com/ioyl2i0bd5c 导航栏扩展模块： https://www.coolapk.com/apk/com.egguncle.xposednavigationbar 今日头条+： https://nalankang.lanzous.com/b00u8r28b 假装分享： https://nalankang.lanzous.com/iOJ09lmdg4b 学习强国模块： https://nalankang.lanzous.com/b00ubkikf 腾爱优芒豆去广告： https://www.lanzoui.com/b00tsmwqd Magisk hide(面具增强模块)： https://www.lanzoui.com/iMmPihuq4wj 太极更新宝(太极阴专用)： https://nalankang.lanzous.com/itU7Fkwxova 一个让你感觉卧曹卧曹的模块： https://www.lanzoui.com/ii7Hzgt6zsj 应用设置重生版： https://nalankang.lanzous.com/b00u7e3ni 上帝模式： https://www.lanzoui.com/ixLFIhnvewh 飞书助手： https://www.lanzoui.com/b00tx61rc 数据过滤： https://nalankang.lanzous.com/b00ubhcpg 阿里系Xposed反检测： https://nalankang.lanzous.com/ikPQ5n2y3pc 淘饭饭(自动领券并返利)： https://nalankang.lanzous.com/b00ue7nlg ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【Magisk面具】 MagiskManager(面具权限管理器)： https://nalankang.lanzous.com/b00u7gcij Magisk卡刷包： https://nalankang.lanzous.com/b00u7gexg ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【Edxposed框架】 Riru Core模块： https://nalankang.lanzous.com/b00u7gi9g Edxposed阿尔法(SandHook版)： https://nalankang.lanzous.com/b00u7gnpc Edxposed阿尔法(YAHFA版)： https://nalankang.lanzous.com/b00u7gq8d Edxposed金丝雀(SandHook&amp;YAHFA)： https://nalankang.lanzous.com/b00u7h7ah ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【LSPosed框架】 Riru模块： https://nalankang.lanzous.com/b00u7gi9g LSPosedManager管理器： https://nalankang.lanzous.com/b00u7ryaj LSPosed： https://nalankang.lanzous.com/b00u7rzoj ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【应用分享】 搞机助手： https://www.lanzoui.com/b00tugduj HiShoot2i(带壳截图)： https://www.coolapk.com/feed/23323081?shareKey=NDc1ZTcyNzA5MzJiNWZjODYxMDE~&amp;shareUid=462071&amp;shareFrom=com.coolapk.market_11.0-beta2 应用转生(Xposed框架免Root)： https://www.lanzoui.com/b00u1wq0f UC浏览器(去广告)： https://www.lanzoui.com/iVQX6gbweha ES管理器美化版： 【 https://nalankang.lanzous.com/iFtrkiltcza 】 【 https://nalankang.lanzous.com/iyenujj4y3e 】 幸运破解器： https://www.lanzoui.com/b00u06nfa Speedtest(网速测试)： https://www.lanzoui.com/b00u06nch 温控拜拜(删除温控)： https://www.lanzoui.com/i8YXOig4oob 镜像助手(提取、刷入REC)： https://www.lanzoui.com/ilViPijcsna TWRP(下载刷入TWRP)： https://www.lanzoui.com/b00u06y2d APK编辑器： https://www.lanzoui.com/b00u06q0d 微软远程桌面： https://www.lanzoui.com/b00u06qib 歌词适配(网易云音乐下载)： https://www.lanzoui.com/b00u06rdc ADM(下载工具)： https://www.lanzoui.com/b00u06sid IDM+(下载工具)： https://www.lanzoui.com/b00u06sje 快图浏览器(魔改版)： https://www.lanzoui.com/b00u06uhe 闪电下载： https://www.lanzoui.com/b00u06tlc 联通营业厅(去广告)： https://www.lanzoui.com/b00u06vxg ES文件管理器(VIP)： https://www.lanzoui.com/b00u2b9pi 黑阈(阻止运行)： https://nalankang.lanzous.com/b00u71uwb 云闪付(谷歌版)： https://nalankang.lanzous.com/b00u71vte Nova启动器： https://nalankang.lanzous.com/b00u7cmgd 钛备份： https://nalankang.lanzous.com/b00u85wmd 山寨版谷哥市场： https://nalankang.lanzous.com/b00u7hyze RE文件管理器： https://nalankang.lanzous.com/b00u85wja ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【Recovery卡刷包】 禁用所有Magisk模块： https://nalankang.lanzous.com/iBN2Fmzpv8b 删除Riru和LSPosed： https://nalankang.lanzous.com/ipIkEmzpv7a ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【面具字体】 筑紫A丸： https://www.lanzoui.com/b00tg4gab ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【面具模块】 AD_hosts(为去广告而生)： https://nalankang.lanzous.com/b00ueb7ve ADB&amp;Fastboot模块： https://nalankang.lanzous.com/iM73flgxzvg Busybox模块： https://nalankang.lanzous.com/iXg8Dlgy02d 音量键极速救砖： https://nalankang.lanzous.com/iacl9lgy0vc ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【微信美化】 微X主题〈带土与琳〉： https://www.coolapk.com/feed/24107117?shareKey=YzMwOGQ2YjljNTM4NjAwMWFkOWY~&amp;shareUid=462071&amp;shareFrom=com.coolapk.market_11.0-rc2 微X主题〈Q版蜘蛛侠〉 https://www.coolapk.com/feed/24230601?shareKey=YzBiYjhjMGQzMDgyNjAwMWFkOWQ~&amp;shareUid=462071&amp;shareFrom=com.coolapk.market_11.0-rc2 微X主题〈漫〉 https://www.coolapk.com/feed/24081843?shareKey=MTMwNTdjMjI0ZmQ5NjAwMWFkYzA~&amp;shareUid=462071&amp;shareFrom=com.coolapk.market_11.0-rc2 微X主题〈无题〉 https://www.coolapk.com/feed/22527630?shareKey=NTJiZTYyN2IzZmIwNjAwMWFlYTY~&amp;shareUid=462071&amp;shareFrom=com.coolapk.market_11.0-rc2 ﹋﹊﹋﹊﹋﹊﹋﹊﹋﹊﹋ 【PC端工具】 秋之盒： https://nalankang.lanzous.com/b00u8myid ADB工具： https://nalankang.lanzous.com/b00u8mv1i j 搞机工具箱： https://nalankang.lanzous.com/b00u8nnqb 搞机助手PC版： https://nalankang.lanzous.com/b00u8muij 小米线刷工具售后版： https://cloud.189.cn/t/e6Vra2jAfuae ","link":"https://cisse-away.github.io/post/xposed-kuang-jia-mo-kuai-he-ji-zhuan-zai/"},{"title":"Leetcode刷题笔记—两数之和","content":"题目链接 题目描述 给定一个整数数组nums和一个整数目标值target，请你在该数组中找出和为目标值的那两个 整数，并返回它们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。 你可以按任意顺序返回答案。 示例 1： 输入： nums = [2,7,11,15], target = 9 输出： [0,1] 解释： 因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。 示例 2： 输入： nums = [3,2,4], target = 6 输出： [1,2] 示例 3： 输入： nums = [3,3], target = 6 输出： [0,1] 提示： 2 &lt;= nums.length &lt;= 103 -109 &lt;= nums[i] &lt;= 109 -109 &lt;= target &lt;= 109 只会存在一个有效答案 个人题解 C++实现:vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) { int n = nums.size(); vector&lt;int&gt; r; int f = 0; for(int i = 0; i &lt; n &amp;&amp; f == 0; i++) { for(int j = i + 1; j &lt; n; j++) { if(nums[i] + nums[j] == target) { r.push_back(i); r.push_back(j); f = 1; break; } } } return r; } Python3实现:def twoSum(self, nums: List[int], target: int) -&gt; List[int]: list = []; f = 0; for i in range(len(nums)): for j in range(i + 1, len(nums)): if nums[i] + nums[j] == target: list.append(i); list.append(j); f = 1; break; if f == 1: break; return list; Leetcode题解 Python3代码简化：def twoSum(self, nums: List[int], target: int) -&gt; List[int]: for i in range(len(nums)): for j in range(i + 1, len(nums)): if nums[i] + nums[j] == target: return i, j 一次循环优化：def twoSum(self, nums: List[int], target: int) -&gt; List[int]: for i in range(len(nums)): if (target - nums[i] in nums) &amp; (i != nums.index(target - nums[i])): return i, nums.index(target - nums[i]) 用字典模拟哈希求解：def twoSum(self, nums: List[int], target: int) -&gt; List[int]: dict = {} for i, n in enumerate(nums): if target - n in dict: return dict[target - n], i else: dict[n] = i 对于一个可迭代的/可遍历的对象(如列表、字符串), enumerate将其组成一个索引序列，利用它可以同时获得索引和值 C++哈希解法：vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) { unordered_map&lt;int,int&gt; m; for(int i = 0; i &lt; nums.size(); i++) { if(m.find(target-nums[i]) != m.end()) return {m[target-nums[i]], i}; m[nums[i]] = i; } return {}; } unordered_map更适合查找类问题，map更适用于有顺序要求问题 ","link":"https://cisse-away.github.io/post/leetcode-shua-ti-bi-ji-liang-shu-zhi-he/"},{"title":"Linux文件权限","content":"参考课程-蓝桥云课 一、查看文件权限 使用较长格式列出文件 ls -l 输出解读： 文件类型和权限： Linux里面一切皆文件 一个目录同时具有读权限和执行权限才可以打开并查看内部文件，而一个目录要有写权限才允许在其中创建其它文件 二、变更文件所有者 sudo chown &lt;变更后的文件所有者名&gt; &lt;要变更的文件&gt; 三、修改文件权限 二进制数字表示 每个文件有三组固定的权限，分别对应拥有者，所属用户组，其他用户，这个顺序是固定的 文件的读写执行对应字母 rwx，以二进制表示就是 111，用十进制表示就是7 例如：rw-rw-rw-换成十进制表示就是666 修改权限示例：chmod 600 &lt;对应文件&gt; 表示只有自己拥有读写权限 加减赋值操作chmod go-rw &lt;对应文件&gt; 同样能够达到上例效果，其中g、o 还有u分别表示 group（用户组）、others（其他用户） 和 user（用户），+和-分别表示增加和去掉相应的权限。 ","link":"https://cisse-away.github.io/post/linux-wen-jian-quan-xian/"},{"title":"A delightful way to teach kids about computers","content":"Speakers：Linda Liukas 视频 原视频 摘要 Code is the next universal language. 编码是下一个世界性的语言。 But unless we give them tools to build with computers, we are raising only consumers instead of creators. 但除非我们给他们工具，让他们能够利用电脑构建世界我们培养的就只是消费者，而不是创造者。 Every time I would run into a problem in trying to teach myself programming like, &quot;What is object-oriented design or what is garbage collection?&quot;, I would try to imagine how a six-year-old little girl would explain the problem. 每当我学习编程，学习诸如何为面对对象设计？何为碎片帐集？我都会想象一个六岁的女孩会如何解释这些问题。 And that's the moment I'm looking for, the moment when the kid realizes that the world is definitely not ready yet, that a really awesome way of making the world more ready is by building technology and that each one of us can be a part of that change. 这就是我想要的时刻！ 在这一时刻，孩子们意识到世界还远没有妥善到位想要让世界变得更完善，有一个好方法就是开发科技。 我们每个人都可能成为改变中的一部分。 Programming gives me this amazing power to build my whole little universe with its own rules and paradigms and practices. Create something out of nothing with the pure power of logic. 编程赋予我这一神奇的力量使我能够建立自己的一方小天地按照自己的规则、范式和实践依靠逻辑的纯粹力量，实现从无到有的创造。 ","link":"https://cisse-away.github.io/post/a-delightful-way-to-teach-kids-about-computers/"},{"title":"Linux用户管理","content":"参考课程-蓝桥云课 一、查看用户 who am i # 或 who mom likes # 或 who -m # 输出 shiyanlou pts/0 2021-04-25 15:16 (:1.0) 第一列表示打开当前伪终端的用户的用户名（要查看当前登录用户的用户名，去掉空格直接使用 whoami 即可） 第二列的 pts/0 中 pts 表示伪终端，所谓伪是相对于 /dev/tty 设备而言的，pts/0 后面那个数字就表示打开的伪终端序号 第三列则表示当前伪终端的启动时间 who 命令其它常用参数 参数 说明 -a 打印能打印的全部 -d 打印死掉的进程 -m 同am i，mom likes -q 打印当前登录用户数及用户名 -u 打印当前登录用户登录信息 -r 打印运行等级 二、创建用户 su，su- 与 sudo su &lt;user&gt; 可以切换到用户 user，执行时需要输入目标用户的密码 sudo &lt;cmd&gt; 可以以特权级别运行 cmd 命令，需要当前用户属于 sudo 组，且需要输入当前用户的密码 su - &lt;user&gt; 命令也是切换用户，但是同时用户的环境变量和工作目录也会跟着改变成目标用户所对应的 新建用户： sudo adduser &lt;username&gt; 首先输入当前用户密码 其次输入新用户密码 之后的选项可直接回车使用默认值 这个命令不但可以添加用户到系统，同时也会默认为新用户在 /home 目录下创建一个工作目录 adduser和useradd的区别：useradd只创建用户，不会创建用户密码和工作目录，创建完了需要使用passwd &lt;username&gt;去设置新用户的密码。adduser在创建用户的同时，会创建工作目录和密码（提示你设置），做这一系列的操作。 切换登录用户： su -l &lt;username&gt; 退出当前用户： exit或Ctrl+D 三、用户组 查看用户所在用户组 使用groups命令 groups &lt;username&gt; # 输出 &lt;username&gt; : &lt;group_name&gt; 其中冒号之前表示用户，后面表示该用户所属的用户组。每次新建用户若不指定用户组，默认会自动创建一个与用户名相同的用户组 如果没有指定用户名则默认为当前进程用户 查看/etc/group文件 grep shiyanlou /etc/group # 输出 shiyanlou:x:5000: 每条记录格式为group_name:password:GID:user_list，如果用户的 GID 等于用户组的 GID，那么最后一个字段 user_list 就是空的 通过id命令 id shiyanlou # 输出 uid=5000(shiyanlou) gid=5000(shiyanlou) 组=5000(shiyanlou) 将其它用户加入sudo用户组groups lilei # 输出 lilei : lilei 执行 sudo usermod -G sudo lileigroups lilei # 输出 lilei : lilei sudo 四、删除用户和用户组 删除用户sudo deluser lilei --remove-home 使用 --remove-home 参数在删除用户时候会一并将该用户的工作目录一并删除 删除用户组 删除用户组可以使用 groupdel 命令，倘若该群组中仍包括某些用户，则必须先删除这些用户后，才能删除群组 ","link":"https://cisse-away.github.io/post/linux-yong-hu-ji-wen-jian-quan-xian-guan-li/"},{"title":"Inside the mind of a master procrastinator","content":"Speaker：Tim Urban 视频 原视频 摘要 Both brains have a Rational Decision-Maker in them, but the procrastinator's brain also has an Instant Gratification Monkey. 两种大脑里头都有一个理性的决策制定者，但是拖延者的大脑里，还有一只叫即时满足的猴子。 Well, turns out the procrastinator has a guardian angel, someone who's always looking down on him and watching over him in his darkest moments -- someone called the Panic Monster. 其实拖延者有个守护天使，总是看不起他并且看守着他，在那些最黑暗的时刻 它被称为恐慌怪兽。 The Panic Monster is dormant most of the time, but he suddenly wakes up anytime a deadline gets too close or there's danger of public embarrassment, a career disaster or some other scary consequence. 恐慌怪兽大部分时间都在冬眠，但是截止期很靠近的时候或者处于在公众面前出丑的危险中时，或面临事业灾难时，或有其他可怕的后果时，它就会突然醒来。 It turns out that there's two kinds of procrastination. Everything I've talked about today, the examples I've given, they all have deadlines. And when there's deadlines, the effects of procrastination are contained to the short term because the Panic Monster gets involved. But there's a second kind of procrastination that happens in situations when there is no deadline. 结果我发现原来有两种拖延。我今天谈到的，上面举过的例子都有截止期。有截止期的时候，拖延的影响 被限制在一个较短的期限内因为恐慌怪兽会介入。但是对于第二种拖延来说它发生在没有截止期的情况下。 I don't think non-procrastinators exist. And some of you may have a healthy relationship with deadlines 我认为不存在不拖延的人。也许有些人可以很好的面对截止日期。 I call this a Life Calendar. That's one box for every week of a 90-year life. That's not that many boxes, especially since we've already used a bunch of those. So I think we need to all take a long, hard look at that calendar. We need to think about what we're really procrastinating on, because everyone is procrastinating on something in life. 我把它叫做生命日历。假设一个人可以活到90岁，每个星期是一个格子。其实没多少格子，尤其是我们已经活了这么多年。我认为我们都需要花些时间，认真看一下这个生命日历。我们需要认真思考我们真正拖延的是什么，因为每个人都在拖延某件事。 ","link":"https://cisse-away.github.io/post/inside-the-mind-of-a-master-procrastinator/"},{"title":"Linux基本操作","content":"参考课程-蓝桥云课 常用快捷键 按键 作用 Ctrl+c 强行终止当前程序 Ctrl+s 暂停当前程序，暂停后按下任意键恢复运行 Ctrl+z 将当前程序放到后台运行，恢复到前台为命令fg Ctrl+a 将光标移至输入行头，相当于Home键 Ctrl+e 将光标移至输入行末，相当于End键 Ctrl+k 删除从光标所在位置到行末 常用通配符 字符 含义 * 匹配 0 或多个字符 ? 匹配任意一个字符 [list] 匹配 list 中的任意单一字符 [^list] 匹配 除 list 中的任意单一字符以外的字符 [c1-c2] 匹配 c1-c2 中的任意单一字符 如：[0-9]，[a-z] {string1,string2,...} 匹配 string1 或 string2 (或更多)其一字符串 {c1..c2} 匹配 c1-c2 中全部字符 如{1..10} 查看命令使用方式 man &lt;command_name&gt; 使用/&lt;关键字&gt;搜索你要找的关键字，查找完毕后你可以使用n键切换到下一个关键字所在处，shift+n为上一个关键字所在处。 使用Space（空格键）翻页 &lt;command_name&gt; --help 大部分命令都会带有的参数，用于快速查看命令某个参数作用 ","link":"https://cisse-away.github.io/post/linux/"},{"title":"Why you should make useless things","content":"Speaker：Simone Giertz 视频 原视频 摘要 As we all know, the easiest way to be at the top of your field is to choose a very small field. 大家都知道， 成为你所在行业顶尖人物的 最简单方法， 就是选择一个非常小的行业。 As I kept on learning about hardware, for the first time in my life, I did not have to deal with my performance anxiety. And as soon as I removed all pressure and expectations from myself, that pressure quickly got replaced by enthusiasm, and it allowed me to just play. 因为当我对硬件的学习更加深入的时候，我人生中第一次，不用应对自我表现方面的焦虑。并且当我卸除了所有对自己的压力与期待后，压力很快就被热情取代，我就可以只是瞎玩了。 Because it's this acknowledgment that you don't always know what the best answer is. And it turns off that voice in your head that tells you that you know exactly how the world works. 因为你已经承认，你不一定知道最好的答案是什么。它让你不再盲目认为自己已经完全了解世界怎么运作。 And maybe a toothbrush helmet isn't the answer, but at least you're asking the question. 可能一台牙刷头盔并不是标准答案，但至少你提出了问题。 ","link":"https://cisse-away.github.io/post/why-you-should-make-useless-things/"},{"title":"How to get empowered, not overpowered, by AI","content":"Speaker：Max Tegmark 视频 原视频 摘要 We also have to figure out, if we're going to be really ambitious, how to steer it and where we want to go with it. 如果我们有足够的雄心壮志，就应当想出如何控制它们的方法，希望它朝着怎样的方向前进。 This is the definition of artificial general intelligence -- AGI, which has been the holy grail of AI research since its inception. 这就成了通用人工智能（Artificial general intelligence）——AGI，从一开始它就是人工智能研究最终的圣杯。 Which means that there's a possibility that further AI progress could be way faster than the typical human research and development timescale of years, raising the controversial possibility of an intelligence explosion where recursively self-improving AI rapidly leaves human intelligence far behind, creating what's known as superintelligence. 那就意味着有可能，进一步提升人工智能水平将会进行得非常迅速，甚至超越用年份来计算时间的典型人类研究和发展，提高到一种极具争议性的可能性，那就是智能爆炸，即能够不断做自我改进的人工智能很快就会遥遥领先人类，创造出所谓的超级人工智能。 But this is going to require a change of strategy because our old strategy has been learning from mistakes. 但这也需要策略上的改变，因为我们以往的策略往往都是从错误中学习的。 One is that we should avoid an arms race and lethal autonomous weapons. Another Asilomar AI principle is that we should mitigate AI-fueled income inequality. You'll appreciate this principle that we should invest much more in AI safety research. 其中一条规则是我们需要避免军备竞赛，以及致命的自动化武器出现；另一条阿西洛玛人工智能会议的原则是，我们应该要减轻由人工智能引起的收入不平等；你们会感谢这条准则，我们应该要投入更多以确保对人工智能安全性的研究。 And this AI safety work has to include work on AI value alignment, because the real threat from AGI isn't malice, like in silly Hollywood movies, but competence -- AGI accomplishing goals that just aren't aligned with ours. 这项人工智能安全性的工作必须包含对人工智能价值观的校准，因为 AGI 会带来的威胁通常并非出于恶意——就像是愚蠢的好莱坞电影中表现的那样，而是源于能力——AGI想完成的目标与我们的目标背道而驰。 With friendly AI, we could simply build all of these societies and give people the freedom to choose which one they want to live in because we would no longer be limited by our intelligence, merely by the laws of physics. 有了友善的人工智能，我们就能轻而易举地建立这些社会，让大家有自由去选择想要生活在怎样的社会里，因为我们不会再受到自身智慧的限制，唯一的限制只有物理的定律。 We can be ambitious -- thinking hard about how to steer our technology and where we want to go with it to create the age of amazement. 我们可以拥有雄心壮志——努力去找到操控我们科技的方法，以及向往的目的地，创造出真正令人惊奇的时代。 ","link":"https://cisse-away.github.io/post/how-to-get-empowered-not-overpowered-by-ai/"},{"title":"The beauty of being a misfit","content":"Speaker：Lidia Yuknavitch 视频 原视频 摘要 It's a person who sort of missed fitting in. Or a person who fits in badly. Or this: &quot;a person who is poorly adapted to new situations and environments.&quot; 这样的人是一个无法融入，或者融入得很差的人，或者“一个很难适应新位置和新环境的人。” Misfit people -- we don't always know how to hope or say yes or choose the big thing, even when it's right in front of us. 异类经常不知道怎样期待和回答也不知道在大事面前如何选择，哪怕它们就在我们面前。 It's called the misfit's myth. And it goes like this: even at the moment of your failure, right then, you are beautiful. You don't know it yet, but you have the ability to reinvent yourself endlessly. That's your beauty. 它是一种叫做异类的神话：即使那时候你失败了，失败的你也是美好的。你可能没有发觉那个不断地试图重生的你是最美丽的。 ","link":"https://cisse-away.github.io/post/the-beauty-of-being-a-misfit/"},{"title":"Math is the hidden secret to understanding the world","content":"Speaker：Roger Antonsen 视频 原视频 摘要 My claim is that understanding has to do with the ability to change your perspective. 我认为理解是一种能力，转变(固有)观点的能力。 My claim is that mathematics has to do with patterns. 我的观点是，数学跟模式有关。 My definition of mathematics is the following: First of all, it's about finding patterns. Second of all, I think it is about representing these patterns with a language. It's also about making assumptions and playing around with these assumptions and just seeing what happens. And finally, it's about doing cool stuff. Mathematics enables us to do so many things. 我对数学有一个的定义： 首先，数学的关键是寻找模式。其次，我认为数学是一种语言， 用来描述各种模式。同时数学也需要进行假设，对假设进行多方验证看看结果如何。最后，数学可以用来做很酷的事情。 能帮我们完成很多事。 When you change your perspective, and if you take another point of view, you learn something new about what you are watching or looking at or hearing. 当你换一种角度来看问题，当你接受另一种观点，你就能在所见所闻中，学到新的东西。 Every mathematical equation where you use that equality sign is actually a metaphor. It's an analogy between two things. You're just viewing something and taking two different points of view, and you're expressing that in a language. 每一个使用等号连接的数学方程 实际上都是隐喻。是两种事物间的类比。你观察一件事情，产生两种观点，然后用一种语言来表达。 My claim is that you understand something if you have the ability to view it from different perspectives. 我认为只有当我们 从多个角度去审视同一事物时，才能说我们理解了它。 If you're able to view a structure from the inside, then you really learn something about it. That's somehow the essence of something. 如果你能从一个结构的内部去进行观察，那你就能够真正认识它。认识到它的本质。 So my conclusion is the following: understanding something really deeply has to do with the ability to change your perspective. 所以我的结论是：深入的理解一件事 与转换角度的能力密切相关。 ","link":"https://cisse-away.github.io/post/math-is-the-hidden-secret-to-understanding-the-world/"},{"title":"Poverty isn't a lack of character; it's a lack of cash","content":"Speaker：Rutger Bregman 视频 原视频 摘要 The effects of living in poverty, it turns out, correspond to losing 14 points of IQ. 在贫困中生活的结果就是—智商降低14点。 You all know this feeling, when you've got too much to do, so the long-term perspective goes out the window. 你们都知道这种感觉，当你有太多事情要做时，这就导致你无法从长远的角度去思考。 The essence of poverty,&quot; he wrote back then, is that it &quot;annihilates the future. 他在书中写到：&quot;贫穷的本质，是摧毁未来。&quot; Here's my dream: I believe in a future where the value of your work is not determined by the size of your paycheck, but by the amount of happiness you spread and the amount of meaning you give. I believe in a future where the point of education is not to prepare you for another useless job but for a life well-lived. I believe in a future where an existence without poverty is not a privilege but a right we all deserve. 这是我的梦想：我相信在未来，你工作的价值不由你赚的钱决定，而是由你传播的快乐和你创造的意义决定。我相信在未来，教育的目的不在于为一个无意义的工作做好准备，而在于为有意义的一生做好准备。我相信在未来，脱离贫困不是一项特权，而是我们应得的一项权利。 ","link":"https://cisse-away.github.io/post/poverty-isnt-a-lack-of-character-its-a-lack-of-cash/"},{"title":"How boredom can lead to your most brilliant ideas","content":"Speaker：Manoush Zomorodi 视频 原视频 摘要 It turns out that when you get bored,you ignite a network in your brain called the &quot;default mode. 其实让你感觉无聊的时候，你大脑里一个叫做“默认模式”的系统启动了 Once you start daydreaming and allow your mind to really wander, you start thinking a little bit beyond the conscious, a little bit into the subconscious, which allows sort of different connections to take place. 当你开始做白日梦、让你的思想四处游荡，你的思考有一点偏离清醒的意识，更偏向于潜意识，使得各种各样的联想开始产生。 Every time you shift your attention from one thing to another, the brain has to engage a neurochemical switch that uses up nutrients in the brain to accomplish that. 每次你将你的注意力从一件事情转移到另一件事情上，大脑都必须进行一次神经化学的转换，并且消耗掉大脑存储的营养。 We find that when people are stressed, they tend to shift their attention more rapidly. 我们发现当人们压力很大时，他们容易更快地来回转换注意力。 ","link":"https://cisse-away.github.io/post/how-boredom-can-lead-to-your-most-brilliant-ideas/"},{"title":"The skill of  self confidence","content":"Speaker：Ivan Joseph 视频 油管 摘要 I use the defination of self-confidence to be the ability or the belief to believe in yourself, to accomplish any task, no matter the odds, no matter the difficulty, no matter the adversity. 我把自信定义为相信自己有能力或是一种信念相信自己，完成各种挑战，不论胜算多大，不论多艰难，哪怕身处逆境 The problelm is, we expect to be self-confident but we can't be unless the skill or the task we're doing is not novel, is not new to us. 关键在于我们期望自己有自信，但除非需要的技能或需要完成的任务是我们所熟悉的，否则我们不会有自信 Because we all repeat something but very few of us really will persist. 因为我们都能重复做一件事情，但很少有人会坚持住 We know that our thoughts influence actions, why do we want to say that negative self-talk to ourselves? 我们知道思想会影响行为，为什么我们还和自己说那些负面的话呢？ Get away from the people who will tear you down. 远离那些会打击你自信的人 No one will believe in you unless you do. 没有人会相信你除非你先相信自己 We're supposed to be different, folks. And when people look at us, believe in yourself. 我们生来就该与众不同。当人们的眼光投向我们时，只管相信自己 ","link":"https://cisse-away.github.io/post/the-skill-of-self-confidence/"},{"title":"Your brain on video games","content":"Speaker：Daphne Bavelier 视频 原视频 摘要 Their vision is really, really good. It's better than those that don't play. And it's better in two different ways. The first way is that they're actually able to resolve small detail in the context of clutter. The other way that they are better is actually being able to resolve different levels of gray. 这些人的视力居然非常好。甚至比那些不玩游戏的人视力还要好。他们视力好体现在两个方面： 其一是他们可以在杂乱中 看到细节。其二是他们能够分辨不同的灰度。 Because I introduced a conflict between the word itself and its color. How good your attention is determines actually how fast you resolve that conflict. What we can show is that when you do this kind of task with people that play a lot of action games, they actually resolve the conflict faster. So clearly playing those action games doesn't lead to attention problems. 因为在这个游戏文字本身的意思和它字体的颜色是矛盾的。你的注意力有多集中决定了你能多快解决这个矛盾。我们会发现如果普通人和经常玩游戏的人一起做这个游戏的时候，经常玩动作电子游戏的人能更快地处理其中的矛盾。 所以很明显地，玩动作电子游戏不会导致 注意力产生问题或是更容易分心。 Those action video game players have many other advantages in terms of attention, and one aspect of attention which is also improved for the better is our ability to track objects around in the world. 动作游戏的玩家在注意力方面还有很多其他优势，其中之一就是能够更好地追踪周围的物体。 They switch really fast, very swiftly. They pay a very small cost. 他们在任务间的切换非常快速敏捷。他们所付出的代价很小。 Those people that identify as being high multimedia-taskers are absolutely abysmal at multitasking. 那些被认为是高效的“多媒体任务处理者”多任务处理的能力非常糟糕。 ","link":"https://cisse-away.github.io/post/your-brain-on-video-games/"},{"title":"How to gain control of your free time","content":"Speaker：Laura Vanderkam 视频 原视频 摘要 We don't build the lives we want by saving time. We build the lives we want, and then time saves itself. 我们不是通过节省时间来打造我们想过的生活；我们应该先建立我们想要的生活，时间就会自然而然节省出来。 Time is highly elastic. We cannot make more time, but time will stretch to accommodate what we choose to put into it. 时间是有弹性的。我们不能创造更多时间，但是时间会自己调整去适应我们选择去做的事情。 Time is a choice. I can acknowledge this is not a matter of lacking time; it's that I don't want to do it. 时间是一种选择。不是因为时间不够，而是不想做。 So take a little bit of time Friday afternoon, make yourself a three-category priority list: career, relationships, self. 在周五下午花一点时间，为自己做一个分成三类的首要事件列表：事业、人际关系、个人。 When we focus on what matters, we can build the lives we want in the time we've got. 当我们关注在重要的事上时，我们可以用所拥有的时间创造我们想要的生活。 ","link":"https://cisse-away.github.io/post/how-to-gain-control-of-your-free-time/"},{"title":"分布式版本控制系统—Git","content":"一.Git安装 全平台下载地址 Linux(Ubuntu) sudo apt install git 若遇到错误 E: Package 'git' has no installation candidate 则先同步更新再安装sudo apt-get update sudo apt-get upgrade Windows 通过安装程序安装 二.配置用户信息 git config --global user.name &quot;Your Name&quot; git config --global user.email &quot;email@example.com&quot; 三. Git 命令 总览 git init # 初始化仓库 git clone # 拷贝远程仓库 git add # 添加文件 git status # 查看仓库状态 git diff # 比较文件差异 git commit # 提交 git reset # 版本回退 git rm # 删除工作区文件 git mv # 移动或重命名工作区文件 git log # 查看历史提交记录 git blame # 以列表形式查看文件历史修改记录 git remote # 远程仓库操作 git fetch # 从远程获取代码库 git pull # 下载远程代码并合并 git push # 上传远程代码并合并 详细用法 git init 在所在目录创建一个仓库 git clone git clone [url] 拷贝一个仓库到本地目录 git clone [url] [name] 拷贝一个仓库到本地目录并以&quot;name&quot;命名 git add git add [file1] [file2] 添加一个或多个文件到暂存区 git add [dir] 添加指定目录到暂存区(包括子目录) git add . 添加当前目录下所有文件到暂存区 git status 查看在你上次提交之后是否有对文件进行再次修改。 ","link":"https://cisse-away.github.io/post/git/"},{"title":"音乐剧《come from away》","content":" 2017年剧评人奖Drama Desk Award最佳音乐剧，由加拿大夫妻Irene Sankoff，David Hein创作。 该剧取材于911事件后加拿大的“黄丝带行动”，根据真实经历改编，讲述了一个小镇的居民向世界敞开家门的精彩而又真实的故事。 背景： 《come from away》所讲的故事背景是&quot;911事件&quot;后，美航总部实施全国范围的禁飞令导致38架来自世界各地的飞机在纽芬兰机场迫降，当地居民克服语言上的障碍帮助这些陌生人渡过难关。充分体现了&quot;有朋自远方来，不亦乐乎&quot;。 内容： 剧中主要讲的就是飞机和小镇上的两批人之间的故事。小镇上的人通过他们的热情使得这些被困的人们从紧张恐慌到放下戒心融入小镇。剧中涉及到的故事有很多，有通过这场劫难相识的异国恋人，有一对同名同性恋人，还有美国第一位女机长以及受到区别对待的穆斯林人等等。最后，当这些受难的人们回到正常的生活后，他们都以自己的方式纪念这段刻骨铭心的经历。 体会： 令我印象深刻的是整个剧组只有12个人，每个人都要分饰多角，通过更换衣物饰品，口音等从一个角色切换到另一个角色。而且转场方式也只靠桌子，椅子与灯光。剧中所有的人物、故事都源自现实生活，甚至还在当地举行了义演。剧中的音乐与舞蹈也体现了当地的风格。这些都让我感受到了剧组的实力与诚心。 整部剧充满了笑点与泪点，表现了人与人之间的复杂情感。合唱震撼心神，solo触动心弦。当然其中还有我没有发现的细节，期待二刷。 ","link":"https://cisse-away.github.io/post/come-from-away/"},{"title":"Hello Gridea","content":"👏 欢迎使用 Gridea ！ ✍️ Gridea 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ... Github Gridea 主页 示例网站 特性👇 📝 你可以使用最酷的 Markdown 语法，进行快速创作 🌉 你可以给文章配上精美的封面图和在文章任意位置插入图片 🏷️ 你可以对文章进行标签分组 📋 你可以自定义菜单，甚至可以创建外部链接菜单 💻 你可以在 Windows，MacOS 或 Linux 设备上使用此客户端 🌎 你可以使用 𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌 或 Coding Pages 向世界展示，未来将支持更多平台 💬 你可以进行简单的配置，接入 Gitalk 或 DisqusJS 评论系统 🇬🇧 你可以使用中文简体或英语 🌁 你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力 🖥 你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步 🌱 当然 Gridea 还很年轻，有很多不足，但请相信，它会不停向前 🏃 未来，它一定会成为你离不开的伙伴 尽情发挥你的才华吧！ 😘 Enjoy~ ","link":"https://cisse-away.github.io/post/hello-gridea/"}]}